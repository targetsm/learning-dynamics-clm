2024-05-31 15:44:34 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/iwslt14.sep.tokenized.de-en', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='de', srcdict=None, target_lang='en', task='translation', tensorboard_logdir=None, testpref='./data/iwslt14.sep.tokenized.de-en/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='./data/iwslt14.sep.tokenized.de-en/train', user_dir=None, validpref='./data/iwslt14.sep.tokenized.de-en/valid', workers=20)
2024-05-31 15:44:35 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8 types
2024-05-31 15:44:36 | INFO | fairseq_cli.preprocess | [de] ./data/iwslt14.sep.tokenized.de-en/train.de: 160239 sents, 160239 tokens, 0.0% replaced by <unk>
2024-05-31 15:44:36 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8 types
2024-05-31 15:44:36 | INFO | fairseq_cli.preprocess | [de] ./data/iwslt14.sep.tokenized.de-en/valid.de: 7283 sents, 7283 tokens, 0.0% replaced by <unk>
2024-05-31 15:44:36 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8 types
2024-05-31 15:44:36 | INFO | fairseq_cli.preprocess | [de] ./data/iwslt14.sep.tokenized.de-en/test.de: 6750 sents, 6750 tokens, 0.0% replaced by <unk>
2024-05-31 15:44:36 | INFO | fairseq_cli.preprocess | [en] Dictionary: 9840 types
2024-05-31 15:44:39 | INFO | fairseq_cli.preprocess | [en] ./data/iwslt14.sep.tokenized.de-en/train.en: 160239 sents, 4025857 tokens, 0.0% replaced by <unk>
2024-05-31 15:44:39 | INFO | fairseq_cli.preprocess | [en] Dictionary: 9840 types
2024-05-31 15:44:39 | INFO | fairseq_cli.preprocess | [en] ./data/iwslt14.sep.tokenized.de-en/valid.en: 7283 sents, 182594 tokens, 0.000548% replaced by <unk>
2024-05-31 15:44:39 | INFO | fairseq_cli.preprocess | [en] Dictionary: 9840 types
2024-05-31 15:44:39 | INFO | fairseq_cli.preprocess | [en] ./data/iwslt14.sep.tokenized.de-en/test.en: 6750 sents, 160684 tokens, 0.00622% replaced by <unk>
2024-05-31 15:44:39 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/iwslt14.sep.tokenized.de-en
2024-05-31 15:44:41 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.sep.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=True, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=1000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=True, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=100, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none')
2024-05-31 15:44:41 | INFO | fairseq.tasks.translation | [de] dictionary: 8 types
2024-05-31 15:44:41 | INFO | fairseq.tasks.translation | [en] dictionary: 9840 types
2024-05-31 15:44:41 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.sep.tokenized.de-en/valid.de-en.de
2024-05-31 15:44:41 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.sep.tokenized.de-en/valid.de-en.en
2024-05-31 15:44:41 | INFO | fairseq.tasks.translation | data-bin/iwslt14.sep.tokenized.de-en valid de-en 7283 examples
2024-05-31 15:44:41 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(9840, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=9840, bias=False)
  )
)
2024-05-31 15:44:41 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2024-05-31 15:44:41 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2024-05-31 15:44:41 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2024-05-31 15:44:41 | INFO | fairseq_cli.train | num. model params: 36585472 (num. trained: 36585472)
2024-05-31 15:44:44 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2024-05-31 15:44:44 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-05-31 15:44:44 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 11.910 GB ; name = TITAN Xp                                
2024-05-31 15:44:44 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-05-31 15:44:44 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-05-31 15:44:44 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None
2024-05-31 15:44:44 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2024-05-31 15:44:44 | INFO | fairseq.trainer | loading train data for epoch 1
2024-05-31 15:44:44 | INFO | fairseq.data.data_utils | loaded 160239 examples from: data-bin/iwslt14.sep.tokenized.de-en/train.de-en.de
2024-05-31 15:44:44 | INFO | fairseq.data.data_utils | loaded 160239 examples from: data-bin/iwslt14.sep.tokenized.de-en/train.de-en.en
2024-05-31 15:44:44 | INFO | fairseq.tasks.translation | data-bin/iwslt14.sep.tokenized.de-en train de-en 160239 examples
2024-05-31 15:44:45 | INFO | fairseq.trainer | begin training epoch 1
2024-05-31 15:45:00 | INFO | train_inner | epoch 001:    100 / 1021 loss=12.835, nll_loss=12.704, ppl=6670.89, wps=27054.6, ups=6.84, wpb=3953.2, bsz=150.9, num_updates=100, lr=1.25e-05, gnorm=3.112, train_wall=15, wall=15
2024-05-31 15:45:00 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:45:01 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_1_100.pt (epoch 1 @ 100 updates, score None) (writing took 0.972448755055666 seconds)
2024-05-31 15:45:16 | INFO | train_inner | epoch 001:    200 / 1021 loss=10.942, nll_loss=10.59, ppl=1541.58, wps=23763.1, ups=6.04, wpb=3936.3, bsz=144.6, num_updates=200, lr=2.5e-05, gnorm=1.578, train_wall=15, wall=32
2024-05-31 15:45:16 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:45:17 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_1_200.pt (epoch 1 @ 200 updates, score None) (writing took 1.0017221798188984 seconds)
2024-05-31 15:45:33 | INFO | train_inner | epoch 001:    300 / 1021 loss=10.073, nll_loss=9.595, ppl=773.3, wps=23827.9, ups=6.07, wpb=3922.9, bsz=162.6, num_updates=300, lr=3.75e-05, gnorm=1.292, train_wall=15, wall=48
2024-05-31 15:45:33 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:45:34 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_1_300.pt (epoch 1 @ 300 updates, score None) (writing took 1.0083568068221211 seconds)
2024-05-31 15:45:48 | INFO | train_inner | epoch 001:    400 / 1021 loss=9.424, nll_loss=8.815, ppl=450.45, wps=25286.6, ups=6.4, wpb=3948.1, bsz=156.6, num_updates=400, lr=5e-05, gnorm=1.433, train_wall=14, wall=64
2024-05-31 15:45:48 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:45:49 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_1_400.pt (epoch 1 @ 400 updates, score None) (writing took 0.9899332709610462 seconds)
2024-05-31 15:46:04 | INFO | train_inner | epoch 001:    500 / 1021 loss=9.091, nll_loss=8.412, ppl=340.62, wps=25397, ups=6.42, wpb=3954, bsz=155.3, num_updates=500, lr=6.25e-05, gnorm=1.284, train_wall=14, wall=79
2024-05-31 15:46:04 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:46:05 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_1_500.pt (epoch 1 @ 500 updates, score None) (writing took 0.9655015817843378 seconds)
2024-05-31 15:46:19 | INFO | train_inner | epoch 001:    600 / 1021 loss=8.878, nll_loss=8.165, ppl=287, wps=25143.9, ups=6.41, wpb=3925.3, bsz=177.8, num_updates=600, lr=7.5e-05, gnorm=1.441, train_wall=15, wall=95
2024-05-31 15:46:19 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:46:20 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_1_600.pt (epoch 1 @ 600 updates, score None) (writing took 0.9811089336872101 seconds)
2024-05-31 15:46:35 | INFO | train_inner | epoch 001:    700 / 1021 loss=8.757, nll_loss=8.027, ppl=260.8, wps=25462.7, ups=6.43, wpb=3957.8, bsz=148.1, num_updates=700, lr=8.75e-05, gnorm=1.22, train_wall=14, wall=110
2024-05-31 15:46:35 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:46:36 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_1_700.pt (epoch 1 @ 700 updates, score None) (writing took 0.9588516461662948 seconds)
2024-05-31 15:46:51 | INFO | train_inner | epoch 001:    800 / 1021 loss=8.542, nll_loss=7.778, ppl=219.5, wps=25358.6, ups=6.41, wpb=3955.6, bsz=166.6, num_updates=800, lr=0.0001, gnorm=1.31, train_wall=15, wall=126
2024-05-31 15:46:51 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:46:51 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_1_800.pt (epoch 1 @ 800 updates, score None) (writing took 0.9540919940918684 seconds)
2024-05-31 15:47:07 | INFO | train_inner | epoch 001:    900 / 1021 loss=8.512, nll_loss=7.742, ppl=214.02, wps=23781.7, ups=6.06, wpb=3921.2, bsz=157.3, num_updates=900, lr=0.0001125, gnorm=1.456, train_wall=15, wall=143
2024-05-31 15:47:07 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:47:08 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_1_900.pt (epoch 1 @ 900 updates, score None) (writing took 0.9805739242583513 seconds)
2024-05-31 15:47:23 | INFO | train_inner | epoch 001:   1000 / 1021 loss=8.381, nll_loss=7.592, ppl=192.93, wps=25188.8, ups=6.38, wpb=3945.6, bsz=148.3, num_updates=1000, lr=0.000125, gnorm=1.135, train_wall=15, wall=158
2024-05-31 15:47:23 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:47:24 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score None) (writing took 0.9580545821227133 seconds)
2024-05-31 15:47:24 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-05-31 15:47:24 | INFO | train | epoch 001 | loss 9.544 | nll_loss 8.942 | ppl 491.94 | wps 24838.3 | ups 6.3 | wpb 3942 | bsz 156.8 | num_updates 1000 | lr 0.000125 | gnorm 1.526 | train_wall 148 | wall 159
2024-05-31 15:47:24 | INFO | fairseq_cli.train | done training in 159.0 seconds
/local/home/ggabriel/ma/alti/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2024-05-31 15:47:25 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.sep.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=True, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=10000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=True, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=500, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none')
2024-05-31 15:47:25 | INFO | fairseq.tasks.translation | [de] dictionary: 8 types
2024-05-31 15:47:25 | INFO | fairseq.tasks.translation | [en] dictionary: 9840 types
2024-05-31 15:47:25 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.sep.tokenized.de-en/valid.de-en.de
2024-05-31 15:47:25 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.sep.tokenized.de-en/valid.de-en.en
2024-05-31 15:47:25 | INFO | fairseq.tasks.translation | data-bin/iwslt14.sep.tokenized.de-en valid de-en 7283 examples
2024-05-31 15:47:26 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(9840, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=9840, bias=False)
  )
)
2024-05-31 15:47:26 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2024-05-31 15:47:26 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2024-05-31 15:47:26 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2024-05-31 15:47:26 | INFO | fairseq_cli.train | num. model params: 36585472 (num. trained: 36585472)
2024-05-31 15:47:29 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2024-05-31 15:47:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-05-31 15:47:29 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 11.910 GB ; name = TITAN Xp                                
2024-05-31 15:47:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-05-31 15:47:29 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-05-31 15:47:29 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None
2024-05-31 15:47:29 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2024-05-31 15:47:29 | INFO | fairseq.trainer | loading train data for epoch 1
2024-05-31 15:47:29 | INFO | fairseq.data.data_utils | loaded 160239 examples from: data-bin/iwslt14.sep.tokenized.de-en/train.de-en.de
2024-05-31 15:47:29 | INFO | fairseq.data.data_utils | loaded 160239 examples from: data-bin/iwslt14.sep.tokenized.de-en/train.de-en.en
2024-05-31 15:47:29 | INFO | fairseq.tasks.translation | data-bin/iwslt14.sep.tokenized.de-en train de-en 160239 examples
2024-05-31 15:47:29 | INFO | fairseq.trainer | begin training epoch 1
2024-05-31 15:47:47 | INFO | train_inner | epoch 001:    100 / 1021 loss=12.835, nll_loss=12.704, ppl=6670.89, wps=22180.4, ups=5.61, wpb=3953.2, bsz=150.9, num_updates=100, lr=1.25e-05, gnorm=3.112, train_wall=18, wall=18
2024-05-31 15:48:03 | INFO | train_inner | epoch 001:    200 / 1021 loss=10.942, nll_loss=10.59, ppl=1541.58, wps=24680.8, ups=6.27, wpb=3936.3, bsz=144.6, num_updates=200, lr=2.5e-05, gnorm=1.578, train_wall=16, wall=34
2024-05-31 15:48:19 | INFO | train_inner | epoch 001:    300 / 1021 loss=10.073, nll_loss=9.595, ppl=773.3, wps=25605, ups=6.53, wpb=3922.9, bsz=162.6, num_updates=300, lr=3.75e-05, gnorm=1.292, train_wall=15, wall=50
2024-05-31 15:48:34 | INFO | train_inner | epoch 001:    400 / 1021 loss=9.424, nll_loss=8.815, ppl=450.45, wps=25701.6, ups=6.51, wpb=3948.1, bsz=156.6, num_updates=400, lr=5e-05, gnorm=1.433, train_wall=15, wall=65
2024-05-31 15:48:49 | INFO | train_inner | epoch 001:    500 / 1021 loss=9.091, nll_loss=8.412, ppl=340.62, wps=25659.1, ups=6.49, wpb=3954, bsz=155.3, num_updates=500, lr=6.25e-05, gnorm=1.284, train_wall=15, wall=80
2024-05-31 15:48:49 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:48:52 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_1_500.pt (epoch 1 @ 500 updates, score None) (writing took 2.6647762791253626 seconds)
2024-05-31 15:49:07 | INFO | train_inner | epoch 001:    600 / 1021 loss=8.878, nll_loss=8.165, ppl=287, wps=21890.7, ups=5.58, wpb=3925.3, bsz=177.8, num_updates=600, lr=7.5e-05, gnorm=1.441, train_wall=15, wall=98
2024-05-31 15:49:23 | INFO | train_inner | epoch 001:    700 / 1021 loss=8.757, nll_loss=8.027, ppl=260.8, wps=25797.6, ups=6.52, wpb=3957.8, bsz=148.1, num_updates=700, lr=8.75e-05, gnorm=1.22, train_wall=15, wall=114
2024-05-31 15:49:38 | INFO | train_inner | epoch 001:    800 / 1021 loss=8.542, nll_loss=7.778, ppl=219.5, wps=25736.3, ups=6.51, wpb=3955.6, bsz=166.6, num_updates=800, lr=0.0001, gnorm=1.31, train_wall=15, wall=129
2024-05-31 15:49:53 | INFO | train_inner | epoch 001:    900 / 1021 loss=8.512, nll_loss=7.742, ppl=214.02, wps=25728, ups=6.56, wpb=3921.2, bsz=157.3, num_updates=900, lr=0.0001125, gnorm=1.456, train_wall=15, wall=144
2024-05-31 15:50:09 | INFO | train_inner | epoch 001:   1000 / 1021 loss=8.381, nll_loss=7.592, ppl=192.93, wps=25597, ups=6.49, wpb=3945.6, bsz=148.3, num_updates=1000, lr=0.000125, gnorm=1.135, train_wall=15, wall=160
2024-05-31 15:50:09 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:50:11 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score None) (writing took 2.6075531649403274 seconds)
2024-05-31 15:50:15 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:50:15 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-05-31 15:50:15 | INFO | train | epoch 001 | loss 9.517 | nll_loss 8.912 | ppl 481.61 | wps 24399.6 | ups 6.19 | wpb 3943.1 | bsz 156.9 | num_updates 1021 | lr 0.000127625 | gnorm 1.517 | train_wall 158 | wall 166
2024-05-31 15:50:15 | INFO | fairseq.trainer | begin training epoch 2
/local/home/ggabriel/ma/alti/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2024-05-31 15:50:27 | INFO | train_inner | epoch 002:     79 / 1021 loss=8.266, nll_loss=7.458, ppl=175.8, wps=21750.8, ups=5.55, wpb=3920.9, bsz=146.8, num_updates=1100, lr=0.0001375, gnorm=1.162, train_wall=15, wall=178
2024-05-31 15:50:42 | INFO | train_inner | epoch 002:    179 / 1021 loss=8.147, nll_loss=7.318, ppl=159.59, wps=25867.6, ups=6.53, wpb=3958.5, bsz=170.2, num_updates=1200, lr=0.00015, gnorm=1.326, train_wall=15, wall=193
2024-05-31 15:50:58 | INFO | train_inner | epoch 002:    279 / 1021 loss=8.073, nll_loss=7.234, ppl=150.52, wps=25205.5, ups=6.37, wpb=3956.4, bsz=171.8, num_updates=1300, lr=0.0001625, gnorm=1.179, train_wall=16, wall=209
2024-05-31 15:51:13 | INFO | train_inner | epoch 002:    379 / 1021 loss=8.047, nll_loss=7.203, ppl=147.29, wps=25503, ups=6.51, wpb=3917.1, bsz=150.6, num_updates=1400, lr=0.000175, gnorm=1.096, train_wall=15, wall=224
2024-05-31 15:51:29 | INFO | train_inner | epoch 002:    479 / 1021 loss=7.96, nll_loss=7.103, ppl=137.43, wps=25728.1, ups=6.5, wpb=3957, bsz=146.1, num_updates=1500, lr=0.0001875, gnorm=1.057, train_wall=15, wall=239
2024-05-31 15:51:29 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:51:29 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_2_1500.pt (epoch 2 @ 1500 updates, score None) (writing took 0.979619403835386 seconds)
2024-05-31 15:51:44 | INFO | train_inner | epoch 002:    579 / 1021 loss=7.89, nll_loss=7.022, ppl=129.93, wps=25440.3, ups=6.44, wpb=3948, bsz=170.4, num_updates=1600, lr=0.0002, gnorm=1.149, train_wall=14, wall=255
2024-05-31 15:51:59 | INFO | train_inner | epoch 002:    679 / 1021 loss=7.83, nll_loss=6.953, ppl=123.9, wps=27429.4, ups=6.89, wpb=3979.4, bsz=159.8, num_updates=1700, lr=0.0002125, gnorm=1.02, train_wall=14, wall=269
2024-05-31 15:52:13 | INFO | train_inner | epoch 002:    779 / 1021 loss=7.854, nll_loss=6.98, ppl=126.23, wps=27331.3, ups=6.97, wpb=3921.9, bsz=138.2, num_updates=1800, lr=0.000225, gnorm=1.045, train_wall=14, wall=284
2024-05-31 15:52:28 | INFO | train_inner | epoch 002:    879 / 1021 loss=7.8, nll_loss=6.918, ppl=120.9, wps=26898.1, ups=6.82, wpb=3944.2, bsz=158.9, num_updates=1900, lr=0.0002375, gnorm=1.052, train_wall=15, wall=298
2024-05-31 15:52:43 | INFO | train_inner | epoch 002:    979 / 1021 loss=7.725, nll_loss=6.832, ppl=113.93, wps=25565.9, ups=6.47, wpb=3951.6, bsz=175.5, num_updates=2000, lr=0.00025, gnorm=1.103, train_wall=15, wall=314
2024-05-31 15:52:43 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:52:44 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score None) (writing took 0.9467363655567169 seconds)
2024-05-31 15:52:50 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:52:50 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-05-31 15:52:50 | INFO | train | epoch 002 | loss 7.945 | nll_loss 7.085 | ppl 135.77 | wps 25875.1 | ups 6.56 | wpb 3943.1 | bsz 156.9 | num_updates 2042 | lr 0.00025525 | gnorm 1.106 | train_wall 152 | wall 321
2024-05-31 15:52:50 | INFO | fairseq.trainer | begin training epoch 3
2024-05-31 15:52:59 | INFO | train_inner | epoch 003:     58 / 1021 loss=7.713, nll_loss=6.817, ppl=112.74, wps=24169.1, ups=6.17, wpb=3914.2, bsz=131.8, num_updates=2100, lr=0.0002625, gnorm=0.884, train_wall=15, wall=330
2024-05-31 15:53:15 | INFO | train_inner | epoch 003:    158 / 1021 loss=7.609, nll_loss=6.698, ppl=103.86, wps=25713, ups=6.47, wpb=3973.3, bsz=161, num_updates=2200, lr=0.000275, gnorm=0.903, train_wall=15, wall=346
2024-05-31 15:53:30 | INFO | train_inner | epoch 003:    258 / 1021 loss=7.585, nll_loss=6.67, ppl=101.81, wps=25377.7, ups=6.4, wpb=3963.5, bsz=151, num_updates=2300, lr=0.0002875, gnorm=0.898, train_wall=15, wall=361
2024-05-31 15:53:46 | INFO | train_inner | epoch 003:    358 / 1021 loss=7.596, nll_loss=6.683, ppl=102.73, wps=25889.5, ups=6.56, wpb=3947.6, bsz=141.7, num_updates=2400, lr=0.0003, gnorm=0.871, train_wall=15, wall=376
2024-05-31 15:54:01 | INFO | train_inner | epoch 003:    458 / 1021 loss=7.537, nll_loss=6.616, ppl=98.08, wps=25843.3, ups=6.55, wpb=3944, bsz=156.2, num_updates=2500, lr=0.0003125, gnorm=0.896, train_wall=15, wall=392
2024-05-31 15:54:01 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:54:02 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_3_2500.pt (epoch 3 @ 2500 updates, score None) (writing took 1.048301654867828 seconds)
2024-05-31 15:54:17 | INFO | train_inner | epoch 003:    558 / 1021 loss=7.518, nll_loss=6.594, ppl=96.58, wps=23929, ups=6.1, wpb=3924.3, bsz=171.6, num_updates=2600, lr=0.000325, gnorm=0.954, train_wall=15, wall=408
2024-05-31 15:54:33 | INFO | train_inner | epoch 003:    658 / 1021 loss=7.476, nll_loss=6.546, ppl=93.42, wps=25694.5, ups=6.52, wpb=3941.1, bsz=161, num_updates=2700, lr=0.0003375, gnorm=0.818, train_wall=15, wall=423
2024-05-31 15:54:48 | INFO | train_inner | epoch 003:    758 / 1021 loss=7.464, nll_loss=6.532, ppl=92.54, wps=25819.9, ups=6.56, wpb=3937.5, bsz=149.5, num_updates=2800, lr=0.00035, gnorm=0.813, train_wall=15, wall=439
2024-05-31 15:55:03 | INFO | train_inner | epoch 003:    858 / 1021 loss=7.429, nll_loss=6.492, ppl=89.99, wps=25623.3, ups=6.51, wpb=3937.9, bsz=167.2, num_updates=2900, lr=0.0003625, gnorm=0.867, train_wall=15, wall=454
2024-05-31 15:55:19 | INFO | train_inner | epoch 003:    958 / 1021 loss=7.416, nll_loss=6.476, ppl=89.05, wps=25580.8, ups=6.47, wpb=3953.7, bsz=155.7, num_updates=3000, lr=0.000375, gnorm=0.801, train_wall=15, wall=470
2024-05-31 15:55:19 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:55:20 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_3_3000.pt (epoch 3 @ 3000 updates, score None) (writing took 0.9779600608162582 seconds)
2024-05-31 15:55:29 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:55:29 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-05-31 15:55:29 | INFO | train | epoch 003 | loss 7.515 | nll_loss 6.59 | ppl 96.36 | wps 25335.7 | ups 6.43 | wpb 3943.1 | bsz 156.9 | num_updates 3063 | lr 0.000382875 | gnorm 0.874 | train_wall 155 | wall 480
2024-05-31 15:55:29 | INFO | fairseq.trainer | begin training epoch 4
2024-05-31 15:55:35 | INFO | train_inner | epoch 004:     37 / 1021 loss=7.357, nll_loss=6.411, ppl=85.07, wps=24057.6, ups=6.09, wpb=3951.6, bsz=175.4, num_updates=3100, lr=0.0003875, gnorm=0.86, train_wall=15, wall=486
2024-05-31 15:55:50 | INFO | train_inner | epoch 004:    137 / 1021 loss=7.387, nll_loss=6.444, ppl=87.05, wps=25676.7, ups=6.56, wpb=3914.3, bsz=137.2, num_updates=3200, lr=0.0004, gnorm=0.763, train_wall=15, wall=501
2024-05-31 15:56:06 | INFO | train_inner | epoch 004:    237 / 1021 loss=7.302, nll_loss=6.346, ppl=81.33, wps=25766.7, ups=6.5, wpb=3965, bsz=149.9, num_updates=3300, lr=0.0004125, gnorm=0.757, train_wall=15, wall=517
2024-05-31 15:56:21 | INFO | train_inner | epoch 004:    337 / 1021 loss=7.323, nll_loss=6.37, ppl=82.69, wps=25774.3, ups=6.58, wpb=3914.4, bsz=140.1, num_updates=3400, lr=0.000425, gnorm=0.745, train_wall=15, wall=532
2024-05-31 15:56:36 | INFO | train_inner | epoch 004:    437 / 1021 loss=7.288, nll_loss=6.33, ppl=80.47, wps=25740.1, ups=6.53, wpb=3942.6, bsz=142.2, num_updates=3500, lr=0.0004375, gnorm=0.726, train_wall=15, wall=547
2024-05-31 15:56:36 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:56:37 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_4_3500.pt (epoch 4 @ 3500 updates, score None) (writing took 0.9855338023044169 seconds)
2024-05-31 15:56:52 | INFO | train_inner | epoch 004:    537 / 1021 loss=7.249, nll_loss=6.286, ppl=78.01, wps=24489.9, ups=6.22, wpb=3936.8, bsz=168.6, num_updates=3600, lr=0.00045, gnorm=0.788, train_wall=15, wall=563
2024-05-31 15:57:08 | INFO | train_inner | epoch 004:    637 / 1021 loss=7.253, nll_loss=6.29, ppl=78.25, wps=25256.9, ups=6.4, wpb=3945.3, bsz=157, num_updates=3700, lr=0.0004625, gnorm=0.764, train_wall=15, wall=579
2024-05-31 15:57:24 | INFO | train_inner | epoch 004:    737 / 1021 loss=7.189, nll_loss=6.217, ppl=74.4, wps=25246.1, ups=6.34, wpb=3980.3, bsz=175.8, num_updates=3800, lr=0.000475, gnorm=0.739, train_wall=16, wall=595
2024-05-31 15:57:40 | INFO | train_inner | epoch 004:    837 / 1021 loss=7.2, nll_loss=6.23, ppl=75.07, wps=24144.7, ups=6.15, wpb=3925.3, bsz=161.1, num_updates=3900, lr=0.0004875, gnorm=0.753, train_wall=16, wall=611
2024-05-31 15:57:56 | INFO | train_inner | epoch 004:    937 / 1021 loss=7.172, nll_loss=6.197, ppl=73.36, wps=24971.6, ups=6.33, wpb=3946, bsz=160.7, num_updates=4000, lr=0.0005, gnorm=0.715, train_wall=16, wall=627
2024-05-31 15:57:56 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:57:57 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_4_4000.pt (epoch 4 @ 4000 updates, score None) (writing took 0.9765366809442639 seconds)
2024-05-31 15:58:09 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:58:09 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-05-31 15:58:09 | INFO | train | epoch 004 | loss 7.257 | nll_loss 6.294 | ppl 78.48 | wps 25138.8 | ups 6.38 | wpb 3943.1 | bsz 156.9 | num_updates 4084 | lr 0.000494831 | gnorm 0.751 | train_wall 156 | wall 640
2024-05-31 15:58:09 | INFO | fairseq.trainer | begin training epoch 5
2024-05-31 15:58:12 | INFO | train_inner | epoch 005:     16 / 1021 loss=7.151, nll_loss=6.174, ppl=72.22, wps=24265.4, ups=6.17, wpb=3935.5, bsz=165.9, num_updates=4100, lr=0.000493865, gnorm=0.72, train_wall=15, wall=643
2024-05-31 15:58:28 | INFO | train_inner | epoch 005:    116 / 1021 loss=7.105, nll_loss=6.121, ppl=69.59, wps=25379.2, ups=6.39, wpb=3972.7, bsz=169.6, num_updates=4200, lr=0.00048795, gnorm=0.717, train_wall=15, wall=658
2024-05-31 15:58:43 | INFO | train_inner | epoch 005:    216 / 1021 loss=7.088, nll_loss=6.101, ppl=68.63, wps=25142.6, ups=6.38, wpb=3941.8, bsz=154.9, num_updates=4300, lr=0.000482243, gnorm=0.71, train_wall=16, wall=674
2024-05-31 15:58:59 | INFO | train_inner | epoch 005:    316 / 1021 loss=7.051, nll_loss=6.058, ppl=66.62, wps=25260.4, ups=6.41, wpb=3942.8, bsz=164.4, num_updates=4400, lr=0.000476731, gnorm=0.698, train_wall=15, wall=690
2024-05-31 15:59:15 | INFO | train_inner | epoch 005:    416 / 1021 loss=7.071, nll_loss=6.081, ppl=67.69, wps=25113.2, ups=6.38, wpb=3936.5, bsz=145, num_updates=4500, lr=0.000471405, gnorm=0.667, train_wall=16, wall=705
2024-05-31 15:59:15 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 15:59:16 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_5_4500.pt (epoch 5 @ 4500 updates, score None) (writing took 0.9751517469994724 seconds)
2024-05-31 15:59:31 | INFO | train_inner | epoch 005:    516 / 1021 loss=7.032, nll_loss=6.036, ppl=65.62, wps=24255.3, ups=6.12, wpb=3961.5, bsz=148.5, num_updates=4600, lr=0.000466252, gnorm=0.663, train_wall=15, wall=722
2024-05-31 15:59:47 | INFO | train_inner | epoch 005:    616 / 1021 loss=7.013, nll_loss=6.014, ppl=64.63, wps=24997.5, ups=6.31, wpb=3960.4, bsz=161, num_updates=4700, lr=0.000461266, gnorm=0.675, train_wall=16, wall=738
2024-05-31 16:00:02 | INFO | train_inner | epoch 005:    716 / 1021 loss=7.013, nll_loss=6.014, ppl=64.63, wps=25205.7, ups=6.39, wpb=3943, bsz=163.3, num_updates=4800, lr=0.000456435, gnorm=0.699, train_wall=15, wall=753
2024-05-31 16:00:18 | INFO | train_inner | epoch 005:    816 / 1021 loss=7.001, nll_loss=6.001, ppl=64.05, wps=25067.3, ups=6.44, wpb=3894.2, bsz=162.1, num_updates=4900, lr=0.000451754, gnorm=0.681, train_wall=15, wall=769
2024-05-31 16:00:33 | INFO | train_inner | epoch 005:    916 / 1021 loss=7.032, nll_loss=6.036, ppl=65.61, wps=25542.1, ups=6.49, wpb=3935.6, bsz=138.1, num_updates=5000, lr=0.000447214, gnorm=0.666, train_wall=15, wall=784
2024-05-31 16:00:33 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:00:34 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_5_5000.pt (epoch 5 @ 5000 updates, score None) (writing took 1.0390294012613595 seconds)
2024-05-31 16:00:50 | INFO | train_inner | epoch 005:   1016 / 1021 loss=6.955, nll_loss=5.949, ppl=61.77, wps=23904.6, ups=6.06, wpb=3943.4, bsz=163.4, num_updates=5100, lr=0.000442807, gnorm=0.659, train_wall=15, wall=801
2024-05-31 16:00:51 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:00:51 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-05-31 16:00:51 | INFO | train | epoch 005 | loss 7.037 | nll_loss 6.042 | ppl 65.89 | wps 24955 | ups 6.33 | wpb 3943.1 | bsz 156.9 | num_updates 5105 | lr 0.000442591 | gnorm 0.683 | train_wall 157 | wall 801
2024-05-31 16:00:51 | INFO | fairseq.trainer | begin training epoch 6
2024-05-31 16:01:05 | INFO | train_inner | epoch 006:     95 / 1021 loss=6.935, nll_loss=5.925, ppl=60.75, wps=25117.1, ups=6.4, wpb=3924.5, bsz=143.4, num_updates=5200, lr=0.000438529, gnorm=0.678, train_wall=15, wall=816
2024-05-31 16:01:21 | INFO | train_inner | epoch 006:    195 / 1021 loss=6.875, nll_loss=5.856, ppl=57.92, wps=25377.1, ups=6.49, wpb=3912, bsz=151, num_updates=5300, lr=0.000434372, gnorm=0.668, train_wall=15, wall=832
2024-05-31 16:01:37 | INFO | train_inner | epoch 006:    295 / 1021 loss=6.877, nll_loss=5.857, ppl=57.97, wps=25084, ups=6.35, wpb=3948.4, bsz=154.3, num_updates=5400, lr=0.000430331, gnorm=0.665, train_wall=16, wall=847
2024-05-31 16:01:52 | INFO | train_inner | epoch 006:    395 / 1021 loss=6.893, nll_loss=5.875, ppl=58.69, wps=24948.7, ups=6.35, wpb=3931.8, bsz=144.9, num_updates=5500, lr=0.000426401, gnorm=0.686, train_wall=16, wall=863
2024-05-31 16:01:52 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:01:53 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_6_5500.pt (epoch 6 @ 5500 updates, score None) (writing took 1.0984880491159856 seconds)
2024-05-31 16:02:09 | INFO | train_inner | epoch 006:    495 / 1021 loss=6.859, nll_loss=5.836, ppl=57.13, wps=23628.2, ups=5.99, wpb=3946.1, bsz=167.3, num_updates=5600, lr=0.000422577, gnorm=0.683, train_wall=15, wall=880
2024-05-31 16:02:25 | INFO | train_inner | epoch 006:    595 / 1021 loss=6.839, nll_loss=5.814, ppl=56.27, wps=25041.4, ups=6.34, wpb=3946.7, bsz=172.1, num_updates=5700, lr=0.000418854, gnorm=0.706, train_wall=16, wall=896
2024-05-31 16:02:41 | INFO | train_inner | epoch 006:    695 / 1021 loss=6.807, nll_loss=5.778, ppl=54.86, wps=25234.5, ups=6.34, wpb=3979.9, bsz=177.7, num_updates=5800, lr=0.000415227, gnorm=0.647, train_wall=16, wall=911
2024-05-31 16:02:56 | INFO | train_inner | epoch 006:    795 / 1021 loss=6.887, nll_loss=5.867, ppl=58.38, wps=25310.1, ups=6.44, wpb=3930.3, bsz=142.2, num_updates=5900, lr=0.000411693, gnorm=0.67, train_wall=15, wall=927
2024-05-31 16:03:12 | INFO | train_inner | epoch 006:    895 / 1021 loss=6.854, nll_loss=5.832, ppl=56.95, wps=25462.7, ups=6.46, wpb=3940, bsz=159.1, num_updates=6000, lr=0.000408248, gnorm=0.673, train_wall=15, wall=942
2024-05-31 16:03:12 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:03:13 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_6_6000.pt (epoch 6 @ 6000 updates, score None) (writing took 1.0963601702824235 seconds)
2024-05-31 16:03:28 | INFO | train_inner | epoch 006:    995 / 1021 loss=6.833, nll_loss=5.807, ppl=56, wps=23685.6, ups=5.98, wpb=3962.2, bsz=157.9, num_updates=6100, lr=0.000404888, gnorm=0.652, train_wall=15, wall=959
2024-05-31 16:03:32 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:03:32 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2024-05-31 16:03:32 | INFO | train | epoch 006 | loss 6.864 | nll_loss 5.842 | ppl 57.37 | wps 24881.4 | ups 6.31 | wpb 3943.1 | bsz 156.9 | num_updates 6126 | lr 0.000404028 | gnorm 0.671 | train_wall 158 | wall 963
2024-05-31 16:03:32 | INFO | fairseq.trainer | begin training epoch 7
2024-05-31 16:03:44 | INFO | train_inner | epoch 007:     74 / 1021 loss=6.747, nll_loss=5.709, ppl=52.31, wps=25135.5, ups=6.35, wpb=3960.7, bsz=153.4, num_updates=6200, lr=0.00040161, gnorm=0.641, train_wall=15, wall=975
2024-05-31 16:03:59 | INFO | train_inner | epoch 007:    174 / 1021 loss=6.745, nll_loss=5.705, ppl=52.17, wps=25428.1, ups=6.49, wpb=3917.4, bsz=156.8, num_updates=6300, lr=0.00039841, gnorm=0.7, train_wall=15, wall=990
2024-05-31 16:04:15 | INFO | train_inner | epoch 007:    274 / 1021 loss=6.742, nll_loss=5.701, ppl=52.03, wps=25409.2, ups=6.4, wpb=3967.4, bsz=162.5, num_updates=6400, lr=0.000395285, gnorm=0.67, train_wall=15, wall=1006
2024-05-31 16:04:31 | INFO | train_inner | epoch 007:    374 / 1021 loss=6.748, nll_loss=5.708, ppl=52.26, wps=25282.5, ups=6.41, wpb=3944, bsz=155.4, num_updates=6500, lr=0.000392232, gnorm=0.669, train_wall=15, wall=1022
2024-05-31 16:04:31 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:04:32 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_7_6500.pt (epoch 7 @ 6500 updates, score None) (writing took 1.056202326901257 seconds)
2024-05-31 16:04:48 | INFO | train_inner | epoch 007:    474 / 1021 loss=6.757, nll_loss=5.718, ppl=52.63, wps=23543.1, ups=5.95, wpb=3959.3, bsz=158.6, num_updates=6600, lr=0.000389249, gnorm=0.678, train_wall=16, wall=1038
2024-05-31 16:05:03 | INFO | train_inner | epoch 007:    574 / 1021 loss=6.79, nll_loss=5.756, ppl=54.04, wps=24901.1, ups=6.4, wpb=3891.2, bsz=145.5, num_updates=6700, lr=0.000386334, gnorm=0.687, train_wall=15, wall=1054
2024-05-31 16:05:19 | INFO | train_inner | epoch 007:    674 / 1021 loss=6.749, nll_loss=5.709, ppl=52.31, wps=24955.7, ups=6.32, wpb=3946.1, bsz=159, num_updates=6800, lr=0.000383482, gnorm=0.664, train_wall=16, wall=1070
2024-05-31 16:05:35 | INFO | train_inner | epoch 007:    774 / 1021 loss=6.735, nll_loss=5.694, ppl=51.75, wps=24664, ups=6.28, wpb=3928.1, bsz=160.2, num_updates=6900, lr=0.000380693, gnorm=0.67, train_wall=16, wall=1086
2024-05-31 16:05:51 | INFO | train_inner | epoch 007:    874 / 1021 loss=6.741, nll_loss=5.7, ppl=51.97, wps=25360.5, ups=6.4, wpb=3965.5, bsz=145.6, num_updates=7000, lr=0.000377964, gnorm=0.649, train_wall=15, wall=1101
2024-05-31 16:05:51 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:05:52 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_7_7000.pt (epoch 7 @ 7000 updates, score None) (writing took 1.027156944386661 seconds)
2024-05-31 16:06:07 | INFO | train_inner | epoch 007:    974 / 1021 loss=6.696, nll_loss=5.649, ppl=50.16, wps=23451.1, ups=5.93, wpb=3954.9, bsz=170.9, num_updates=7100, lr=0.000375293, gnorm=0.678, train_wall=16, wall=1118
2024-05-31 16:06:15 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:06:15 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-05-31 16:06:15 | INFO | train | epoch 007 | loss 6.742 | nll_loss 5.701 | ppl 52.02 | wps 24778 | ups 6.28 | wpb 3943.1 | bsz 156.9 | num_updates 7147 | lr 0.000374057 | gnorm 0.671 | train_wall 158 | wall 1126
2024-05-31 16:06:15 | INFO | fairseq.trainer | begin training epoch 8
2024-05-31 16:06:23 | INFO | train_inner | epoch 008:     53 / 1021 loss=6.698, nll_loss=5.651, ppl=50.24, wps=24353, ups=6.24, wpb=3904.3, bsz=149.9, num_updates=7200, lr=0.000372678, gnorm=0.687, train_wall=16, wall=1134
2024-05-31 16:06:39 | INFO | train_inner | epoch 008:    153 / 1021 loss=6.608, nll_loss=5.548, ppl=46.78, wps=25045.6, ups=6.3, wpb=3972.6, bsz=178.8, num_updates=7300, lr=0.000370117, gnorm=0.662, train_wall=16, wall=1150
2024-05-31 16:06:55 | INFO | train_inner | epoch 008:    253 / 1021 loss=6.665, nll_loss=5.61, ppl=48.85, wps=25256.3, ups=6.4, wpb=3944.7, bsz=136.1, num_updates=7400, lr=0.000367607, gnorm=0.656, train_wall=15, wall=1166
2024-05-31 16:07:10 | INFO | train_inner | epoch 008:    353 / 1021 loss=6.64, nll_loss=5.583, ppl=47.93, wps=25599.3, ups=6.46, wpb=3963.8, bsz=181.4, num_updates=7500, lr=0.000365148, gnorm=0.7, train_wall=15, wall=1181
2024-05-31 16:07:10 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:07:11 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_8_7500.pt (epoch 8 @ 7500 updates, score None) (writing took 1.0854383609257638 seconds)
2024-05-31 16:07:27 | INFO | train_inner | epoch 008:    453 / 1021 loss=6.637, nll_loss=5.579, ppl=47.81, wps=23788.4, ups=6.03, wpb=3948, bsz=154.9, num_updates=7600, lr=0.000362738, gnorm=0.663, train_wall=15, wall=1198
2024-05-31 16:07:43 | INFO | train_inner | epoch 008:    553 / 1021 loss=6.642, nll_loss=5.584, ppl=47.98, wps=25232.9, ups=6.37, wpb=3961.7, bsz=152.5, num_updates=7700, lr=0.000360375, gnorm=0.655, train_wall=16, wall=1214
2024-05-31 16:07:58 | INFO | train_inner | epoch 008:    653 / 1021 loss=6.657, nll_loss=5.602, ppl=48.57, wps=25405.7, ups=6.44, wpb=3944.4, bsz=164.3, num_updates=7800, lr=0.000358057, gnorm=0.689, train_wall=15, wall=1229
2024-05-31 16:08:14 | INFO | train_inner | epoch 008:    753 / 1021 loss=6.662, nll_loss=5.607, ppl=48.76, wps=25320.6, ups=6.44, wpb=3931.9, bsz=139.4, num_updates=7900, lr=0.000355784, gnorm=0.662, train_wall=15, wall=1245
2024-05-31 16:08:29 | INFO | train_inner | epoch 008:    853 / 1021 loss=6.64, nll_loss=5.583, ppl=47.93, wps=25237.1, ups=6.43, wpb=3924.1, bsz=157.3, num_updates=8000, lr=0.000353553, gnorm=0.675, train_wall=15, wall=1260
2024-05-31 16:08:29 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:08:30 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_8_8000.pt (epoch 8 @ 8000 updates, score None) (writing took 1.1163028250448406 seconds)
2024-05-31 16:08:46 | INFO | train_inner | epoch 008:    953 / 1021 loss=6.643, nll_loss=5.586, ppl=48.02, wps=23397.7, ups=5.93, wpb=3946.2, bsz=161.8, num_updates=8100, lr=0.000351364, gnorm=0.718, train_wall=16, wall=1277
2024-05-31 16:08:57 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:08:57 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-05-31 16:08:57 | INFO | train | epoch 008 | loss 6.648 | nll_loss 5.591 | ppl 48.21 | wps 24858.1 | ups 6.3 | wpb 3943.1 | bsz 156.9 | num_updates 8168 | lr 0.000349899 | gnorm 0.676 | train_wall 158 | wall 1288
2024-05-31 16:08:57 | INFO | fairseq.trainer | begin training epoch 9
2024-05-31 16:09:02 | INFO | train_inner | epoch 009:     32 / 1021 loss=6.636, nll_loss=5.578, ppl=47.77, wps=24749, ups=6.31, wpb=3920.5, bsz=144.3, num_updates=8200, lr=0.000349215, gnorm=0.666, train_wall=15, wall=1293
2024-05-31 16:09:18 | INFO | train_inner | epoch 009:    132 / 1021 loss=6.56, nll_loss=5.491, ppl=44.97, wps=24855.1, ups=6.28, wpb=3957.4, bsz=157.4, num_updates=8300, lr=0.000347105, gnorm=0.687, train_wall=16, wall=1309
2024-05-31 16:09:34 | INFO | train_inner | epoch 009:    232 / 1021 loss=6.582, nll_loss=5.516, ppl=45.74, wps=24885.2, ups=6.33, wpb=3932.2, bsz=149.2, num_updates=8400, lr=0.000345033, gnorm=0.673, train_wall=16, wall=1325
2024-05-31 16:09:49 | INFO | train_inner | epoch 009:    332 / 1021 loss=6.539, nll_loss=5.465, ppl=44.18, wps=25123.8, ups=6.34, wpb=3964.9, bsz=163.8, num_updates=8500, lr=0.000342997, gnorm=0.674, train_wall=16, wall=1340
2024-05-31 16:09:49 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:09:51 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_9_8500.pt (epoch 9 @ 8500 updates, score None) (writing took 1.0353762554004788 seconds)
2024-05-31 16:10:06 | INFO | train_inner | epoch 009:    432 / 1021 loss=6.556, nll_loss=5.486, ppl=44.82, wps=23814.1, ups=6.03, wpb=3950.1, bsz=168.8, num_updates=8600, lr=0.000340997, gnorm=0.689, train_wall=15, wall=1357
2024-05-31 16:10:22 | INFO | train_inner | epoch 009:    532 / 1021 loss=6.593, nll_loss=5.527, ppl=46.1, wps=25153.6, ups=6.39, wpb=3939.2, bsz=144.9, num_updates=8700, lr=0.000339032, gnorm=0.672, train_wall=15, wall=1373
2024-05-31 16:10:38 | INFO | train_inner | epoch 009:    632 / 1021 loss=6.577, nll_loss=5.508, ppl=45.51, wps=24974.7, ups=6.33, wpb=3943.5, bsz=168.6, num_updates=8800, lr=0.0003371, gnorm=0.702, train_wall=16, wall=1388
2024-05-31 16:10:53 | INFO | train_inner | epoch 009:    732 / 1021 loss=6.594, nll_loss=5.529, ppl=46.16, wps=24915.9, ups=6.36, wpb=3919.8, bsz=154.6, num_updates=8900, lr=0.000335201, gnorm=0.706, train_wall=16, wall=1404
2024-05-31 16:11:09 | INFO | train_inner | epoch 009:    832 / 1021 loss=6.585, nll_loss=5.517, ppl=45.78, wps=25186.3, ups=6.39, wpb=3940.2, bsz=156.7, num_updates=9000, lr=0.000333333, gnorm=0.716, train_wall=15, wall=1420
2024-05-31 16:11:09 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:11:10 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_9_9000.pt (epoch 9 @ 9000 updates, score None) (writing took 1.029330343939364 seconds)
2024-05-31 16:11:25 | INFO | train_inner | epoch 009:    932 / 1021 loss=6.575, nll_loss=5.506, ppl=45.45, wps=24874.3, ups=6.29, wpb=3953.2, bsz=145.8, num_updates=9100, lr=0.000331497, gnorm=0.663, train_wall=15, wall=1436
2024-05-31 16:11:38 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:11:38 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2024-05-31 16:11:38 | INFO | train | epoch 009 | loss 6.574 | nll_loss 5.506 | ppl 45.45 | wps 24902.4 | ups 6.32 | wpb 3943.1 | bsz 156.9 | num_updates 9189 | lr 0.000329888 | gnorm 0.688 | train_wall 158 | wall 1449
2024-05-31 16:11:39 | INFO | fairseq.trainer | begin training epoch 10
2024-05-31 16:11:40 | INFO | train_inner | epoch 010:     11 / 1021 loss=6.585, nll_loss=5.518, ppl=45.82, wps=25322.7, ups=6.43, wpb=3936.8, bsz=157.9, num_updates=9200, lr=0.00032969, gnorm=0.699, train_wall=15, wall=1451
2024-05-31 16:11:56 | INFO | train_inner | epoch 010:    111 / 1021 loss=6.456, nll_loss=5.371, ppl=41.38, wps=25342.5, ups=6.39, wpb=3965.1, bsz=167.2, num_updates=9300, lr=0.000327913, gnorm=0.699, train_wall=15, wall=1467
2024-05-31 16:12:12 | INFO | train_inner | epoch 010:    211 / 1021 loss=6.514, nll_loss=5.436, ppl=43.28, wps=25263.3, ups=6.43, wpb=3928.9, bsz=146.3, num_updates=9400, lr=0.000326164, gnorm=0.692, train_wall=15, wall=1482
2024-05-31 16:12:27 | INFO | train_inner | epoch 010:    311 / 1021 loss=6.504, nll_loss=5.424, ppl=42.93, wps=25322.1, ups=6.42, wpb=3945.6, bsz=149.3, num_updates=9500, lr=0.000324443, gnorm=0.682, train_wall=15, wall=1498
2024-05-31 16:12:27 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:12:28 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_10_9500.pt (epoch 10 @ 9500 updates, score None) (writing took 1.0297961491160095 seconds)
2024-05-31 16:12:43 | INFO | train_inner | epoch 010:    411 / 1021 loss=6.506, nll_loss=5.426, ppl=42.98, wps=25173.7, ups=6.37, wpb=3951.1, bsz=154.6, num_updates=9600, lr=0.000322749, gnorm=0.696, train_wall=15, wall=1514
2024-05-31 16:12:58 | INFO | train_inner | epoch 010:    511 / 1021 loss=6.512, nll_loss=5.432, ppl=43.18, wps=26874.3, ups=6.81, wpb=3948.3, bsz=151.7, num_updates=9700, lr=0.000321081, gnorm=0.691, train_wall=15, wall=1528
2024-05-31 16:13:13 | INFO | train_inner | epoch 010:    611 / 1021 loss=6.526, nll_loss=5.448, ppl=43.66, wps=25762.1, ups=6.52, wpb=3953.1, bsz=160, num_updates=9800, lr=0.000319438, gnorm=0.718, train_wall=15, wall=1544
2024-05-31 16:13:28 | INFO | train_inner | epoch 010:    711 / 1021 loss=6.514, nll_loss=5.435, ppl=43.27, wps=25308.8, ups=6.41, wpb=3949.9, bsz=156.8, num_updates=9900, lr=0.000317821, gnorm=0.676, train_wall=15, wall=1559
2024-05-31 16:13:44 | INFO | train_inner | epoch 010:    811 / 1021 loss=6.527, nll_loss=5.45, ppl=43.71, wps=25097.4, ups=6.39, wpb=3929.3, bsz=164.8, num_updates=10000, lr=0.000316228, gnorm=0.744, train_wall=15, wall=1575
2024-05-31 16:13:44 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:13:45 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_10_10000.pt (epoch 10 @ 10000 updates, score None) (writing took 1.050120314117521 seconds)
2024-05-31 16:13:45 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-05-31 16:13:45 | INFO | train | epoch 010 | loss 6.508 | nll_loss 5.428 | ppl 43.06 | wps 25250.8 | ups 6.4 | wpb 3945.9 | bsz 155.6 | num_updates 10000 | lr 0.000316228 | gnorm 0.699 | train_wall 123 | wall 1576
2024-05-31 16:13:45 | INFO | fairseq_cli.train | done training in 1575.9 seconds
2024-05-31 16:13:47 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.sep.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=True, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=100000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=True, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=1000, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none')
2024-05-31 16:13:47 | INFO | fairseq.tasks.translation | [de] dictionary: 8 types
2024-05-31 16:13:47 | INFO | fairseq.tasks.translation | [en] dictionary: 9840 types
2024-05-31 16:13:47 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.sep.tokenized.de-en/valid.de-en.de
2024-05-31 16:13:47 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.sep.tokenized.de-en/valid.de-en.en
2024-05-31 16:13:47 | INFO | fairseq.tasks.translation | data-bin/iwslt14.sep.tokenized.de-en valid de-en 7283 examples
2024-05-31 16:13:48 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(9840, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=9840, bias=False)
  )
)
2024-05-31 16:13:48 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2024-05-31 16:13:48 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2024-05-31 16:13:48 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2024-05-31 16:13:48 | INFO | fairseq_cli.train | num. model params: 36585472 (num. trained: 36585472)
2024-05-31 16:13:51 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2024-05-31 16:13:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-05-31 16:13:51 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 11.910 GB ; name = TITAN Xp                                
2024-05-31 16:13:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-05-31 16:13:51 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-05-31 16:13:51 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None
2024-05-31 16:13:51 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2024-05-31 16:13:51 | INFO | fairseq.trainer | loading train data for epoch 1
2024-05-31 16:13:51 | INFO | fairseq.data.data_utils | loaded 160239 examples from: data-bin/iwslt14.sep.tokenized.de-en/train.de-en.de
2024-05-31 16:13:51 | INFO | fairseq.data.data_utils | loaded 160239 examples from: data-bin/iwslt14.sep.tokenized.de-en/train.de-en.en
2024-05-31 16:13:51 | INFO | fairseq.tasks.translation | data-bin/iwslt14.sep.tokenized.de-en train de-en 160239 examples
2024-05-31 16:13:51 | INFO | fairseq.trainer | begin training epoch 1
2024-05-31 16:14:08 | INFO | train_inner | epoch 001:    100 / 1021 loss=12.835, nll_loss=12.704, ppl=6670.89, wps=22879.5, ups=5.79, wpb=3953.2, bsz=150.9, num_updates=100, lr=1.25e-05, gnorm=3.112, train_wall=17, wall=18
2024-05-31 16:14:24 | INFO | train_inner | epoch 001:    200 / 1021 loss=10.942, nll_loss=10.59, ppl=1541.58, wps=24928.3, ups=6.33, wpb=3936.3, bsz=144.6, num_updates=200, lr=2.5e-05, gnorm=1.578, train_wall=16, wall=34
2024-05-31 16:14:40 | INFO | train_inner | epoch 001:    300 / 1021 loss=10.073, nll_loss=9.595, ppl=773.3, wps=24891.8, ups=6.35, wpb=3922.9, bsz=162.6, num_updates=300, lr=3.75e-05, gnorm=1.292, train_wall=16, wall=49
2024-05-31 16:14:56 | INFO | train_inner | epoch 001:    400 / 1021 loss=9.424, nll_loss=8.815, ppl=450.45, wps=25020.3, ups=6.34, wpb=3948.1, bsz=156.6, num_updates=400, lr=5e-05, gnorm=1.433, train_wall=16, wall=65
2024-05-31 16:15:12 | INFO | train_inner | epoch 001:    500 / 1021 loss=9.091, nll_loss=8.412, ppl=340.62, wps=24734.8, ups=6.26, wpb=3954, bsz=155.3, num_updates=500, lr=6.25e-05, gnorm=1.284, train_wall=16, wall=81
2024-05-31 16:15:29 | INFO | train_inner | epoch 001:    600 / 1021 loss=8.878, nll_loss=8.165, ppl=287, wps=23073.2, ups=5.88, wpb=3925.3, bsz=177.8, num_updates=600, lr=7.5e-05, gnorm=1.441, train_wall=17, wall=98
2024-05-31 16:15:45 | INFO | train_inner | epoch 001:    700 / 1021 loss=8.757, nll_loss=8.027, ppl=260.8, wps=23825.2, ups=6.02, wpb=3957.8, bsz=148.1, num_updates=700, lr=8.75e-05, gnorm=1.22, train_wall=16, wall=115
2024-05-31 16:16:01 | INFO | train_inner | epoch 001:    800 / 1021 loss=8.542, nll_loss=7.778, ppl=219.5, wps=24976.6, ups=6.31, wpb=3955.6, bsz=166.6, num_updates=800, lr=0.0001, gnorm=1.31, train_wall=16, wall=131
2024-05-31 16:16:17 | INFO | train_inner | epoch 001:    900 / 1021 loss=8.512, nll_loss=7.742, ppl=214.02, wps=24953.6, ups=6.36, wpb=3921.2, bsz=157.3, num_updates=900, lr=0.0001125, gnorm=1.456, train_wall=16, wall=146
2024-05-31 16:16:33 | INFO | train_inner | epoch 001:   1000 / 1021 loss=8.381, nll_loss=7.592, ppl=192.93, wps=24287.3, ups=6.16, wpb=3945.6, bsz=148.3, num_updates=1000, lr=0.000125, gnorm=1.135, train_wall=16, wall=162
2024-05-31 16:16:33 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:16:36 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score None) (writing took 2.411393743008375 seconds)
2024-05-31 16:16:39 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:16:39 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-05-31 16:16:39 | INFO | train | epoch 001 | loss 9.517 | nll_loss 8.912 | ppl 481.61 | wps 24002.6 | ups 6.09 | wpb 3943.1 | bsz 156.9 | num_updates 1021 | lr 0.000127625 | gnorm 1.517 | train_wall 164 | wall 168
2024-05-31 16:16:39 | INFO | fairseq.trainer | begin training epoch 2
/local/home/ggabriel/ma/alti/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2024-05-31 16:16:51 | INFO | train_inner | epoch 002:     79 / 1021 loss=8.266, nll_loss=7.458, ppl=175.8, wps=21574.7, ups=5.5, wpb=3920.9, bsz=146.8, num_updates=1100, lr=0.0001375, gnorm=1.162, train_wall=15, wall=181
2024-05-31 16:17:07 | INFO | train_inner | epoch 002:    179 / 1021 loss=8.147, nll_loss=7.318, ppl=159.59, wps=24868.6, ups=6.28, wpb=3958.5, bsz=170.2, num_updates=1200, lr=0.00015, gnorm=1.326, train_wall=16, wall=197
2024-05-31 16:17:23 | INFO | train_inner | epoch 002:    279 / 1021 loss=8.073, nll_loss=7.234, ppl=150.52, wps=25093.3, ups=6.34, wpb=3956.4, bsz=171.8, num_updates=1300, lr=0.0001625, gnorm=1.179, train_wall=16, wall=212
2024-05-31 16:17:39 | INFO | train_inner | epoch 002:    379 / 1021 loss=8.047, nll_loss=7.203, ppl=147.29, wps=25005.9, ups=6.38, wpb=3917.1, bsz=150.6, num_updates=1400, lr=0.000175, gnorm=1.096, train_wall=16, wall=228
2024-05-31 16:17:55 | INFO | train_inner | epoch 002:    479 / 1021 loss=7.96, nll_loss=7.103, ppl=137.43, wps=23806, ups=6.02, wpb=3957, bsz=146.1, num_updates=1500, lr=0.0001875, gnorm=1.057, train_wall=16, wall=245
2024-05-31 16:18:11 | INFO | train_inner | epoch 002:    579 / 1021 loss=7.89, nll_loss=7.022, ppl=129.93, wps=24866.6, ups=6.3, wpb=3948, bsz=170.4, num_updates=1600, lr=0.0002, gnorm=1.149, train_wall=16, wall=261
2024-05-31 16:18:27 | INFO | train_inner | epoch 002:    679 / 1021 loss=7.83, nll_loss=6.953, ppl=123.9, wps=25068.7, ups=6.3, wpb=3979.4, bsz=159.8, num_updates=1700, lr=0.0002125, gnorm=1.02, train_wall=16, wall=276
2024-05-31 16:18:43 | INFO | train_inner | epoch 002:    779 / 1021 loss=7.854, nll_loss=6.98, ppl=126.23, wps=24764.6, ups=6.31, wpb=3921.9, bsz=138.2, num_updates=1800, lr=0.000225, gnorm=1.045, train_wall=16, wall=292
2024-05-31 16:18:59 | INFO | train_inner | epoch 002:    879 / 1021 loss=7.8, nll_loss=6.918, ppl=120.9, wps=25034.4, ups=6.35, wpb=3944.2, bsz=158.9, num_updates=1900, lr=0.0002375, gnorm=1.052, train_wall=16, wall=308
2024-05-31 16:19:14 | INFO | train_inner | epoch 002:    979 / 1021 loss=7.725, nll_loss=6.832, ppl=113.93, wps=25124, ups=6.36, wpb=3951.6, bsz=175.5, num_updates=2000, lr=0.00025, gnorm=1.103, train_wall=16, wall=324
2024-05-31 16:19:14 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:19:17 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score None) (writing took 2.6740832421928644 seconds)
2024-05-31 16:19:24 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:19:24 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-05-31 16:19:24 | INFO | train | epoch 002 | loss 7.945 | nll_loss 7.085 | ppl 135.77 | wps 24441.4 | ups 6.2 | wpb 3943.1 | bsz 156.9 | num_updates 2042 | lr 0.00025525 | gnorm 1.106 | train_wall 160 | wall 333
2024-05-31 16:19:24 | INFO | fairseq.trainer | begin training epoch 3
2024-05-31 16:19:33 | INFO | train_inner | epoch 003:     58 / 1021 loss=7.713, nll_loss=6.817, ppl=112.74, wps=21125.5, ups=5.4, wpb=3914.2, bsz=131.8, num_updates=2100, lr=0.0002625, gnorm=0.884, train_wall=16, wall=342
2024-05-31 16:19:49 | INFO | train_inner | epoch 003:    158 / 1021 loss=7.609, nll_loss=6.698, ppl=103.86, wps=25117.3, ups=6.32, wpb=3973.3, bsz=161, num_updates=2200, lr=0.000275, gnorm=0.903, train_wall=16, wall=358
2024-05-31 16:20:04 | INFO | train_inner | epoch 003:    258 / 1021 loss=7.585, nll_loss=6.67, ppl=101.81, wps=25062.8, ups=6.32, wpb=3963.5, bsz=151, num_updates=2300, lr=0.0002875, gnorm=0.898, train_wall=16, wall=374
2024-05-31 16:20:20 | INFO | train_inner | epoch 003:    358 / 1021 loss=7.596, nll_loss=6.683, ppl=102.73, wps=25296.8, ups=6.41, wpb=3947.6, bsz=141.7, num_updates=2400, lr=0.0003, gnorm=0.871, train_wall=15, wall=389
2024-05-31 16:20:36 | INFO | train_inner | epoch 003:    458 / 1021 loss=7.537, nll_loss=6.616, ppl=98.08, wps=24988.5, ups=6.34, wpb=3944, bsz=156.2, num_updates=2500, lr=0.0003125, gnorm=0.896, train_wall=16, wall=405
2024-05-31 16:20:52 | INFO | train_inner | epoch 003:    558 / 1021 loss=7.518, nll_loss=6.594, ppl=96.58, wps=24956.4, ups=6.36, wpb=3924.3, bsz=171.6, num_updates=2600, lr=0.000325, gnorm=0.954, train_wall=16, wall=421
2024-05-31 16:21:07 | INFO | train_inner | epoch 003:    658 / 1021 loss=7.476, nll_loss=6.546, ppl=93.42, wps=25110.2, ups=6.37, wpb=3941.1, bsz=161, num_updates=2700, lr=0.0003375, gnorm=0.818, train_wall=16, wall=437
2024-05-31 16:21:23 | INFO | train_inner | epoch 003:    758 / 1021 loss=7.464, nll_loss=6.532, ppl=92.54, wps=25058.2, ups=6.36, wpb=3937.5, bsz=149.5, num_updates=2800, lr=0.00035, gnorm=0.813, train_wall=16, wall=452
2024-05-31 16:21:39 | INFO | train_inner | epoch 003:    858 / 1021 loss=7.429, nll_loss=6.492, ppl=89.99, wps=24967.4, ups=6.34, wpb=3937.9, bsz=167.2, num_updates=2900, lr=0.0003625, gnorm=0.867, train_wall=16, wall=468
2024-05-31 16:21:55 | INFO | train_inner | epoch 003:    958 / 1021 loss=7.416, nll_loss=6.476, ppl=89.05, wps=25047.2, ups=6.34, wpb=3953.7, bsz=155.7, num_updates=3000, lr=0.000375, gnorm=0.801, train_wall=16, wall=484
2024-05-31 16:21:55 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:21:57 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_3_3000.pt (epoch 3 @ 3000 updates, score None) (writing took 2.6148498109541833 seconds)
2024-05-31 16:22:07 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:22:07 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-05-31 16:22:07 | INFO | train | epoch 003 | loss 7.515 | nll_loss 6.59 | ppl 96.36 | wps 24619.4 | ups 6.24 | wpb 3943.1 | bsz 156.9 | num_updates 3063 | lr 0.000382875 | gnorm 0.874 | train_wall 159 | wall 496
2024-05-31 16:22:07 | INFO | fairseq.trainer | begin training epoch 4
2024-05-31 16:22:13 | INFO | train_inner | epoch 004:     37 / 1021 loss=7.357, nll_loss=6.411, ppl=85.07, wps=21277.2, ups=5.38, wpb=3951.6, bsz=175.4, num_updates=3100, lr=0.0003875, gnorm=0.86, train_wall=16, wall=503
2024-05-31 16:22:29 | INFO | train_inner | epoch 004:    137 / 1021 loss=7.387, nll_loss=6.444, ppl=87.05, wps=25254.3, ups=6.45, wpb=3914.3, bsz=137.2, num_updates=3200, lr=0.0004, gnorm=0.763, train_wall=15, wall=518
2024-05-31 16:22:45 | INFO | train_inner | epoch 004:    237 / 1021 loss=7.302, nll_loss=6.346, ppl=81.33, wps=24978.7, ups=6.3, wpb=3965, bsz=149.9, num_updates=3300, lr=0.0004125, gnorm=0.757, train_wall=16, wall=534
2024-05-31 16:23:01 | INFO | train_inner | epoch 004:    337 / 1021 loss=7.323, nll_loss=6.37, ppl=82.69, wps=24381.4, ups=6.23, wpb=3914.4, bsz=140.1, num_updates=3400, lr=0.000425, gnorm=0.745, train_wall=16, wall=550
2024-05-31 16:23:16 | INFO | train_inner | epoch 004:    437 / 1021 loss=7.288, nll_loss=6.33, ppl=80.47, wps=25034.8, ups=6.35, wpb=3942.6, bsz=142.2, num_updates=3500, lr=0.0004375, gnorm=0.726, train_wall=16, wall=566
2024-05-31 16:23:32 | INFO | train_inner | epoch 004:    537 / 1021 loss=7.249, nll_loss=6.286, ppl=78.01, wps=24425.4, ups=6.2, wpb=3936.8, bsz=168.6, num_updates=3600, lr=0.00045, gnorm=0.788, train_wall=16, wall=582
2024-05-31 16:23:48 | INFO | train_inner | epoch 004:    637 / 1021 loss=7.253, nll_loss=6.29, ppl=78.25, wps=24666.1, ups=6.25, wpb=3945.3, bsz=157, num_updates=3700, lr=0.0004625, gnorm=0.764, train_wall=16, wall=598
2024-05-31 16:24:05 | INFO | train_inner | epoch 004:    737 / 1021 loss=7.189, nll_loss=6.217, ppl=74.4, wps=23820.8, ups=5.98, wpb=3980.3, bsz=175.8, num_updates=3800, lr=0.000475, gnorm=0.739, train_wall=17, wall=615
2024-05-31 16:24:21 | INFO | train_inner | epoch 004:    837 / 1021 loss=7.2, nll_loss=6.23, ppl=75.07, wps=24461.4, ups=6.23, wpb=3925.3, bsz=161.1, num_updates=3900, lr=0.0004875, gnorm=0.753, train_wall=16, wall=631
2024-05-31 16:24:37 | INFO | train_inner | epoch 004:    937 / 1021 loss=7.172, nll_loss=6.197, ppl=73.36, wps=24556, ups=6.22, wpb=3946, bsz=160.7, num_updates=4000, lr=0.0005, gnorm=0.715, train_wall=16, wall=647
2024-05-31 16:24:37 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:24:40 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_4_4000.pt (epoch 4 @ 4000 updates, score None) (writing took 2.608984366990626 seconds)
2024-05-31 16:24:53 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:24:53 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-05-31 16:24:53 | INFO | train | epoch 004 | loss 7.257 | nll_loss 6.294 | ppl 78.48 | wps 24223.3 | ups 6.14 | wpb 3943.1 | bsz 156.9 | num_updates 4084 | lr 0.000494831 | gnorm 0.751 | train_wall 162 | wall 663
2024-05-31 16:24:53 | INFO | fairseq.trainer | begin training epoch 5
2024-05-31 16:24:56 | INFO | train_inner | epoch 005:     16 / 1021 loss=7.151, nll_loss=6.174, ppl=72.22, wps=21010, ups=5.34, wpb=3935.5, bsz=165.9, num_updates=4100, lr=0.000493865, gnorm=0.72, train_wall=16, wall=665
2024-05-31 16:25:12 | INFO | train_inner | epoch 005:    116 / 1021 loss=7.105, nll_loss=6.121, ppl=69.59, wps=24871.4, ups=6.26, wpb=3972.7, bsz=169.6, num_updates=4200, lr=0.00048795, gnorm=0.717, train_wall=16, wall=681
2024-05-31 16:25:28 | INFO | train_inner | epoch 005:    216 / 1021 loss=7.088, nll_loss=6.101, ppl=68.63, wps=24953.6, ups=6.33, wpb=3941.8, bsz=154.9, num_updates=4300, lr=0.000482243, gnorm=0.71, train_wall=16, wall=697
2024-05-31 16:25:44 | INFO | train_inner | epoch 005:    316 / 1021 loss=7.051, nll_loss=6.058, ppl=66.62, wps=24925.4, ups=6.32, wpb=3942.8, bsz=164.4, num_updates=4400, lr=0.000476731, gnorm=0.698, train_wall=16, wall=713
2024-05-31 16:26:00 | INFO | train_inner | epoch 005:    416 / 1021 loss=7.071, nll_loss=6.081, ppl=67.69, wps=24227.6, ups=6.15, wpb=3936.5, bsz=145, num_updates=4500, lr=0.000471405, gnorm=0.667, train_wall=16, wall=729
2024-05-31 16:26:16 | INFO | train_inner | epoch 005:    516 / 1021 loss=7.032, nll_loss=6.036, ppl=65.62, wps=24625.3, ups=6.22, wpb=3961.5, bsz=148.5, num_updates=4600, lr=0.000466252, gnorm=0.663, train_wall=16, wall=745
2024-05-31 16:26:32 | INFO | train_inner | epoch 005:    616 / 1021 loss=7.013, nll_loss=6.014, ppl=64.63, wps=25176.7, ups=6.36, wpb=3960.4, bsz=161, num_updates=4700, lr=0.000461266, gnorm=0.675, train_wall=16, wall=761
2024-05-31 16:26:47 | INFO | train_inner | epoch 005:    716 / 1021 loss=7.013, nll_loss=6.014, ppl=64.63, wps=25193.4, ups=6.39, wpb=3943, bsz=163.3, num_updates=4800, lr=0.000456435, gnorm=0.699, train_wall=15, wall=777
2024-05-31 16:27:03 | INFO | train_inner | epoch 005:    816 / 1021 loss=7.001, nll_loss=6.001, ppl=64.05, wps=24904.9, ups=6.4, wpb=3894.2, bsz=162.1, num_updates=4900, lr=0.000451754, gnorm=0.681, train_wall=15, wall=792
2024-05-31 16:27:19 | INFO | train_inner | epoch 005:    916 / 1021 loss=7.032, nll_loss=6.036, ppl=65.61, wps=24763.4, ups=6.29, wpb=3935.6, bsz=138.1, num_updates=5000, lr=0.000447214, gnorm=0.666, train_wall=16, wall=808
2024-05-31 16:27:19 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:27:21 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_5_5000.pt (epoch 5 @ 5000 updates, score None) (writing took 2.639992241282016 seconds)
2024-05-31 16:27:37 | INFO | train_inner | epoch 005:   1016 / 1021 loss=6.955, nll_loss=5.949, ppl=61.77, wps=21380.6, ups=5.42, wpb=3943.4, bsz=163.4, num_updates=5100, lr=0.000442807, gnorm=0.659, train_wall=16, wall=827
2024-05-31 16:27:38 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:27:38 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-05-31 16:27:38 | INFO | train | epoch 005 | loss 7.037 | nll_loss 6.042 | ppl 65.89 | wps 24430.3 | ups 6.2 | wpb 3943.1 | bsz 156.9 | num_updates 5105 | lr 0.000442591 | gnorm 0.683 | train_wall 160 | wall 827
2024-05-31 16:27:38 | INFO | fairseq.trainer | begin training epoch 6
2024-05-31 16:27:54 | INFO | train_inner | epoch 006:     95 / 1021 loss=6.935, nll_loss=5.925, ppl=60.75, wps=24024.2, ups=6.12, wpb=3924.5, bsz=143.4, num_updates=5200, lr=0.000438529, gnorm=0.678, train_wall=16, wall=843
2024-05-31 16:28:10 | INFO | train_inner | epoch 006:    195 / 1021 loss=6.875, nll_loss=5.856, ppl=57.92, wps=24113.5, ups=6.16, wpb=3912, bsz=151, num_updates=5300, lr=0.000434372, gnorm=0.668, train_wall=16, wall=859
2024-05-31 16:28:26 | INFO | train_inner | epoch 006:    295 / 1021 loss=6.877, nll_loss=5.857, ppl=57.97, wps=24557.4, ups=6.22, wpb=3948.4, bsz=154.3, num_updates=5400, lr=0.000430331, gnorm=0.665, train_wall=16, wall=875
2024-05-31 16:28:41 | INFO | train_inner | epoch 006:    395 / 1021 loss=6.893, nll_loss=5.875, ppl=58.69, wps=25916.7, ups=6.59, wpb=3931.8, bsz=144.9, num_updates=5500, lr=0.000426401, gnorm=0.686, train_wall=15, wall=890
2024-05-31 16:28:56 | INFO | train_inner | epoch 006:    495 / 1021 loss=6.859, nll_loss=5.836, ppl=57.13, wps=26631.1, ups=6.75, wpb=3946.1, bsz=167.3, num_updates=5600, lr=0.000422577, gnorm=0.683, train_wall=15, wall=905
2024-05-31 16:29:14 | INFO | train_inner | epoch 006:    595 / 1021 loss=6.839, nll_loss=5.814, ppl=56.27, wps=21891, ups=5.55, wpb=3946.7, bsz=172.1, num_updates=5700, lr=0.000418854, gnorm=0.706, train_wall=18, wall=923
2024-05-31 16:29:32 | INFO | train_inner | epoch 006:    695 / 1021 loss=6.807, nll_loss=5.778, ppl=54.86, wps=22107, ups=5.55, wpb=3979.9, bsz=177.7, num_updates=5800, lr=0.000415227, gnorm=0.647, train_wall=18, wall=941
2024-05-31 16:29:48 | INFO | train_inner | epoch 006:    795 / 1021 loss=6.887, nll_loss=5.867, ppl=58.38, wps=24954, ups=6.35, wpb=3930.3, bsz=142.2, num_updates=5900, lr=0.000411693, gnorm=0.67, train_wall=16, wall=957
2024-05-31 16:30:04 | INFO | train_inner | epoch 006:    895 / 1021 loss=6.854, nll_loss=5.832, ppl=56.95, wps=24906, ups=6.32, wpb=3940, bsz=159.1, num_updates=6000, lr=0.000408248, gnorm=0.673, train_wall=16, wall=973
2024-05-31 16:30:04 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:30:06 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_6_6000.pt (epoch 6 @ 6000 updates, score None) (writing took 2.70754560129717 seconds)
2024-05-31 16:30:22 | INFO | train_inner | epoch 006:    995 / 1021 loss=6.833, nll_loss=5.807, ppl=56, wps=21385.2, ups=5.4, wpb=3962.2, bsz=157.9, num_updates=6100, lr=0.000404888, gnorm=0.652, train_wall=16, wall=991
2024-05-31 16:30:26 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:30:26 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2024-05-31 16:30:26 | INFO | train | epoch 006 | loss 6.864 | nll_loss 5.842 | ppl 57.37 | wps 23950.1 | ups 6.07 | wpb 3943.1 | bsz 156.9 | num_updates 6126 | lr 0.000404028 | gnorm 0.671 | train_wall 164 | wall 996
2024-05-31 16:30:26 | INFO | fairseq.trainer | begin training epoch 7
2024-05-31 16:30:38 | INFO | train_inner | epoch 007:     74 / 1021 loss=6.747, nll_loss=5.709, ppl=52.31, wps=24302.6, ups=6.14, wpb=3960.7, bsz=153.4, num_updates=6200, lr=0.00040161, gnorm=0.641, train_wall=16, wall=1008
2024-05-31 16:30:54 | INFO | train_inner | epoch 007:    174 / 1021 loss=6.745, nll_loss=5.705, ppl=52.17, wps=24503.9, ups=6.26, wpb=3917.4, bsz=156.8, num_updates=6300, lr=0.00039841, gnorm=0.7, train_wall=16, wall=1024
2024-05-31 16:31:10 | INFO | train_inner | epoch 007:    274 / 1021 loss=6.742, nll_loss=5.701, ppl=52.03, wps=24638.7, ups=6.21, wpb=3967.4, bsz=162.5, num_updates=6400, lr=0.000395285, gnorm=0.67, train_wall=16, wall=1040
2024-05-31 16:31:26 | INFO | train_inner | epoch 007:    374 / 1021 loss=6.748, nll_loss=5.708, ppl=52.26, wps=24612.8, ups=6.24, wpb=3944, bsz=155.4, num_updates=6500, lr=0.000392232, gnorm=0.669, train_wall=16, wall=1056
2024-05-31 16:31:42 | INFO | train_inner | epoch 007:    474 / 1021 loss=6.757, nll_loss=5.718, ppl=52.63, wps=24839.1, ups=6.27, wpb=3959.3, bsz=158.6, num_updates=6600, lr=0.000389249, gnorm=0.678, train_wall=16, wall=1072
2024-05-31 16:31:58 | INFO | train_inner | epoch 007:    574 / 1021 loss=6.79, nll_loss=5.756, ppl=54.04, wps=24586.3, ups=6.32, wpb=3891.2, bsz=145.5, num_updates=6700, lr=0.000386334, gnorm=0.687, train_wall=16, wall=1088
2024-05-31 16:32:14 | INFO | train_inner | epoch 007:    674 / 1021 loss=6.749, nll_loss=5.709, ppl=52.31, wps=24645.9, ups=6.25, wpb=3946.1, bsz=159, num_updates=6800, lr=0.000383482, gnorm=0.664, train_wall=16, wall=1104
2024-05-31 16:32:30 | INFO | train_inner | epoch 007:    774 / 1021 loss=6.735, nll_loss=5.694, ppl=51.75, wps=24583.7, ups=6.26, wpb=3928.1, bsz=160.2, num_updates=6900, lr=0.000380693, gnorm=0.67, train_wall=16, wall=1120
2024-05-31 16:32:46 | INFO | train_inner | epoch 007:    874 / 1021 loss=6.741, nll_loss=5.7, ppl=51.97, wps=24586.5, ups=6.2, wpb=3965.5, bsz=145.6, num_updates=7000, lr=0.000377964, gnorm=0.649, train_wall=16, wall=1136
2024-05-31 16:32:46 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:32:49 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_7_7000.pt (epoch 7 @ 7000 updates, score None) (writing took 2.7485645660199225 seconds)
2024-05-31 16:33:05 | INFO | train_inner | epoch 007:    974 / 1021 loss=6.696, nll_loss=5.649, ppl=50.16, wps=21037.1, ups=5.32, wpb=3954.9, bsz=170.9, num_updates=7100, lr=0.000375293, gnorm=0.678, train_wall=16, wall=1155
2024-05-31 16:33:13 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:33:13 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-05-31 16:33:13 | INFO | train | epoch 007 | loss 6.742 | nll_loss 5.701 | ppl 52.02 | wps 24175.5 | ups 6.13 | wpb 3943.1 | bsz 156.9 | num_updates 7147 | lr 0.000374057 | gnorm 0.671 | train_wall 162 | wall 1162
2024-05-31 16:33:13 | INFO | fairseq.trainer | begin training epoch 8
2024-05-31 16:33:21 | INFO | train_inner | epoch 008:     53 / 1021 loss=6.698, nll_loss=5.651, ppl=50.24, wps=24132.8, ups=6.18, wpb=3904.3, bsz=149.9, num_updates=7200, lr=0.000372678, gnorm=0.687, train_wall=16, wall=1171
2024-05-31 16:33:38 | INFO | train_inner | epoch 008:    153 / 1021 loss=6.608, nll_loss=5.548, ppl=46.78, wps=24458.3, ups=6.16, wpb=3972.6, bsz=178.8, num_updates=7300, lr=0.000370117, gnorm=0.662, train_wall=16, wall=1187
2024-05-31 16:33:54 | INFO | train_inner | epoch 008:    253 / 1021 loss=6.665, nll_loss=5.61, ppl=48.85, wps=24531.9, ups=6.22, wpb=3944.7, bsz=136.1, num_updates=7400, lr=0.000367607, gnorm=0.656, train_wall=16, wall=1203
2024-05-31 16:34:10 | INFO | train_inner | epoch 008:    353 / 1021 loss=6.64, nll_loss=5.583, ppl=47.93, wps=24587.8, ups=6.2, wpb=3963.8, bsz=181.4, num_updates=7500, lr=0.000365148, gnorm=0.7, train_wall=16, wall=1219
2024-05-31 16:34:26 | INFO | train_inner | epoch 008:    453 / 1021 loss=6.637, nll_loss=5.579, ppl=47.81, wps=24480.3, ups=6.2, wpb=3948, bsz=154.9, num_updates=7600, lr=0.000362738, gnorm=0.663, train_wall=16, wall=1235
2024-05-31 16:34:42 | INFO | train_inner | epoch 008:    553 / 1021 loss=6.642, nll_loss=5.584, ppl=47.98, wps=24758.5, ups=6.25, wpb=3961.7, bsz=152.5, num_updates=7700, lr=0.000360375, gnorm=0.655, train_wall=16, wall=1251
2024-05-31 16:34:58 | INFO | train_inner | epoch 008:    653 / 1021 loss=6.657, nll_loss=5.602, ppl=48.57, wps=24791.3, ups=6.29, wpb=3944.4, bsz=164.3, num_updates=7800, lr=0.000358057, gnorm=0.689, train_wall=16, wall=1267
2024-05-31 16:35:14 | INFO | train_inner | epoch 008:    753 / 1021 loss=6.662, nll_loss=5.607, ppl=48.76, wps=24538.1, ups=6.24, wpb=3931.9, bsz=139.4, num_updates=7900, lr=0.000355784, gnorm=0.662, train_wall=16, wall=1283
2024-05-31 16:35:30 | INFO | train_inner | epoch 008:    853 / 1021 loss=6.64, nll_loss=5.583, ppl=47.93, wps=24733.2, ups=6.3, wpb=3924.1, bsz=157.3, num_updates=8000, lr=0.000353553, gnorm=0.675, train_wall=16, wall=1299
2024-05-31 16:35:30 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:35:32 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_8_8000.pt (epoch 8 @ 8000 updates, score None) (writing took 2.7108618039637804 seconds)
2024-05-31 16:35:48 | INFO | train_inner | epoch 008:    953 / 1021 loss=6.643, nll_loss=5.586, ppl=48.02, wps=21081.5, ups=5.34, wpb=3946.2, bsz=161.8, num_updates=8100, lr=0.000351364, gnorm=0.718, train_wall=16, wall=1318
2024-05-31 16:35:59 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:35:59 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-05-31 16:35:59 | INFO | train | epoch 008 | loss 6.648 | nll_loss 5.591 | ppl 48.21 | wps 24132.8 | ups 6.12 | wpb 3943.1 | bsz 156.9 | num_updates 8168 | lr 0.000349899 | gnorm 0.676 | train_wall 162 | wall 1329
2024-05-31 16:36:00 | INFO | fairseq.trainer | begin training epoch 9
2024-05-31 16:36:05 | INFO | train_inner | epoch 009:     32 / 1021 loss=6.636, nll_loss=5.578, ppl=47.77, wps=23808.8, ups=6.07, wpb=3920.5, bsz=144.3, num_updates=8200, lr=0.000349215, gnorm=0.666, train_wall=16, wall=1334
2024-05-31 16:36:21 | INFO | train_inner | epoch 009:    132 / 1021 loss=6.56, nll_loss=5.491, ppl=44.97, wps=24416.9, ups=6.17, wpb=3957.4, bsz=157.4, num_updates=8300, lr=0.000347105, gnorm=0.687, train_wall=16, wall=1350
2024-05-31 16:36:37 | INFO | train_inner | epoch 009:    232 / 1021 loss=6.582, nll_loss=5.516, ppl=45.74, wps=24847.6, ups=6.32, wpb=3932.2, bsz=149.2, num_updates=8400, lr=0.000345033, gnorm=0.673, train_wall=16, wall=1366
2024-05-31 16:36:53 | INFO | train_inner | epoch 009:    332 / 1021 loss=6.539, nll_loss=5.465, ppl=44.18, wps=24935.3, ups=6.29, wpb=3964.9, bsz=163.8, num_updates=8500, lr=0.000342997, gnorm=0.674, train_wall=16, wall=1382
2024-05-31 16:37:09 | INFO | train_inner | epoch 009:    432 / 1021 loss=6.556, nll_loss=5.486, ppl=44.82, wps=24878.4, ups=6.3, wpb=3950.1, bsz=168.8, num_updates=8600, lr=0.000340997, gnorm=0.689, train_wall=16, wall=1398
2024-05-31 16:37:25 | INFO | train_inner | epoch 009:    532 / 1021 loss=6.593, nll_loss=5.527, ppl=46.1, wps=24859.8, ups=6.31, wpb=3939.2, bsz=144.9, num_updates=8700, lr=0.000339032, gnorm=0.672, train_wall=16, wall=1414
2024-05-31 16:37:40 | INFO | train_inner | epoch 009:    632 / 1021 loss=6.577, nll_loss=5.508, ppl=45.51, wps=24782.7, ups=6.28, wpb=3943.5, bsz=168.6, num_updates=8800, lr=0.0003371, gnorm=0.702, train_wall=16, wall=1430
2024-05-31 16:37:56 | INFO | train_inner | epoch 009:    732 / 1021 loss=6.594, nll_loss=5.529, ppl=46.16, wps=24706.4, ups=6.3, wpb=3919.8, bsz=154.6, num_updates=8900, lr=0.000335201, gnorm=0.706, train_wall=16, wall=1446
2024-05-31 16:38:12 | INFO | train_inner | epoch 009:    832 / 1021 loss=6.585, nll_loss=5.517, ppl=45.78, wps=24614.6, ups=6.25, wpb=3940.2, bsz=156.7, num_updates=9000, lr=0.000333333, gnorm=0.716, train_wall=16, wall=1462
2024-05-31 16:38:12 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:38:15 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_9_9000.pt (epoch 9 @ 9000 updates, score None) (writing took 2.7656932431273162 seconds)
2024-05-31 16:38:31 | INFO | train_inner | epoch 009:    932 / 1021 loss=6.575, nll_loss=5.506, ppl=45.45, wps=21113.2, ups=5.34, wpb=3953.2, bsz=145.8, num_updates=9100, lr=0.000331497, gnorm=0.663, train_wall=16, wall=1480
2024-05-31 16:38:45 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:38:45 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2024-05-31 16:38:45 | INFO | train | epoch 009 | loss 6.574 | nll_loss 5.506 | ppl 45.45 | wps 24289 | ups 6.16 | wpb 3943.1 | bsz 156.9 | num_updates 9189 | lr 0.000329888 | gnorm 0.688 | train_wall 161 | wall 1495
2024-05-31 16:38:45 | INFO | fairseq.trainer | begin training epoch 10
2024-05-31 16:38:47 | INFO | train_inner | epoch 010:     11 / 1021 loss=6.585, nll_loss=5.518, ppl=45.82, wps=24473, ups=6.22, wpb=3936.8, bsz=157.9, num_updates=9200, lr=0.00032969, gnorm=0.699, train_wall=16, wall=1497
2024-05-31 16:39:03 | INFO | train_inner | epoch 010:    111 / 1021 loss=6.456, nll_loss=5.371, ppl=41.38, wps=24873.2, ups=6.27, wpb=3965.1, bsz=167.2, num_updates=9300, lr=0.000327913, gnorm=0.699, train_wall=16, wall=1512
2024-05-31 16:39:19 | INFO | train_inner | epoch 010:    211 / 1021 loss=6.514, nll_loss=5.436, ppl=43.28, wps=24882, ups=6.33, wpb=3928.9, bsz=146.3, num_updates=9400, lr=0.000326164, gnorm=0.692, train_wall=16, wall=1528
2024-05-31 16:39:35 | INFO | train_inner | epoch 010:    311 / 1021 loss=6.504, nll_loss=5.424, ppl=42.93, wps=24964.3, ups=6.33, wpb=3945.6, bsz=149.3, num_updates=9500, lr=0.000324443, gnorm=0.682, train_wall=16, wall=1544
2024-05-31 16:39:51 | INFO | train_inner | epoch 010:    411 / 1021 loss=6.506, nll_loss=5.426, ppl=42.98, wps=24882.5, ups=6.3, wpb=3951.1, bsz=154.6, num_updates=9600, lr=0.000322749, gnorm=0.696, train_wall=16, wall=1560
2024-05-31 16:40:07 | INFO | train_inner | epoch 010:    511 / 1021 loss=6.512, nll_loss=5.432, ppl=43.18, wps=24697, ups=6.26, wpb=3948.3, bsz=151.7, num_updates=9700, lr=0.000321081, gnorm=0.691, train_wall=16, wall=1576
2024-05-31 16:40:23 | INFO | train_inner | epoch 010:    611 / 1021 loss=6.526, nll_loss=5.448, ppl=43.66, wps=24470.1, ups=6.19, wpb=3953.1, bsz=160, num_updates=9800, lr=0.000319438, gnorm=0.718, train_wall=16, wall=1592
2024-05-31 16:40:39 | INFO | train_inner | epoch 010:    711 / 1021 loss=6.514, nll_loss=5.435, ppl=43.27, wps=24558.8, ups=6.22, wpb=3949.9, bsz=156.8, num_updates=9900, lr=0.000317821, gnorm=0.676, train_wall=16, wall=1608
2024-05-31 16:40:55 | INFO | train_inner | epoch 010:    811 / 1021 loss=6.527, nll_loss=5.45, ppl=43.71, wps=24622.6, ups=6.27, wpb=3929.3, bsz=164.8, num_updates=10000, lr=0.000316228, gnorm=0.744, train_wall=16, wall=1624
2024-05-31 16:40:55 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:40:57 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_10_10000.pt (epoch 10 @ 10000 updates, score None) (writing took 2.7187480749562383 seconds)
2024-05-31 16:41:13 | INFO | train_inner | epoch 010:    911 / 1021 loss=6.56, nll_loss=5.487, ppl=44.85, wps=21071.5, ups=5.37, wpb=3923.5, bsz=146.2, num_updates=10100, lr=0.000314658, gnorm=0.703, train_wall=16, wall=1643
2024-05-31 16:41:29 | INFO | train_inner | epoch 010:   1011 / 1021 loss=6.494, nll_loss=5.414, ppl=42.63, wps=24705.6, ups=6.27, wpb=3940.6, bsz=178.3, num_updates=10200, lr=0.000313112, gnorm=0.715, train_wall=16, wall=1659
2024-05-31 16:41:31 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:41:31 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-05-31 16:41:31 | INFO | train | epoch 010 | loss 6.512 | nll_loss 5.433 | ppl 43.2 | wps 24302.6 | ups 6.16 | wpb 3943.1 | bsz 156.9 | num_updates 10210 | lr 0.000312959 | gnorm 0.701 | train_wall 161 | wall 1660
2024-05-31 16:41:31 | INFO | fairseq.trainer | begin training epoch 11
2024-05-31 16:41:45 | INFO | train_inner | epoch 011:     90 / 1021 loss=6.423, nll_loss=5.331, ppl=40.26, wps=24213, ups=6.19, wpb=3912.7, bsz=146.6, num_updates=10300, lr=0.000311588, gnorm=0.694, train_wall=16, wall=1675
2024-05-31 16:42:02 | INFO | train_inner | epoch 011:    190 / 1021 loss=6.448, nll_loss=5.358, ppl=41.02, wps=24324.7, ups=6.17, wpb=3945.3, bsz=146.2, num_updates=10400, lr=0.000310087, gnorm=0.705, train_wall=16, wall=1691
2024-05-31 16:42:18 | INFO | train_inner | epoch 011:    290 / 1021 loss=6.425, nll_loss=5.332, ppl=40.29, wps=24588.2, ups=6.2, wpb=3964.5, bsz=176.3, num_updates=10500, lr=0.000308607, gnorm=0.72, train_wall=16, wall=1707
2024-05-31 16:42:34 | INFO | train_inner | epoch 011:    390 / 1021 loss=6.45, nll_loss=5.36, ppl=41.07, wps=24227.6, ups=6.17, wpb=3927.8, bsz=158, num_updates=10600, lr=0.000307148, gnorm=0.715, train_wall=16, wall=1723
2024-05-31 16:42:50 | INFO | train_inner | epoch 011:    490 / 1021 loss=6.455, nll_loss=5.365, ppl=41.23, wps=24516.6, ups=6.21, wpb=3945.4, bsz=163.6, num_updates=10700, lr=0.000305709, gnorm=0.744, train_wall=16, wall=1740
2024-05-31 16:43:06 | INFO | train_inner | epoch 011:    590 / 1021 loss=6.46, nll_loss=5.373, ppl=41.43, wps=24676.5, ups=6.25, wpb=3947.7, bsz=171.3, num_updates=10800, lr=0.00030429, gnorm=0.704, train_wall=16, wall=1756
2024-05-31 16:43:22 | INFO | train_inner | epoch 011:    690 / 1021 loss=6.455, nll_loss=5.366, ppl=41.24, wps=24623, ups=6.22, wpb=3957.5, bsz=164.6, num_updates=10900, lr=0.000302891, gnorm=0.703, train_wall=16, wall=1772
2024-05-31 16:43:38 | INFO | train_inner | epoch 011:    790 / 1021 loss=6.514, nll_loss=5.432, ppl=43.17, wps=24655.1, ups=6.27, wpb=3932.8, bsz=136.1, num_updates=11000, lr=0.000301511, gnorm=0.702, train_wall=16, wall=1788
2024-05-31 16:43:38 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:43:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_11_11000.pt (epoch 11 @ 11000 updates, score None) (writing took 1.0318165719509125 seconds)
2024-05-31 16:43:54 | INFO | train_inner | epoch 011:    890 / 1021 loss=6.498, nll_loss=5.414, ppl=42.65, wps=24489.2, ups=6.21, wpb=3940.7, bsz=153.8, num_updates=11100, lr=0.00030015, gnorm=0.731, train_wall=15, wall=1804
2024-05-31 16:44:10 | INFO | train_inner | epoch 011:    990 / 1021 loss=6.476, nll_loss=5.39, ppl=41.94, wps=25087.8, ups=6.35, wpb=3949.8, bsz=149.2, num_updates=11200, lr=0.000298807, gnorm=0.686, train_wall=16, wall=1819
2024-05-31 16:44:15 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:44:15 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2024-05-31 16:44:15 | INFO | train | epoch 011 | loss 6.459 | nll_loss 5.371 | ppl 41.37 | wps 24538.1 | ups 6.22 | wpb 3943.1 | bsz 156.9 | num_updates 11231 | lr 0.000298394 | gnorm 0.71 | train_wall 161 | wall 1824
2024-05-31 16:44:15 | INFO | fairseq.trainer | begin training epoch 12
2024-05-31 16:44:26 | INFO | train_inner | epoch 012:     69 / 1021 loss=6.417, nll_loss=5.322, ppl=40, wps=24310.1, ups=6.22, wpb=3911.2, bsz=140.6, num_updates=11300, lr=0.000297482, gnorm=0.744, train_wall=16, wall=1835
2024-05-31 16:44:42 | INFO | train_inner | epoch 012:    169 / 1021 loss=6.379, nll_loss=5.278, ppl=38.81, wps=24700.5, ups=6.29, wpb=3928.6, bsz=163, num_updates=11400, lr=0.000296174, gnorm=0.727, train_wall=16, wall=1851
2024-05-31 16:44:58 | INFO | train_inner | epoch 012:    269 / 1021 loss=6.38, nll_loss=5.28, ppl=38.86, wps=24742.5, ups=6.23, wpb=3974.3, bsz=179.4, num_updates=11500, lr=0.000294884, gnorm=0.726, train_wall=16, wall=1867
2024-05-31 16:45:14 | INFO | train_inner | epoch 012:    369 / 1021 loss=6.408, nll_loss=5.311, ppl=39.69, wps=24518.6, ups=6.23, wpb=3933.4, bsz=160.2, num_updates=11600, lr=0.00029361, gnorm=0.728, train_wall=16, wall=1883
2024-05-31 16:45:30 | INFO | train_inner | epoch 012:    469 / 1021 loss=6.409, nll_loss=5.313, ppl=39.74, wps=24737.6, ups=6.26, wpb=3949.6, bsz=164.8, num_updates=11700, lr=0.000292353, gnorm=0.72, train_wall=16, wall=1899
2024-05-31 16:45:46 | INFO | train_inner | epoch 012:    569 / 1021 loss=6.428, nll_loss=5.334, ppl=40.33, wps=24758.8, ups=6.28, wpb=3942.7, bsz=146.5, num_updates=11800, lr=0.000291111, gnorm=0.713, train_wall=16, wall=1915
2024-05-31 16:46:02 | INFO | train_inner | epoch 012:    669 / 1021 loss=6.42, nll_loss=5.324, ppl=40.06, wps=24739.8, ups=6.27, wpb=3945.1, bsz=151.5, num_updates=11900, lr=0.000289886, gnorm=0.719, train_wall=16, wall=1931
2024-05-31 16:46:18 | INFO | train_inner | epoch 012:    769 / 1021 loss=6.418, nll_loss=5.322, ppl=40.01, wps=24388.3, ups=6.15, wpb=3966.4, bsz=170, num_updates=12000, lr=0.000288675, gnorm=0.736, train_wall=16, wall=1948
2024-05-31 16:46:18 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:46:19 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_12_12000.pt (epoch 12 @ 12000 updates, score None) (writing took 1.0520114949904382 seconds)
2024-05-31 16:46:35 | INFO | train_inner | epoch 012:    869 / 1021 loss=6.437, nll_loss=5.344, ppl=40.6, wps=23549.6, ups=6, wpb=3926.2, bsz=153, num_updates=12100, lr=0.00028748, gnorm=0.729, train_wall=15, wall=1964
2024-05-31 16:46:51 | INFO | train_inner | epoch 012:    969 / 1021 loss=6.427, nll_loss=5.333, ppl=40.29, wps=25215.7, ups=6.39, wpb=3944.3, bsz=142.1, num_updates=12200, lr=0.000286299, gnorm=0.694, train_wall=15, wall=1980
2024-05-31 16:46:59 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:46:59 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2024-05-31 16:46:59 | INFO | train | epoch 012 | loss 6.414 | nll_loss 5.318 | ppl 39.89 | wps 24563.2 | ups 6.23 | wpb 3943.1 | bsz 156.9 | num_updates 12252 | lr 0.000285691 | gnorm 0.728 | train_wall 161 | wall 1988
2024-05-31 16:46:59 | INFO | fairseq.trainer | begin training epoch 13
2024-05-31 16:47:07 | INFO | train_inner | epoch 013:     48 / 1021 loss=6.388, nll_loss=5.288, ppl=39.07, wps=24399.6, ups=6.15, wpb=3964.4, bsz=166.6, num_updates=12300, lr=0.000285133, gnorm=0.758, train_wall=16, wall=1996
2024-05-31 16:47:23 | INFO | train_inner | epoch 013:    148 / 1021 loss=6.338, nll_loss=5.23, ppl=37.52, wps=24532.4, ups=6.25, wpb=3926.5, bsz=141, num_updates=12400, lr=0.000283981, gnorm=0.708, train_wall=16, wall=2012
2024-05-31 16:47:39 | INFO | train_inner | epoch 013:    248 / 1021 loss=6.359, nll_loss=5.254, ppl=38.15, wps=24702.8, ups=6.24, wpb=3956.2, bsz=162.7, num_updates=12500, lr=0.000282843, gnorm=0.761, train_wall=16, wall=2028
2024-05-31 16:47:55 | INFO | train_inner | epoch 013:    348 / 1021 loss=6.389, nll_loss=5.288, ppl=39.06, wps=24734.2, ups=6.29, wpb=3930.6, bsz=146.9, num_updates=12600, lr=0.000281718, gnorm=0.723, train_wall=16, wall=2044
2024-05-31 16:48:11 | INFO | train_inner | epoch 013:    448 / 1021 loss=6.349, nll_loss=5.242, ppl=37.86, wps=24526.1, ups=6.21, wpb=3948.5, bsz=175.7, num_updates=12700, lr=0.000280607, gnorm=0.775, train_wall=16, wall=2060
2024-05-31 16:48:27 | INFO | train_inner | epoch 013:    548 / 1021 loss=6.372, nll_loss=5.269, ppl=38.55, wps=24267, ups=6.15, wpb=3944.8, bsz=175, num_updates=12800, lr=0.000279508, gnorm=0.752, train_wall=16, wall=2076
2024-05-31 16:48:43 | INFO | train_inner | epoch 013:    648 / 1021 loss=6.364, nll_loss=5.259, ppl=38.3, wps=24310.6, ups=6.16, wpb=3945.3, bsz=147.5, num_updates=12900, lr=0.000278423, gnorm=0.711, train_wall=16, wall=2093
2024-05-31 16:48:59 | INFO | train_inner | epoch 013:    748 / 1021 loss=6.384, nll_loss=5.282, ppl=38.9, wps=24340.7, ups=6.17, wpb=3943.9, bsz=153, num_updates=13000, lr=0.00027735, gnorm=0.732, train_wall=16, wall=2109
2024-05-31 16:48:59 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:49:00 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_13_13000.pt (epoch 13 @ 13000 updates, score None) (writing took 1.0271707200445235 seconds)
2024-05-31 16:49:16 | INFO | train_inner | epoch 013:    848 / 1021 loss=6.389, nll_loss=5.287, ppl=39.04, wps=24307.7, ups=6.16, wpb=3944.4, bsz=152.6, num_updates=13100, lr=0.000276289, gnorm=0.732, train_wall=15, wall=2125
2024-05-31 16:49:31 | INFO | train_inner | epoch 013:    948 / 1021 loss=6.409, nll_loss=5.311, ppl=39.69, wps=25877.6, ups=6.59, wpb=3926.5, bsz=148.7, num_updates=13200, lr=0.000275241, gnorm=0.724, train_wall=15, wall=2140
2024-05-31 16:49:42 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:49:42 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2024-05-31 16:49:42 | INFO | train | epoch 013 | loss 6.372 | nll_loss 5.268 | ppl 38.54 | wps 24671.5 | ups 6.26 | wpb 3943.1 | bsz 156.9 | num_updates 13273 | lr 0.000274483 | gnorm 0.734 | train_wall 160 | wall 2151
2024-05-31 16:49:42 | INFO | fairseq.trainer | begin training epoch 14
2024-05-31 16:49:47 | INFO | train_inner | epoch 014:     27 / 1021 loss=6.368, nll_loss=5.265, ppl=38.45, wps=25122.6, ups=6.38, wpb=3937, bsz=158.6, num_updates=13300, lr=0.000274204, gnorm=0.735, train_wall=15, wall=2156
2024-05-31 16:50:02 | INFO | train_inner | epoch 014:    127 / 1021 loss=6.296, nll_loss=5.181, ppl=36.29, wps=24735.7, ups=6.27, wpb=3948.2, bsz=158.2, num_updates=13400, lr=0.000273179, gnorm=0.745, train_wall=16, wall=2172
2024-05-31 16:50:18 | INFO | train_inner | epoch 014:    227 / 1021 loss=6.337, nll_loss=5.226, ppl=37.44, wps=24903.8, ups=6.34, wpb=3926.8, bsz=142.6, num_updates=13500, lr=0.000272166, gnorm=0.737, train_wall=16, wall=2188
2024-05-31 16:50:34 | INFO | train_inner | epoch 014:    327 / 1021 loss=6.313, nll_loss=5.199, ppl=36.73, wps=24941.3, ups=6.29, wpb=3967.2, bsz=151.1, num_updates=13600, lr=0.000271163, gnorm=0.719, train_wall=16, wall=2204
2024-05-31 16:50:50 | INFO | train_inner | epoch 014:    427 / 1021 loss=6.355, nll_loss=5.246, ppl=37.94, wps=24885.6, ups=6.33, wpb=3930.5, bsz=133.4, num_updates=13700, lr=0.000270172, gnorm=0.742, train_wall=16, wall=2219
2024-05-31 16:51:06 | INFO | train_inner | epoch 014:    527 / 1021 loss=6.338, nll_loss=5.228, ppl=37.48, wps=24767.6, ups=6.28, wpb=3946.5, bsz=167.1, num_updates=13800, lr=0.000269191, gnorm=0.762, train_wall=16, wall=2235
2024-05-31 16:51:22 | INFO | train_inner | epoch 014:    627 / 1021 loss=6.321, nll_loss=5.211, ppl=37.03, wps=24633.4, ups=6.28, wpb=3925.5, bsz=180.6, num_updates=13900, lr=0.000268221, gnorm=0.761, train_wall=16, wall=2251
2024-05-31 16:51:38 | INFO | train_inner | epoch 014:    727 / 1021 loss=6.348, nll_loss=5.239, ppl=37.76, wps=24371.3, ups=6.14, wpb=3968.2, bsz=151.6, num_updates=14000, lr=0.000267261, gnorm=0.734, train_wall=16, wall=2267
2024-05-31 16:51:38 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:51:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_14_14000.pt (epoch 14 @ 14000 updates, score None) (writing took 1.0364613700658083 seconds)
2024-05-31 16:51:55 | INFO | train_inner | epoch 014:    827 / 1021 loss=6.354, nll_loss=5.247, ppl=37.97, wps=23509.6, ups=5.97, wpb=3938.3, bsz=157.7, num_updates=14100, lr=0.000266312, gnorm=0.761, train_wall=16, wall=2284
2024-05-31 16:52:11 | INFO | train_inner | epoch 014:    927 / 1021 loss=6.357, nll_loss=5.249, ppl=38.03, wps=24692.9, ups=6.25, wpb=3950, bsz=156.5, num_updates=14200, lr=0.000265372, gnorm=0.767, train_wall=16, wall=2300
2024-05-31 16:52:26 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:52:26 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2024-05-31 16:52:26 | INFO | train | epoch 014 | loss 6.335 | nll_loss 5.224 | ppl 37.39 | wps 24577.7 | ups 6.23 | wpb 3943.1 | bsz 156.9 | num_updates 14294 | lr 0.000264498 | gnorm 0.749 | train_wall 161 | wall 2315
2024-05-31 16:52:26 | INFO | fairseq.trainer | begin training epoch 15
2024-05-31 16:52:27 | INFO | train_inner | epoch 015:      6 / 1021 loss=6.343, nll_loss=5.235, ppl=37.66, wps=24502.8, ups=6.2, wpb=3949.7, bsz=170.2, num_updates=14300, lr=0.000264443, gnorm=0.758, train_wall=16, wall=2316
2024-05-31 16:52:43 | INFO | train_inner | epoch 015:    106 / 1021 loss=6.272, nll_loss=5.152, ppl=35.56, wps=24722.9, ups=6.24, wpb=3961, bsz=159, num_updates=14400, lr=0.000263523, gnorm=0.739, train_wall=16, wall=2332
2024-05-31 16:52:59 | INFO | train_inner | epoch 015:    206 / 1021 loss=6.29, nll_loss=5.171, ppl=36.03, wps=24839.3, ups=6.28, wpb=3954.2, bsz=141.4, num_updates=14500, lr=0.000262613, gnorm=0.737, train_wall=16, wall=2348
2024-05-31 16:53:15 | INFO | train_inner | epoch 015:    306 / 1021 loss=6.302, nll_loss=5.186, ppl=36.39, wps=24807.6, ups=6.32, wpb=3926.4, bsz=164.1, num_updates=14600, lr=0.000261712, gnorm=0.795, train_wall=16, wall=2364
2024-05-31 16:53:31 | INFO | train_inner | epoch 015:    406 / 1021 loss=6.286, nll_loss=5.166, ppl=35.9, wps=24761.4, ups=6.28, wpb=3942, bsz=151.1, num_updates=14700, lr=0.00026082, gnorm=0.733, train_wall=16, wall=2380
2024-05-31 16:53:47 | INFO | train_inner | epoch 015:    506 / 1021 loss=6.291, nll_loss=5.173, ppl=36.07, wps=24901.9, ups=6.29, wpb=3960.1, bsz=171.2, num_updates=14800, lr=0.000259938, gnorm=0.788, train_wall=16, wall=2396
2024-05-31 16:54:02 | INFO | train_inner | epoch 015:    606 / 1021 loss=6.321, nll_loss=5.207, ppl=36.93, wps=24828.3, ups=6.3, wpb=3939.8, bsz=155.8, num_updates=14900, lr=0.000259064, gnorm=0.746, train_wall=16, wall=2412
2024-05-31 16:54:18 | INFO | train_inner | epoch 015:    706 / 1021 loss=6.324, nll_loss=5.209, ppl=37, wps=24894.9, ups=6.31, wpb=3947.8, bsz=147.5, num_updates=15000, lr=0.000258199, gnorm=0.755, train_wall=16, wall=2428
2024-05-31 16:54:18 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:54:19 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_15_15000.pt (epoch 15 @ 15000 updates, score None) (writing took 1.0214649341069162 seconds)
2024-05-31 16:54:35 | INFO | train_inner | epoch 015:    806 / 1021 loss=6.311, nll_loss=5.197, ppl=36.67, wps=23542.9, ups=5.97, wpb=3941.6, bsz=164.9, num_updates=15100, lr=0.000257343, gnorm=0.754, train_wall=16, wall=2444
2024-05-31 16:54:51 | INFO | train_inner | epoch 015:    906 / 1021 loss=6.302, nll_loss=5.186, ppl=36.39, wps=24630.2, ups=6.29, wpb=3916.8, bsz=159.3, num_updates=15200, lr=0.000256495, gnorm=0.753, train_wall=16, wall=2460
2024-05-31 16:55:07 | INFO | train_inner | epoch 015:   1006 / 1021 loss=6.314, nll_loss=5.2, ppl=36.75, wps=24551.3, ups=6.23, wpb=3940.3, bsz=158.2, num_updates=15300, lr=0.000255655, gnorm=0.774, train_wall=16, wall=2476
2024-05-31 16:55:09 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:55:09 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2024-05-31 16:55:09 | INFO | train | epoch 015 | loss 6.302 | nll_loss 5.185 | ppl 36.38 | wps 24606.9 | ups 6.24 | wpb 3943.1 | bsz 156.9 | num_updates 15315 | lr 0.00025553 | gnorm 0.756 | train_wall 161 | wall 2479
2024-05-31 16:55:10 | INFO | fairseq.trainer | begin training epoch 16
2024-05-31 16:55:23 | INFO | train_inner | epoch 016:     85 / 1021 loss=6.251, nll_loss=5.127, ppl=34.95, wps=23881, ups=6.11, wpb=3906.7, bsz=140.2, num_updates=15400, lr=0.000254824, gnorm=0.764, train_wall=16, wall=2493
2024-05-31 16:55:40 | INFO | train_inner | epoch 016:    185 / 1021 loss=6.257, nll_loss=5.133, ppl=35.08, wps=24295.5, ups=6.18, wpb=3928.9, bsz=137.4, num_updates=15500, lr=0.000254, gnorm=0.751, train_wall=16, wall=2509
2024-05-31 16:55:55 | INFO | train_inner | epoch 016:    285 / 1021 loss=6.267, nll_loss=5.144, ppl=35.36, wps=24766.2, ups=6.29, wpb=3939.3, bsz=149.9, num_updates=15600, lr=0.000253185, gnorm=0.758, train_wall=16, wall=2525
2024-05-31 16:56:11 | INFO | train_inner | epoch 016:    385 / 1021 loss=6.24, nll_loss=5.114, ppl=34.62, wps=24766.2, ups=6.26, wpb=3957.3, bsz=176.1, num_updates=15700, lr=0.000252377, gnorm=0.788, train_wall=16, wall=2541
2024-05-31 16:56:27 | INFO | train_inner | epoch 016:    485 / 1021 loss=6.273, nll_loss=5.151, ppl=35.52, wps=24745.3, ups=6.27, wpb=3946, bsz=154.2, num_updates=15800, lr=0.000251577, gnorm=0.757, train_wall=16, wall=2557
2024-05-31 16:56:43 | INFO | train_inner | epoch 016:    585 / 1021 loss=6.274, nll_loss=5.153, ppl=35.58, wps=24889, ups=6.28, wpb=3960.9, bsz=169.4, num_updates=15900, lr=0.000250785, gnorm=0.78, train_wall=16, wall=2573
2024-05-31 16:56:59 | INFO | train_inner | epoch 016:    685 / 1021 loss=6.27, nll_loss=5.146, ppl=35.4, wps=24840.9, ups=6.29, wpb=3949, bsz=148.5, num_updates=16000, lr=0.00025, gnorm=0.74, train_wall=16, wall=2589
2024-05-31 16:56:59 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:57:00 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_16_16000.pt (epoch 16 @ 16000 updates, score None) (writing took 1.0154584757983685 seconds)
2024-05-31 16:57:16 | INFO | train_inner | epoch 016:    785 / 1021 loss=6.303, nll_loss=5.185, ppl=36.38, wps=23692.3, ups=5.99, wpb=3958, bsz=161.8, num_updates=16100, lr=0.000249222, gnorm=0.814, train_wall=16, wall=2605
2024-05-31 16:57:32 | INFO | train_inner | epoch 016:    885 / 1021 loss=6.29, nll_loss=5.171, ppl=36.04, wps=24628.2, ups=6.25, wpb=3942.6, bsz=162.8, num_updates=16200, lr=0.000248452, gnorm=0.772, train_wall=16, wall=2621
2024-05-31 16:57:48 | INFO | train_inner | epoch 016:    985 / 1021 loss=6.293, nll_loss=5.174, ppl=36.11, wps=24594.6, ups=6.27, wpb=3921.3, bsz=161.2, num_updates=16300, lr=0.000247689, gnorm=0.787, train_wall=16, wall=2637
2024-05-31 16:57:54 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:57:54 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2024-05-31 16:57:54 | INFO | train | epoch 016 | loss 6.271 | nll_loss 5.149 | ppl 35.49 | wps 24513.3 | ups 6.22 | wpb 3943.1 | bsz 156.9 | num_updates 16336 | lr 0.000247416 | gnorm 0.771 | train_wall 161 | wall 2643
2024-05-31 16:57:54 | INFO | fairseq.trainer | begin training epoch 17
2024-05-31 16:58:04 | INFO | train_inner | epoch 017:     64 / 1021 loss=6.239, nll_loss=5.113, ppl=34.61, wps=24250.1, ups=6.13, wpb=3958.5, bsz=164.2, num_updates=16400, lr=0.000246932, gnorm=0.755, train_wall=16, wall=2654
2024-05-31 16:58:20 | INFO | train_inner | epoch 017:    164 / 1021 loss=6.188, nll_loss=5.054, ppl=33.22, wps=24596.3, ups=6.22, wpb=3956.3, bsz=175.7, num_updates=16500, lr=0.000246183, gnorm=0.815, train_wall=16, wall=2670
2024-05-31 16:58:36 | INFO | train_inner | epoch 017:    264 / 1021 loss=6.228, nll_loss=5.098, ppl=34.26, wps=24721, ups=6.24, wpb=3960.6, bsz=160.6, num_updates=16600, lr=0.00024544, gnorm=0.771, train_wall=16, wall=2686
2024-05-31 16:58:52 | INFO | train_inner | epoch 017:    364 / 1021 loss=6.239, nll_loss=5.111, ppl=34.56, wps=24750.9, ups=6.27, wpb=3949.7, bsz=155.4, num_updates=16700, lr=0.000244704, gnorm=0.775, train_wall=16, wall=2702
2024-05-31 16:59:08 | INFO | train_inner | epoch 017:    464 / 1021 loss=6.236, nll_loss=5.108, ppl=34.48, wps=24671.1, ups=6.23, wpb=3962.2, bsz=154.1, num_updates=16800, lr=0.000243975, gnorm=0.761, train_wall=16, wall=2718
2024-05-31 16:59:24 | INFO | train_inner | epoch 017:    564 / 1021 loss=6.252, nll_loss=5.125, ppl=34.9, wps=24666.2, ups=6.29, wpb=3924.2, bsz=151.8, num_updates=16900, lr=0.000243252, gnorm=0.781, train_wall=16, wall=2734
2024-05-31 16:59:40 | INFO | train_inner | epoch 017:    664 / 1021 loss=6.258, nll_loss=5.132, ppl=35.07, wps=24689.2, ups=6.27, wpb=3934.7, bsz=156, num_updates=17000, lr=0.000242536, gnorm=0.779, train_wall=16, wall=2750
2024-05-31 16:59:40 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 16:59:41 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_17_17000.pt (epoch 17 @ 17000 updates, score None) (writing took 1.0182529832236469 seconds)
2024-05-31 16:59:56 | INFO | train_inner | epoch 017:    764 / 1021 loss=6.244, nll_loss=5.118, ppl=34.72, wps=24703.9, ups=6.23, wpb=3962.8, bsz=164.2, num_updates=17100, lr=0.000241825, gnorm=0.769, train_wall=15, wall=2766
2024-05-31 17:00:12 | INFO | train_inner | epoch 017:    864 / 1021 loss=6.263, nll_loss=5.139, ppl=35.23, wps=25055.8, ups=6.39, wpb=3918.7, bsz=161.4, num_updates=17200, lr=0.000241121, gnorm=0.794, train_wall=15, wall=2781
2024-05-31 17:00:28 | INFO | train_inner | epoch 017:    964 / 1021 loss=6.284, nll_loss=5.162, ppl=35.8, wps=24604.9, ups=6.27, wpb=3925.1, bsz=143.8, num_updates=17300, lr=0.000240424, gnorm=0.775, train_wall=16, wall=2797
2024-05-31 17:00:37 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:00:37 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2024-05-31 17:00:37 | INFO | train | epoch 017 | loss 6.244 | nll_loss 5.117 | ppl 34.7 | wps 24646.4 | ups 6.25 | wpb 3943.1 | bsz 156.9 | num_updates 17357 | lr 0.000240028 | gnorm 0.778 | train_wall 161 | wall 2806
2024-05-31 17:00:37 | INFO | fairseq.trainer | begin training epoch 18
2024-05-31 17:00:44 | INFO | train_inner | epoch 018:     43 / 1021 loss=6.249, nll_loss=5.122, ppl=34.81, wps=24016.1, ups=6.1, wpb=3940.2, bsz=134.4, num_updates=17400, lr=0.000239732, gnorm=0.766, train_wall=16, wall=2814
2024-05-31 17:01:00 | INFO | train_inner | epoch 018:    143 / 1021 loss=6.198, nll_loss=5.062, ppl=33.41, wps=24622.4, ups=6.25, wpb=3941, bsz=144.2, num_updates=17500, lr=0.000239046, gnorm=0.775, train_wall=16, wall=2830
2024-05-31 17:01:16 | INFO | train_inner | epoch 018:    243 / 1021 loss=6.197, nll_loss=5.06, ppl=33.37, wps=24727.7, ups=6.28, wpb=3940.4, bsz=140.1, num_updates=17600, lr=0.000238366, gnorm=0.776, train_wall=16, wall=2846
2024-05-31 17:01:32 | INFO | train_inner | epoch 018:    343 / 1021 loss=6.226, nll_loss=5.094, ppl=34.16, wps=24634.5, ups=6.24, wpb=3945, bsz=156, num_updates=17700, lr=0.000237691, gnorm=0.832, train_wall=16, wall=2862
2024-05-31 17:01:48 | INFO | train_inner | epoch 018:    443 / 1021 loss=6.197, nll_loss=5.063, ppl=33.42, wps=24461.6, ups=6.18, wpb=3957.6, bsz=171.9, num_updates=17800, lr=0.000237023, gnorm=0.79, train_wall=16, wall=2878
2024-05-31 17:02:04 | INFO | train_inner | epoch 018:    543 / 1021 loss=6.204, nll_loss=5.069, ppl=33.57, wps=24609.7, ups=6.21, wpb=3965.8, bsz=167.4, num_updates=17900, lr=0.00023636, gnorm=0.779, train_wall=16, wall=2894
2024-05-31 17:02:20 | INFO | train_inner | epoch 018:    643 / 1021 loss=6.223, nll_loss=5.092, ppl=34.11, wps=24606.4, ups=6.24, wpb=3943.9, bsz=178.4, num_updates=18000, lr=0.000235702, gnorm=0.808, train_wall=16, wall=2910
2024-05-31 17:02:20 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:02:21 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_18_18000.pt (epoch 18 @ 18000 updates, score None) (writing took 1.0173494522459805 seconds)
2024-05-31 17:02:37 | INFO | train_inner | epoch 018:    743 / 1021 loss=6.224, nll_loss=5.093, ppl=34.13, wps=23529.8, ups=5.94, wpb=3959.2, bsz=157.7, num_updates=18100, lr=0.00023505, gnorm=0.77, train_wall=16, wall=2927
2024-05-31 17:02:53 | INFO | train_inner | epoch 018:    843 / 1021 loss=6.244, nll_loss=5.115, ppl=34.64, wps=25476.1, ups=6.52, wpb=3904.6, bsz=160.1, num_updates=18200, lr=0.000234404, gnorm=0.854, train_wall=15, wall=2942
2024-05-31 17:03:09 | INFO | train_inner | epoch 018:    943 / 1021 loss=6.245, nll_loss=5.117, ppl=34.71, wps=24222.9, ups=6.17, wpb=3923.3, bsz=154.9, num_updates=18300, lr=0.000233762, gnorm=0.77, train_wall=16, wall=2958
2024-05-31 17:03:21 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:03:21 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2024-05-31 17:03:21 | INFO | train | epoch 018 | loss 6.218 | nll_loss 5.086 | ppl 33.97 | wps 24575.3 | ups 6.23 | wpb 3943.1 | bsz 156.9 | num_updates 18378 | lr 0.000233266 | gnorm 0.792 | train_wall 161 | wall 2970
2024-05-31 17:03:21 | INFO | fairseq.trainer | begin training epoch 19
2024-05-31 17:03:24 | INFO | train_inner | epoch 019:     22 / 1021 loss=6.227, nll_loss=5.096, ppl=34.2, wps=25220.6, ups=6.38, wpb=3952.5, bsz=148.3, num_updates=18400, lr=0.000233126, gnorm=0.774, train_wall=15, wall=2974
2024-05-31 17:03:40 | INFO | train_inner | epoch 019:    122 / 1021 loss=6.165, nll_loss=5.024, ppl=32.54, wps=25091.6, ups=6.37, wpb=3941.3, bsz=151.3, num_updates=18500, lr=0.000232495, gnorm=0.793, train_wall=16, wall=2990
2024-05-31 17:03:56 | INFO | train_inner | epoch 019:    222 / 1021 loss=6.165, nll_loss=5.025, ppl=32.55, wps=25282.1, ups=6.37, wpb=3971, bsz=163.5, num_updates=18600, lr=0.000231869, gnorm=0.801, train_wall=16, wall=3005
2024-05-31 17:04:13 | INFO | train_inner | epoch 019:    322 / 1021 loss=6.183, nll_loss=5.045, ppl=33, wps=23764.2, ups=6, wpb=3963.9, bsz=167.9, num_updates=18700, lr=0.000231249, gnorm=0.794, train_wall=17, wall=3022
2024-05-31 17:04:28 | INFO | train_inner | epoch 019:    422 / 1021 loss=6.201, nll_loss=5.065, ppl=33.46, wps=25192.4, ups=6.37, wpb=3956.2, bsz=159.1, num_updates=18800, lr=0.000230633, gnorm=0.797, train_wall=16, wall=3038
2024-05-31 17:04:44 | INFO | train_inner | epoch 019:    522 / 1021 loss=6.173, nll_loss=5.033, ppl=32.75, wps=25051.4, ups=6.35, wpb=3946.8, bsz=170.2, num_updates=18900, lr=0.000230022, gnorm=0.832, train_wall=16, wall=3053
2024-05-31 17:05:00 | INFO | train_inner | epoch 019:    622 / 1021 loss=6.226, nll_loss=5.093, ppl=34.14, wps=25106.7, ups=6.42, wpb=3911.2, bsz=147.4, num_updates=19000, lr=0.000229416, gnorm=0.817, train_wall=15, wall=3069
2024-05-31 17:05:00 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:05:01 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_19_19000.pt (epoch 19 @ 19000 updates, score None) (writing took 0.9904253291897476 seconds)
2024-05-31 17:05:16 | INFO | train_inner | epoch 019:    722 / 1021 loss=6.216, nll_loss=5.082, ppl=33.86, wps=23575.4, ups=5.98, wpb=3942.5, bsz=139.8, num_updates=19100, lr=0.000228814, gnorm=0.771, train_wall=16, wall=3086
2024-05-31 17:05:32 | INFO | train_inner | epoch 019:    822 / 1021 loss=6.193, nll_loss=5.057, ppl=33.29, wps=25151, ups=6.35, wpb=3962.6, bsz=190.7, num_updates=19200, lr=0.000228218, gnorm=0.814, train_wall=16, wall=3101
2024-05-31 17:05:48 | INFO | train_inner | epoch 019:    922 / 1021 loss=6.188, nll_loss=5.05, ppl=33.14, wps=25011.4, ups=6.36, wpb=3935.7, bsz=147.2, num_updates=19300, lr=0.000227626, gnorm=0.77, train_wall=16, wall=3117
2024-05-31 17:06:03 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:06:03 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2024-05-31 17:06:03 | INFO | train | epoch 019 | loss 6.195 | nll_loss 5.058 | ppl 33.31 | wps 24784 | ups 6.29 | wpb 3943.1 | bsz 156.9 | num_updates 19399 | lr 0.000227044 | gnorm 0.799 | train_wall 160 | wall 3133
2024-05-31 17:06:03 | INFO | fairseq.trainer | begin training epoch 20
2024-05-31 17:06:04 | INFO | train_inner | epoch 020:      1 / 1021 loss=6.245, nll_loss=5.115, ppl=34.65, wps=24720.9, ups=6.34, wpb=3899.5, bsz=132.6, num_updates=19400, lr=0.000227038, gnorm=0.808, train_wall=15, wall=3133
2024-05-31 17:06:19 | INFO | train_inner | epoch 020:    101 / 1021 loss=6.14, nll_loss=4.996, ppl=31.91, wps=25188.6, ups=6.41, wpb=3928.5, bsz=140.2, num_updates=19500, lr=0.000226455, gnorm=0.797, train_wall=15, wall=3149
2024-05-31 17:06:35 | INFO | train_inner | epoch 020:    201 / 1021 loss=6.128, nll_loss=4.981, ppl=31.58, wps=24507.3, ups=6.21, wpb=3949.3, bsz=163.7, num_updates=19600, lr=0.000225877, gnorm=0.81, train_wall=16, wall=3165
2024-05-31 17:06:51 | INFO | train_inner | epoch 020:    301 / 1021 loss=6.163, nll_loss=5.02, ppl=32.46, wps=24902.4, ups=6.31, wpb=3944, bsz=154.8, num_updates=19700, lr=0.000225303, gnorm=0.813, train_wall=16, wall=3181
2024-05-31 17:07:07 | INFO | train_inner | epoch 020:    401 / 1021 loss=6.151, nll_loss=5.008, ppl=32.18, wps=25014.8, ups=6.3, wpb=3972.4, bsz=177.8, num_updates=19800, lr=0.000224733, gnorm=0.813, train_wall=16, wall=3196
2024-05-31 17:07:23 | INFO | train_inner | epoch 020:    501 / 1021 loss=6.171, nll_loss=5.03, ppl=32.66, wps=25145.7, ups=6.35, wpb=3959.1, bsz=154.8, num_updates=19900, lr=0.000224168, gnorm=0.795, train_wall=16, wall=3212
2024-05-31 17:07:38 | INFO | train_inner | epoch 020:    601 / 1021 loss=6.178, nll_loss=5.038, ppl=32.86, wps=25255.3, ups=6.38, wpb=3955.9, bsz=155.9, num_updates=20000, lr=0.000223607, gnorm=0.803, train_wall=16, wall=3228
2024-05-31 17:07:38 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:07:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_20_20000.pt (epoch 20 @ 20000 updates, score None) (writing took 0.9776515960693359 seconds)
2024-05-31 17:07:54 | INFO | train_inner | epoch 020:    701 / 1021 loss=6.188, nll_loss=5.048, ppl=33.09, wps=25038.8, ups=6.4, wpb=3911.1, bsz=146.1, num_updates=20100, lr=0.00022305, gnorm=0.804, train_wall=15, wall=3243
2024-05-31 17:08:09 | INFO | train_inner | epoch 020:    801 / 1021 loss=6.197, nll_loss=5.059, ppl=33.34, wps=26872.8, ups=6.81, wpb=3948.3, bsz=155.2, num_updates=20200, lr=0.000222497, gnorm=0.806, train_wall=15, wall=3258
2024-05-31 17:08:23 | INFO | train_inner | epoch 020:    901 / 1021 loss=6.199, nll_loss=5.062, ppl=33.4, wps=26742.3, ups=6.79, wpb=3937.3, bsz=168.2, num_updates=20300, lr=0.000221948, gnorm=0.865, train_wall=15, wall=3273
2024-05-31 17:08:38 | INFO | train_inner | epoch 020:   1001 / 1021 loss=6.204, nll_loss=5.068, ppl=33.55, wps=26775.8, ups=6.83, wpb=3919.6, bsz=154.9, num_updates=20400, lr=0.000221404, gnorm=0.804, train_wall=15, wall=3288
2024-05-31 17:08:41 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:08:41 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2024-05-31 17:08:41 | INFO | train | epoch 020 | loss 6.172 | nll_loss 5.031 | ppl 32.7 | wps 25513.6 | ups 6.47 | wpb 3943.1 | bsz 156.9 | num_updates 20420 | lr 0.000221295 | gnorm 0.81 | train_wall 155 | wall 3290
2024-05-31 17:08:41 | INFO | fairseq.trainer | begin training epoch 21
2024-05-31 17:08:54 | INFO | train_inner | epoch 021:     80 / 1021 loss=6.131, nll_loss=4.984, ppl=31.64, wps=25395.4, ups=6.43, wpb=3950.6, bsz=155.5, num_updates=20500, lr=0.000220863, gnorm=0.824, train_wall=15, wall=3303
2024-05-31 17:09:09 | INFO | train_inner | epoch 021:    180 / 1021 loss=6.13, nll_loss=4.981, ppl=31.57, wps=24997.6, ups=6.36, wpb=3928.4, bsz=141.2, num_updates=20600, lr=0.000220326, gnorm=0.805, train_wall=16, wall=3319
2024-05-31 17:09:25 | INFO | train_inner | epoch 021:    280 / 1021 loss=6.128, nll_loss=4.98, ppl=31.55, wps=25008.3, ups=6.37, wpb=3926.2, bsz=160.2, num_updates=20700, lr=0.000219793, gnorm=0.826, train_wall=16, wall=3334
2024-05-31 17:09:41 | INFO | train_inner | epoch 021:    380 / 1021 loss=6.129, nll_loss=4.98, ppl=31.57, wps=25242.8, ups=6.41, wpb=3936.9, bsz=149.2, num_updates=20800, lr=0.000219265, gnorm=0.799, train_wall=15, wall=3350
2024-05-31 17:09:57 | INFO | train_inner | epoch 021:    480 / 1021 loss=6.136, nll_loss=4.99, ppl=31.78, wps=23778.1, ups=6.01, wpb=3956.8, bsz=184.8, num_updates=20900, lr=0.000218739, gnorm=0.842, train_wall=16, wall=3367
2024-05-31 17:10:13 | INFO | train_inner | epoch 021:    580 / 1021 loss=6.15, nll_loss=5.005, ppl=32.11, wps=24307.5, ups=6.19, wpb=3926.6, bsz=156.2, num_updates=21000, lr=0.000218218, gnorm=0.82, train_wall=16, wall=3383
2024-05-31 17:10:13 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:10:14 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_21_21000.pt (epoch 21 @ 21000 updates, score None) (writing took 0.976559353992343 seconds)
2024-05-31 17:10:29 | INFO | train_inner | epoch 021:    680 / 1021 loss=6.155, nll_loss=5.011, ppl=32.25, wps=25263.1, ups=6.39, wpb=3952.3, bsz=164.2, num_updates=21100, lr=0.0002177, gnorm=0.811, train_wall=15, wall=3399
2024-05-31 17:10:44 | INFO | train_inner | epoch 021:    780 / 1021 loss=6.162, nll_loss=5.019, ppl=32.43, wps=26956.1, ups=6.82, wpb=3951, bsz=158.7, num_updates=21200, lr=0.000217186, gnorm=0.811, train_wall=15, wall=3413
2024-05-31 17:10:59 | INFO | train_inner | epoch 021:    880 / 1021 loss=6.187, nll_loss=5.047, ppl=33.07, wps=26072.2, ups=6.58, wpb=3959.6, bsz=161.7, num_updates=21300, lr=0.000216676, gnorm=0.859, train_wall=15, wall=3428
2024-05-31 17:11:15 | INFO | train_inner | epoch 021:    980 / 1021 loss=6.183, nll_loss=5.042, ppl=32.96, wps=25134.3, ups=6.37, wpb=3945.9, bsz=141.2, num_updates=21400, lr=0.000216169, gnorm=0.788, train_wall=16, wall=3444
2024-05-31 17:11:21 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:11:21 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2024-05-31 17:11:21 | INFO | train | epoch 021 | loss 6.151 | nll_loss 5.006 | ppl 32.13 | wps 25145.7 | ups 6.38 | wpb 3943.1 | bsz 156.9 | num_updates 21441 | lr 0.000215962 | gnorm 0.82 | train_wall 157 | wall 3451
2024-05-31 17:11:21 | INFO | fairseq.trainer | begin training epoch 22
2024-05-31 17:11:31 | INFO | train_inner | epoch 022:     59 / 1021 loss=6.133, nll_loss=4.986, ppl=31.69, wps=24717.6, ups=6.29, wpb=3931.4, bsz=145.6, num_updates=21500, lr=0.000215666, gnorm=0.814, train_wall=16, wall=3460
2024-05-31 17:11:46 | INFO | train_inner | epoch 022:    159 / 1021 loss=6.078, nll_loss=4.922, ppl=30.31, wps=25113.7, ups=6.35, wpb=3954.6, bsz=155.1, num_updates=21600, lr=0.000215166, gnorm=0.816, train_wall=16, wall=3476
2024-05-31 17:12:02 | INFO | train_inner | epoch 022:    259 / 1021 loss=6.104, nll_loss=4.951, ppl=30.93, wps=25209.2, ups=6.35, wpb=3969.2, bsz=176.2, num_updates=21700, lr=0.000214669, gnorm=0.842, train_wall=16, wall=3491
2024-05-31 17:12:18 | INFO | train_inner | epoch 022:    359 / 1021 loss=6.129, nll_loss=4.98, ppl=31.55, wps=23999.6, ups=6.1, wpb=3935.6, bsz=153.2, num_updates=21800, lr=0.000214176, gnorm=0.848, train_wall=16, wall=3508
2024-05-31 17:12:36 | INFO | train_inner | epoch 022:    459 / 1021 loss=6.142, nll_loss=4.994, ppl=31.88, wps=21867, ups=5.55, wpb=3937.2, bsz=148.7, num_updates=21900, lr=0.000213687, gnorm=0.819, train_wall=18, wall=3526
2024-05-31 17:12:53 | INFO | train_inner | epoch 022:    559 / 1021 loss=6.131, nll_loss=4.982, ppl=31.61, wps=23610.5, ups=5.99, wpb=3941.6, bsz=158.1, num_updates=22000, lr=0.000213201, gnorm=0.818, train_wall=17, wall=3543
2024-05-31 17:12:53 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:12:54 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_22_22000.pt (epoch 22 @ 22000 updates, score None) (writing took 0.9779317141510546 seconds)
2024-05-31 17:13:10 | INFO | train_inner | epoch 022:    659 / 1021 loss=6.151, nll_loss=5.005, ppl=32.12, wps=23733.4, ups=5.99, wpb=3964.3, bsz=167.1, num_updates=22100, lr=0.000212718, gnorm=0.847, train_wall=16, wall=3559
2024-05-31 17:13:26 | INFO | train_inner | epoch 022:    759 / 1021 loss=6.138, nll_loss=4.991, ppl=31.8, wps=25028, ups=6.4, wpb=3909.6, bsz=145.6, num_updates=22200, lr=0.000212238, gnorm=0.809, train_wall=15, wall=3575
2024-05-31 17:13:41 | INFO | train_inner | epoch 022:    859 / 1021 loss=6.17, nll_loss=5.026, ppl=32.59, wps=25214.8, ups=6.4, wpb=3940.8, bsz=147.7, num_updates=22300, lr=0.000211762, gnorm=0.818, train_wall=15, wall=3591
2024-05-31 17:13:58 | INFO | train_inner | epoch 022:    959 / 1021 loss=6.155, nll_loss=5.009, ppl=32.2, wps=23695.9, ups=6.01, wpb=3942.9, bsz=158.4, num_updates=22400, lr=0.000211289, gnorm=0.873, train_wall=16, wall=3607
2024-05-31 17:14:09 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:14:09 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2024-05-31 17:14:09 | INFO | train | epoch 022 | loss 6.132 | nll_loss 4.984 | ppl 31.64 | wps 24022.8 | ups 6.09 | wpb 3943.1 | bsz 156.9 | num_updates 22462 | lr 0.000210997 | gnorm 0.83 | train_wall 165 | wall 3618
2024-05-31 17:14:09 | INFO | fairseq.trainer | begin training epoch 23
2024-05-31 17:14:15 | INFO | train_inner | epoch 023:     38 / 1021 loss=6.139, nll_loss=4.992, ppl=31.83, wps=22608.4, ups=5.74, wpb=3941.6, bsz=154.2, num_updates=22500, lr=0.000210819, gnorm=0.819, train_wall=17, wall=3625
2024-05-31 17:14:32 | INFO | train_inner | epoch 023:    138 / 1021 loss=6.083, nll_loss=4.927, ppl=30.42, wps=23748.9, ups=6.02, wpb=3944.1, bsz=162.6, num_updates=22600, lr=0.000210352, gnorm=0.842, train_wall=16, wall=3641
2024-05-31 17:14:48 | INFO | train_inner | epoch 023:    238 / 1021 loss=6.078, nll_loss=4.92, ppl=30.28, wps=23688.9, ups=6.04, wpb=3922.4, bsz=151.8, num_updates=22700, lr=0.000209888, gnorm=0.864, train_wall=16, wall=3658
2024-05-31 17:15:05 | INFO | train_inner | epoch 023:    338 / 1021 loss=6.1, nll_loss=4.945, ppl=30.8, wps=24080.7, ups=6.09, wpb=3952.6, bsz=157.9, num_updates=22800, lr=0.000209427, gnorm=0.829, train_wall=16, wall=3674
2024-05-31 17:15:21 | INFO | train_inner | epoch 023:    438 / 1021 loss=6.096, nll_loss=4.941, ppl=30.73, wps=24819.1, ups=6.29, wpb=3943.2, bsz=159.5, num_updates=22900, lr=0.000208969, gnorm=0.82, train_wall=16, wall=3690
2024-05-31 17:15:36 | INFO | train_inner | epoch 023:    538 / 1021 loss=6.126, nll_loss=4.974, ppl=31.42, wps=25029.1, ups=6.36, wpb=3932.8, bsz=135.6, num_updates=23000, lr=0.000208514, gnorm=0.83, train_wall=16, wall=3706
2024-05-31 17:15:36 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:15:37 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_23_23000.pt (epoch 23 @ 23000 updates, score None) (writing took 1.0068347752094269 seconds)
2024-05-31 17:15:53 | INFO | train_inner | epoch 023:    638 / 1021 loss=6.134, nll_loss=4.984, ppl=31.66, wps=23516.6, ups=5.96, wpb=3946.7, bsz=161.7, num_updates=23100, lr=0.000208063, gnorm=0.849, train_wall=16, wall=3723
2024-05-31 17:16:09 | INFO | train_inner | epoch 023:    738 / 1021 loss=6.131, nll_loss=4.982, ppl=31.6, wps=25121.2, ups=6.34, wpb=3962.7, bsz=162.3, num_updates=23200, lr=0.000207614, gnorm=0.835, train_wall=16, wall=3738
2024-05-31 17:16:25 | INFO | train_inner | epoch 023:    838 / 1021 loss=6.12, nll_loss=4.969, ppl=31.31, wps=25094.6, ups=6.37, wpb=3942.5, bsz=162.9, num_updates=23300, lr=0.000207168, gnorm=0.847, train_wall=16, wall=3754
2024-05-31 17:16:40 | INFO | train_inner | epoch 023:    938 / 1021 loss=6.141, nll_loss=4.993, ppl=31.85, wps=24957.5, ups=6.33, wpb=3944.5, bsz=165.4, num_updates=23400, lr=0.000206725, gnorm=0.834, train_wall=16, wall=3770
2024-05-31 17:16:54 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:16:54 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2024-05-31 17:16:54 | INFO | train | epoch 023 | loss 6.114 | nll_loss 4.961 | ppl 31.16 | wps 24424.6 | ups 6.19 | wpb 3943.1 | bsz 156.9 | num_updates 23483 | lr 0.000206359 | gnorm 0.838 | train_wall 162 | wall 3783
2024-05-31 17:16:54 | INFO | fairseq.trainer | begin training epoch 24
2024-05-31 17:16:56 | INFO | train_inner | epoch 024:     17 / 1021 loss=6.129, nll_loss=4.98, ppl=31.56, wps=24686.9, ups=6.26, wpb=3941.5, bsz=154.7, num_updates=23500, lr=0.000206284, gnorm=0.84, train_wall=16, wall=3786
2024-05-31 17:17:12 | INFO | train_inner | epoch 024:    117 / 1021 loss=6.062, nll_loss=4.902, ppl=29.9, wps=25019.3, ups=6.34, wpb=3947.5, bsz=164.1, num_updates=23600, lr=0.000205847, gnorm=0.858, train_wall=16, wall=3802
2024-05-31 17:17:28 | INFO | train_inner | epoch 024:    217 / 1021 loss=6.065, nll_loss=4.905, ppl=29.97, wps=24981.1, ups=6.38, wpb=3916.8, bsz=149.5, num_updates=23700, lr=0.000205412, gnorm=0.826, train_wall=16, wall=3817
2024-05-31 17:17:43 | INFO | train_inner | epoch 024:    317 / 1021 loss=6.077, nll_loss=4.917, ppl=30.21, wps=25129.6, ups=6.44, wpb=3902.4, bsz=144.6, num_updates=23800, lr=0.00020498, gnorm=0.843, train_wall=15, wall=3833
2024-05-31 17:17:59 | INFO | train_inner | epoch 024:    417 / 1021 loss=6.075, nll_loss=4.917, ppl=30.22, wps=25104.8, ups=6.31, wpb=3977.6, bsz=173.5, num_updates=23900, lr=0.000204551, gnorm=0.84, train_wall=16, wall=3849
2024-05-31 17:18:15 | INFO | train_inner | epoch 024:    517 / 1021 loss=6.095, nll_loss=4.94, ppl=30.69, wps=25173.6, ups=6.34, wpb=3967.5, bsz=158.4, num_updates=24000, lr=0.000204124, gnorm=0.829, train_wall=16, wall=3864
2024-05-31 17:18:15 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:18:16 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_24_24000.pt (epoch 24 @ 24000 updates, score None) (writing took 0.9765971610322595 seconds)
2024-05-31 17:18:32 | INFO | train_inner | epoch 024:    617 / 1021 loss=6.113, nll_loss=4.959, ppl=31.1, wps=23622.3, ups=6.01, wpb=3933.3, bsz=156.6, num_updates=24100, lr=0.0002037, gnorm=0.857, train_wall=16, wall=3881
2024-05-31 17:18:47 | INFO | train_inner | epoch 024:    717 / 1021 loss=6.12, nll_loss=4.968, ppl=31.29, wps=25119.9, ups=6.35, wpb=3954.2, bsz=156.6, num_updates=24200, lr=0.000203279, gnorm=0.862, train_wall=16, wall=3897
2024-05-31 17:19:03 | INFO | train_inner | epoch 024:    817 / 1021 loss=6.124, nll_loss=4.972, ppl=31.38, wps=25221, ups=6.36, wpb=3964, bsz=151.4, num_updates=24300, lr=0.00020286, gnorm=0.859, train_wall=16, wall=3913
2024-05-31 17:19:19 | INFO | train_inner | epoch 024:    917 / 1021 loss=6.121, nll_loss=4.97, ppl=31.34, wps=24973.4, ups=6.37, wpb=3919.7, bsz=163.8, num_updates=24400, lr=0.000202444, gnorm=0.947, train_wall=16, wall=3928
2024-05-31 17:19:35 | INFO | train_inner | epoch 024:   1017 / 1021 loss=6.132, nll_loss=4.981, ppl=31.59, wps=25127.6, ups=6.37, wpb=3944.4, bsz=155, num_updates=24500, lr=0.000202031, gnorm=0.837, train_wall=16, wall=3944
2024-05-31 17:19:35 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:19:35 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2024-05-31 17:19:35 | INFO | train | epoch 024 | loss 6.098 | nll_loss 4.942 | ppl 30.74 | wps 24913.1 | ups 6.32 | wpb 3943.1 | bsz 156.9 | num_updates 24504 | lr 0.000202014 | gnorm 0.855 | train_wall 159 | wall 3945
2024-05-31 17:19:35 | INFO | fairseq.trainer | begin training epoch 25
2024-05-31 17:19:50 | INFO | train_inner | epoch 025:     96 / 1021 loss=6.027, nll_loss=4.861, ppl=29.07, wps=24932.7, ups=6.28, wpb=3967.9, bsz=159, num_updates=24600, lr=0.000201619, gnorm=0.923, train_wall=16, wall=3960
2024-05-31 17:20:06 | INFO | train_inner | epoch 025:    196 / 1021 loss=6.046, nll_loss=4.883, ppl=29.51, wps=25072.3, ups=6.34, wpb=3956.4, bsz=170.1, num_updates=24700, lr=0.000201211, gnorm=0.843, train_wall=16, wall=3976
2024-05-31 17:20:22 | INFO | train_inner | epoch 025:    296 / 1021 loss=6.068, nll_loss=4.907, ppl=30.01, wps=24173.8, ups=6.15, wpb=3930.7, bsz=160.5, num_updates=24800, lr=0.000200805, gnorm=0.871, train_wall=16, wall=3992
2024-05-31 17:20:38 | INFO | train_inner | epoch 025:    396 / 1021 loss=6.067, nll_loss=4.905, ppl=29.97, wps=24920.6, ups=6.29, wpb=3964.8, bsz=155.4, num_updates=24900, lr=0.000200401, gnorm=0.836, train_wall=16, wall=4008
2024-05-31 17:20:54 | INFO | train_inner | epoch 025:    496 / 1021 loss=6.077, nll_loss=4.918, ppl=30.23, wps=24636.3, ups=6.24, wpb=3950.4, bsz=156.8, num_updates=25000, lr=0.0002, gnorm=0.842, train_wall=16, wall=4024
2024-05-31 17:20:54 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:20:55 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_25_25000.pt (epoch 25 @ 25000 updates, score None) (writing took 0.979825277812779 seconds)
2024-05-31 17:21:10 | INFO | train_inner | epoch 025:    596 / 1021 loss=6.095, nll_loss=4.938, ppl=30.65, wps=24648.7, ups=6.25, wpb=3942.3, bsz=144, num_updates=25100, lr=0.000199601, gnorm=0.844, train_wall=15, wall=4040
2024-05-31 17:21:25 | INFO | train_inner | epoch 025:    696 / 1021 loss=6.114, nll_loss=4.959, ppl=31.1, wps=26484.5, ups=6.8, wpb=3894.9, bsz=143.4, num_updates=25200, lr=0.000199205, gnorm=0.884, train_wall=15, wall=4055
2024-05-31 17:21:41 | INFO | train_inner | epoch 025:    796 / 1021 loss=6.056, nll_loss=4.895, ppl=29.76, wps=25415.5, ups=6.41, wpb=3966.1, bsz=183.3, num_updates=25300, lr=0.000198811, gnorm=0.884, train_wall=15, wall=4070
2024-05-31 17:21:56 | INFO | train_inner | epoch 025:    896 / 1021 loss=6.111, nll_loss=4.957, ppl=31.07, wps=25154.5, ups=6.39, wpb=3934, bsz=162.4, num_updates=25400, lr=0.000198419, gnorm=0.867, train_wall=15, wall=4086
2024-05-31 17:22:12 | INFO | train_inner | epoch 025:    996 / 1021 loss=6.14, nll_loss=4.99, ppl=31.78, wps=25290.3, ups=6.42, wpb=3938.8, bsz=141.2, num_updates=25500, lr=0.00019803, gnorm=0.85, train_wall=15, wall=4101
2024-05-31 17:22:16 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:22:16 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2024-05-31 17:22:16 | INFO | train | epoch 025 | loss 6.081 | nll_loss 4.922 | ppl 30.32 | wps 25058.4 | ups 6.36 | wpb 3943.1 | bsz 156.9 | num_updates 25525 | lr 0.000197933 | gnorm 0.864 | train_wall 158 | wall 4105
2024-05-31 17:22:16 | INFO | fairseq.trainer | begin training epoch 26
2024-05-31 17:22:28 | INFO | train_inner | epoch 026:     75 / 1021 loss=6.021, nll_loss=4.854, ppl=28.92, wps=24801.7, ups=6.28, wpb=3948.4, bsz=163, num_updates=25600, lr=0.000197642, gnorm=0.835, train_wall=16, wall=4117
2024-05-31 17:22:44 | INFO | train_inner | epoch 026:    175 / 1021 loss=6.012, nll_loss=4.843, ppl=28.7, wps=25142.2, ups=6.38, wpb=3940, bsz=159.3, num_updates=25700, lr=0.000197257, gnorm=0.837, train_wall=16, wall=4133
2024-05-31 17:22:59 | INFO | train_inner | epoch 026:    275 / 1021 loss=6.036, nll_loss=4.869, ppl=29.22, wps=25148.6, ups=6.39, wpb=3938.5, bsz=132.6, num_updates=25800, lr=0.000196875, gnorm=0.837, train_wall=16, wall=4149
2024-05-31 17:23:15 | INFO | train_inner | epoch 026:    375 / 1021 loss=6.062, nll_loss=4.899, ppl=29.84, wps=25102, ups=6.37, wpb=3943.2, bsz=154.2, num_updates=25900, lr=0.000196494, gnorm=0.87, train_wall=16, wall=4164
2024-05-31 17:23:31 | INFO | train_inner | epoch 026:    475 / 1021 loss=6.081, nll_loss=4.92, ppl=30.28, wps=25098.8, ups=6.4, wpb=3924.1, bsz=149.5, num_updates=26000, lr=0.000196116, gnorm=0.909, train_wall=15, wall=4180
2024-05-31 17:23:31 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:23:32 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_26_26000.pt (epoch 26 @ 26000 updates, score None) (writing took 0.9718220150098205 seconds)
2024-05-31 17:23:46 | INFO | train_inner | epoch 026:    575 / 1021 loss=6.077, nll_loss=4.917, ppl=30.21, wps=24702.3, ups=6.3, wpb=3923.8, bsz=150.2, num_updates=26100, lr=0.00019574, gnorm=0.861, train_wall=15, wall=4196
2024-05-31 17:24:02 | INFO | train_inner | epoch 026:    675 / 1021 loss=6.093, nll_loss=4.935, ppl=30.59, wps=25137.1, ups=6.37, wpb=3944.1, bsz=141.4, num_updates=26200, lr=0.000195366, gnorm=0.859, train_wall=16, wall=4212
2024-05-31 17:24:18 | INFO | train_inner | epoch 026:    775 / 1021 loss=6.07, nll_loss=4.91, ppl=30.07, wps=24980.6, ups=6.3, wpb=3962.3, bsz=177.1, num_updates=26300, lr=0.000194994, gnorm=0.89, train_wall=16, wall=4227
2024-05-31 17:24:34 | INFO | train_inner | epoch 026:    875 / 1021 loss=6.067, nll_loss=4.906, ppl=29.99, wps=25142.7, ups=6.34, wpb=3965.2, bsz=171.2, num_updates=26400, lr=0.000194625, gnorm=0.842, train_wall=16, wall=4243
2024-05-31 17:24:49 | INFO | train_inner | epoch 026:    975 / 1021 loss=6.111, nll_loss=4.956, ppl=31.04, wps=25033.5, ups=6.39, wpb=3917.5, bsz=157.4, num_updates=26500, lr=0.000194257, gnorm=0.887, train_wall=15, wall=4259
2024-05-31 17:24:57 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:24:57 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2024-05-31 17:24:57 | INFO | train | epoch 026 | loss 6.065 | nll_loss 4.903 | ppl 29.92 | wps 24963.9 | ups 6.33 | wpb 3943.1 | bsz 156.9 | num_updates 26546 | lr 0.000194089 | gnorm 0.865 | train_wall 159 | wall 4266
2024-05-31 17:24:57 | INFO | fairseq.trainer | begin training epoch 27
2024-05-31 17:25:06 | INFO | train_inner | epoch 027:     54 / 1021 loss=6.061, nll_loss=4.899, ppl=29.84, wps=24133.7, ups=6.09, wpb=3962.2, bsz=166.8, num_updates=26600, lr=0.000193892, gnorm=0.866, train_wall=16, wall=4275
2024-05-31 17:25:22 | INFO | train_inner | epoch 027:    154 / 1021 loss=6.018, nll_loss=4.85, ppl=28.85, wps=25160.4, ups=6.36, wpb=3959, bsz=167.3, num_updates=26700, lr=0.000193528, gnorm=0.87, train_wall=16, wall=4291
2024-05-31 17:25:37 | INFO | train_inner | epoch 027:    254 / 1021 loss=6.012, nll_loss=4.842, ppl=28.68, wps=25137.9, ups=6.38, wpb=3942.1, bsz=159.2, num_updates=26800, lr=0.000193167, gnorm=0.857, train_wall=16, wall=4307
2024-05-31 17:25:53 | INFO | train_inner | epoch 027:    354 / 1021 loss=6.034, nll_loss=4.866, ppl=29.16, wps=25238.6, ups=6.38, wpb=3952.9, bsz=156.8, num_updates=26900, lr=0.000192807, gnorm=0.86, train_wall=16, wall=4322
2024-05-31 17:26:09 | INFO | train_inner | epoch 027:    454 / 1021 loss=6.036, nll_loss=4.87, ppl=29.25, wps=25135, ups=6.4, wpb=3926.6, bsz=178.6, num_updates=27000, lr=0.00019245, gnorm=0.888, train_wall=15, wall=4338
2024-05-31 17:26:09 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:26:10 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_27_27000.pt (epoch 27 @ 27000 updates, score None) (writing took 0.9714790461584926 seconds)
2024-05-31 17:26:25 | INFO | train_inner | epoch 027:    554 / 1021 loss=6.042, nll_loss=4.875, ppl=29.35, wps=24401.8, ups=6.18, wpb=3946.5, bsz=149.4, num_updates=27100, lr=0.000192095, gnorm=0.84, train_wall=15, wall=4354
2024-05-31 17:26:40 | INFO | train_inner | epoch 027:    654 / 1021 loss=6.081, nll_loss=4.921, ppl=30.29, wps=25321.2, ups=6.38, wpb=3967.7, bsz=162.5, num_updates=27200, lr=0.000191741, gnorm=0.889, train_wall=16, wall=4370
2024-05-31 17:26:56 | INFO | train_inner | epoch 027:    754 / 1021 loss=6.074, nll_loss=4.911, ppl=30.1, wps=25138, ups=6.37, wpb=3944.9, bsz=151, num_updates=27300, lr=0.00019139, gnorm=0.892, train_wall=16, wall=4385
2024-05-31 17:27:13 | INFO | train_inner | epoch 027:    854 / 1021 loss=6.073, nll_loss=4.912, ppl=30.1, wps=23446.8, ups=5.99, wpb=3917.1, bsz=150.6, num_updates=27400, lr=0.00019104, gnorm=0.884, train_wall=17, wall=4402
2024-05-31 17:27:29 | INFO | train_inner | epoch 027:    954 / 1021 loss=6.086, nll_loss=4.926, ppl=30.41, wps=24410.1, ups=6.2, wpb=3938.2, bsz=147.3, num_updates=27500, lr=0.000190693, gnorm=0.856, train_wall=16, wall=4418
2024-05-31 17:27:39 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:27:39 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2024-05-31 17:27:39 | INFO | train | epoch 027 | loss 6.05 | nll_loss 4.886 | ppl 29.56 | wps 24802.3 | ups 6.29 | wpb 3943.1 | bsz 156.9 | num_updates 27567 | lr 0.000190461 | gnorm 0.868 | train_wall 160 | wall 4429
2024-05-31 17:27:39 | INFO | fairseq.trainer | begin training epoch 28
2024-05-31 17:27:45 | INFO | train_inner | epoch 028:     33 / 1021 loss=6.052, nll_loss=4.888, ppl=29.61, wps=24733, ups=6.3, wpb=3925.7, bsz=149, num_updates=27600, lr=0.000190347, gnorm=0.863, train_wall=16, wall=4434
2024-05-31 17:28:01 | INFO | train_inner | epoch 028:    133 / 1021 loss=5.998, nll_loss=4.826, ppl=28.37, wps=24881.3, ups=6.31, wpb=3941.3, bsz=164.5, num_updates=27700, lr=0.000190003, gnorm=0.885, train_wall=16, wall=4450
2024-05-31 17:28:16 | INFO | train_inner | epoch 028:    233 / 1021 loss=6.03, nll_loss=4.861, ppl=29.07, wps=24957.8, ups=6.38, wpb=3914.2, bsz=145.4, num_updates=27800, lr=0.000189661, gnorm=0.88, train_wall=16, wall=4466
2024-05-31 17:28:32 | INFO | train_inner | epoch 028:    333 / 1021 loss=6.023, nll_loss=4.852, ppl=28.87, wps=24755.6, ups=6.33, wpb=3912.7, bsz=138.8, num_updates=27900, lr=0.000189321, gnorm=0.872, train_wall=16, wall=4481
2024-05-31 17:28:48 | INFO | train_inner | epoch 028:    433 / 1021 loss=6.029, nll_loss=4.859, ppl=29.03, wps=24071.8, ups=6.11, wpb=3942.9, bsz=145.8, num_updates=28000, lr=0.000188982, gnorm=0.857, train_wall=16, wall=4498
2024-05-31 17:28:48 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:28:49 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_28_28000.pt (epoch 28 @ 28000 updates, score None) (writing took 0.9827418471686542 seconds)
2024-05-31 17:29:05 | INFO | train_inner | epoch 028:    533 / 1021 loss=6.032, nll_loss=4.865, ppl=29.14, wps=24462.7, ups=6.18, wpb=3960.5, bsz=178.2, num_updates=28100, lr=0.000188646, gnorm=0.898, train_wall=15, wall=4514
2024-05-31 17:29:20 | INFO | train_inner | epoch 028:    633 / 1021 loss=6.045, nll_loss=4.879, ppl=29.44, wps=25293.8, ups=6.38, wpb=3965.3, bsz=165.3, num_updates=28200, lr=0.000188311, gnorm=0.86, train_wall=16, wall=4530
2024-05-31 17:29:36 | INFO | train_inner | epoch 028:    733 / 1021 loss=6.062, nll_loss=4.897, ppl=29.8, wps=25162, ups=6.36, wpb=3956.1, bsz=146.1, num_updates=28300, lr=0.000187978, gnorm=0.863, train_wall=16, wall=4545
2024-05-31 17:29:52 | INFO | train_inner | epoch 028:    833 / 1021 loss=6.044, nll_loss=4.878, ppl=29.4, wps=25444.3, ups=6.42, wpb=3966, bsz=160.2, num_updates=28400, lr=0.000187647, gnorm=0.899, train_wall=15, wall=4561
2024-05-31 17:30:07 | INFO | train_inner | epoch 028:    933 / 1021 loss=6.064, nll_loss=4.9, ppl=29.86, wps=25230.1, ups=6.39, wpb=3950, bsz=158.8, num_updates=28500, lr=0.000187317, gnorm=0.91, train_wall=15, wall=4577
2024-05-31 17:30:21 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:30:21 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2024-05-31 17:30:21 | INFO | train | epoch 028 | loss 6.036 | nll_loss 4.869 | ppl 29.22 | wps 24925.7 | ups 6.32 | wpb 3943.1 | bsz 156.9 | num_updates 28588 | lr 0.000187029 | gnorm 0.881 | train_wall 159 | wall 4590
2024-05-31 17:30:21 | INFO | fairseq.trainer | begin training epoch 29
2024-05-31 17:30:23 | INFO | train_inner | epoch 029:     12 / 1021 loss=6.049, nll_loss=4.884, ppl=29.53, wps=25061.5, ups=6.4, wpb=3917.7, bsz=158.1, num_updates=28600, lr=0.000186989, gnorm=0.884, train_wall=15, wall=4592
2024-05-31 17:30:39 | INFO | train_inner | epoch 029:    112 / 1021 loss=5.971, nll_loss=4.794, ppl=27.74, wps=25152.5, ups=6.39, wpb=3935, bsz=166, num_updates=28700, lr=0.000186663, gnorm=0.917, train_wall=15, wall=4608
2024-05-31 17:30:54 | INFO | train_inner | epoch 029:    212 / 1021 loss=5.987, nll_loss=4.812, ppl=28.09, wps=25129.8, ups=6.39, wpb=3931.8, bsz=158, num_updates=28800, lr=0.000186339, gnorm=0.886, train_wall=15, wall=4624
2024-05-31 17:31:10 | INFO | train_inner | epoch 029:    312 / 1021 loss=6.004, nll_loss=4.83, ppl=28.45, wps=25125.1, ups=6.39, wpb=3930.4, bsz=155.8, num_updates=28900, lr=0.000186016, gnorm=0.88, train_wall=15, wall=4639
2024-05-31 17:31:26 | INFO | train_inner | epoch 029:    412 / 1021 loss=5.994, nll_loss=4.82, ppl=28.25, wps=25138.8, ups=6.33, wpb=3968.8, bsz=169.6, num_updates=29000, lr=0.000185695, gnorm=0.865, train_wall=16, wall=4655
2024-05-31 17:31:26 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:31:27 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_29_29000.pt (epoch 29 @ 29000 updates, score None) (writing took 0.992067710030824 seconds)
2024-05-31 17:31:41 | INFO | train_inner | epoch 029:    512 / 1021 loss=6.017, nll_loss=4.845, ppl=28.74, wps=25244.3, ups=6.39, wpb=3948.5, bsz=146.4, num_updates=29100, lr=0.000185376, gnorm=0.901, train_wall=15, wall=4671
2024-05-31 17:31:56 | INFO | train_inner | epoch 029:    612 / 1021 loss=6.035, nll_loss=4.866, ppl=29.16, wps=26936.7, ups=6.81, wpb=3957.6, bsz=159.8, num_updates=29200, lr=0.000185058, gnorm=0.893, train_wall=15, wall=4685
2024-05-31 17:32:11 | INFO | train_inner | epoch 029:    712 / 1021 loss=6.059, nll_loss=4.893, ppl=29.72, wps=26778.2, ups=6.82, wpb=3923.6, bsz=151.8, num_updates=29300, lr=0.000184742, gnorm=0.925, train_wall=15, wall=4700
2024-05-31 17:32:26 | INFO | train_inner | epoch 029:    812 / 1021 loss=6.057, nll_loss=4.893, ppl=29.71, wps=26540, ups=6.73, wpb=3945.8, bsz=157.7, num_updates=29400, lr=0.000184428, gnorm=0.882, train_wall=15, wall=4715
2024-05-31 17:32:41 | INFO | train_inner | epoch 029:    912 / 1021 loss=6.044, nll_loss=4.877, ppl=29.38, wps=25054.1, ups=6.36, wpb=3939.6, bsz=159, num_updates=29500, lr=0.000184115, gnorm=0.878, train_wall=16, wall=4731
2024-05-31 17:32:57 | INFO | train_inner | epoch 029:   1012 / 1021 loss=6.052, nll_loss=4.886, ppl=29.57, wps=25004.8, ups=6.31, wpb=3959.8, bsz=155.4, num_updates=29600, lr=0.000183804, gnorm=0.867, train_wall=16, wall=4746
2024-05-31 17:32:59 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:32:59 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2024-05-31 17:32:59 | INFO | train | epoch 029 | loss 6.023 | nll_loss 4.852 | ppl 28.89 | wps 25549.1 | ups 6.48 | wpb 3943.1 | bsz 156.9 | num_updates 29609 | lr 0.000183776 | gnorm 0.889 | train_wall 155 | wall 4748
2024-05-31 17:32:59 | INFO | fairseq.trainer | begin training epoch 30
2024-05-31 17:33:13 | INFO | train_inner | epoch 030:     91 / 1021 loss=5.972, nll_loss=4.794, ppl=27.74, wps=24508.9, ups=6.24, wpb=3927.1, bsz=142.1, num_updates=29700, lr=0.000183494, gnorm=0.871, train_wall=16, wall=4763
2024-05-31 17:33:29 | INFO | train_inner | epoch 030:    191 / 1021 loss=5.958, nll_loss=4.778, ppl=27.44, wps=24945.1, ups=6.31, wpb=3951.6, bsz=170.2, num_updates=29800, lr=0.000183186, gnorm=0.891, train_wall=16, wall=4778
2024-05-31 17:33:45 | INFO | train_inner | epoch 030:    291 / 1021 loss=6.001, nll_loss=4.828, ppl=28.39, wps=25087.3, ups=6.31, wpb=3975.9, bsz=169, num_updates=29900, lr=0.000182879, gnorm=0.891, train_wall=16, wall=4794
2024-05-31 17:34:01 | INFO | train_inner | epoch 030:    391 / 1021 loss=5.99, nll_loss=4.815, ppl=28.14, wps=25030.9, ups=6.33, wpb=3955.6, bsz=160.6, num_updates=30000, lr=0.000182574, gnorm=0.876, train_wall=16, wall=4810
2024-05-31 17:34:01 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:34:02 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_30_30000.pt (epoch 30 @ 30000 updates, score None) (writing took 0.9809790481813252 seconds)
2024-05-31 17:34:16 | INFO | train_inner | epoch 030:    491 / 1021 loss=6.021, nll_loss=4.849, ppl=28.82, wps=24823.4, ups=6.31, wpb=3933.5, bsz=153.4, num_updates=30100, lr=0.000182271, gnorm=0.895, train_wall=15, wall=4826
2024-05-31 17:34:31 | INFO | train_inner | epoch 030:    591 / 1021 loss=6.03, nll_loss=4.858, ppl=29.01, wps=26667.3, ups=6.79, wpb=3928.8, bsz=148.7, num_updates=30200, lr=0.000181969, gnorm=0.933, train_wall=15, wall=4841
2024-05-31 17:34:46 | INFO | train_inner | epoch 030:    691 / 1021 loss=6.013, nll_loss=4.842, ppl=28.68, wps=26786.1, ups=6.76, wpb=3961.1, bsz=159.9, num_updates=30300, lr=0.000181668, gnorm=0.881, train_wall=15, wall=4855
2024-05-31 17:35:01 | INFO | train_inner | epoch 030:    791 / 1021 loss=6.028, nll_loss=4.857, ppl=28.99, wps=26692.4, ups=6.76, wpb=3951.1, bsz=156.9, num_updates=30400, lr=0.000181369, gnorm=0.881, train_wall=15, wall=4870
2024-05-31 17:35:17 | INFO | train_inner | epoch 030:    891 / 1021 loss=6.035, nll_loss=4.867, ppl=29.18, wps=24945.3, ups=6.34, wpb=3935.3, bsz=161.9, num_updates=30500, lr=0.000181071, gnorm=0.902, train_wall=16, wall=4886
2024-05-31 17:35:32 | INFO | train_inner | epoch 030:    991 / 1021 loss=6.04, nll_loss=4.872, ppl=29.27, wps=25003.9, ups=6.4, wpb=3908.2, bsz=145.4, num_updates=30600, lr=0.000180775, gnorm=0.882, train_wall=15, wall=4902
2024-05-31 17:35:37 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:35:37 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2024-05-31 17:35:37 | INFO | train | epoch 030 | loss 6.01 | nll_loss 4.837 | ppl 28.58 | wps 25418.8 | ups 6.45 | wpb 3943.1 | bsz 156.9 | num_updates 30630 | lr 0.000180687 | gnorm 0.891 | train_wall 156 | wall 4906
2024-05-31 17:35:37 | INFO | fairseq.trainer | begin training epoch 31
2024-05-31 17:35:48 | INFO | train_inner | epoch 031:     70 / 1021 loss=5.983, nll_loss=4.806, ppl=27.98, wps=24782.6, ups=6.28, wpb=3949.3, bsz=164.5, num_updates=30700, lr=0.000180481, gnorm=0.903, train_wall=16, wall=4918
2024-05-31 17:36:04 | INFO | train_inner | epoch 031:    170 / 1021 loss=5.956, nll_loss=4.776, ppl=27.39, wps=25052.1, ups=6.32, wpb=3961, bsz=166.6, num_updates=30800, lr=0.000180187, gnorm=0.878, train_wall=16, wall=4933
2024-05-31 17:36:20 | INFO | train_inner | epoch 031:    270 / 1021 loss=5.985, nll_loss=4.806, ppl=27.98, wps=24959.5, ups=6.33, wpb=3945.6, bsz=154.2, num_updates=30900, lr=0.000179896, gnorm=0.896, train_wall=16, wall=4949
2024-05-31 17:36:36 | INFO | train_inner | epoch 031:    370 / 1021 loss=5.988, nll_loss=4.81, ppl=28.05, wps=24601.6, ups=6.32, wpb=3893.4, bsz=144.9, num_updates=31000, lr=0.000179605, gnorm=0.931, train_wall=16, wall=4965
2024-05-31 17:36:36 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:36:37 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_31_31000.pt (epoch 31 @ 31000 updates, score None) (writing took 0.9806293668225408 seconds)
2024-05-31 17:36:52 | INFO | train_inner | epoch 031:    470 / 1021 loss=6.002, nll_loss=4.827, ppl=28.38, wps=24541.5, ups=6.25, wpb=3923.8, bsz=161.3, num_updates=31100, lr=0.000179316, gnorm=0.94, train_wall=15, wall=4981
2024-05-31 17:37:08 | INFO | train_inner | epoch 031:    570 / 1021 loss=6.007, nll_loss=4.833, ppl=28.5, wps=24767.8, ups=6.26, wpb=3955.6, bsz=158.6, num_updates=31200, lr=0.000179029, gnorm=0.9, train_wall=16, wall=4997
2024-05-31 17:37:23 | INFO | train_inner | epoch 031:    670 / 1021 loss=6.009, nll_loss=4.835, ppl=28.53, wps=24777.8, ups=6.28, wpb=3945.2, bsz=155.2, num_updates=31300, lr=0.000178743, gnorm=0.89, train_wall=16, wall=5013
2024-05-31 17:37:39 | INFO | train_inner | epoch 031:    770 / 1021 loss=6.017, nll_loss=4.844, ppl=28.72, wps=25121.8, ups=6.38, wpb=3937.7, bsz=143.4, num_updates=31400, lr=0.000178458, gnorm=0.886, train_wall=16, wall=5029
2024-05-31 17:37:55 | INFO | train_inner | epoch 031:    870 / 1021 loss=6.008, nll_loss=4.835, ppl=28.54, wps=25034.9, ups=6.32, wpb=3959.1, bsz=163.6, num_updates=31500, lr=0.000178174, gnorm=0.881, train_wall=16, wall=5044
2024-05-31 17:38:11 | INFO | train_inner | epoch 031:    970 / 1021 loss=6.021, nll_loss=4.849, ppl=28.81, wps=24929.3, ups=6.31, wpb=3949.5, bsz=145.5, num_updates=31600, lr=0.000177892, gnorm=0.88, train_wall=16, wall=5060
2024-05-31 17:38:19 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:38:19 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2024-05-31 17:38:19 | INFO | train | epoch 031 | loss 5.997 | nll_loss 4.822 | ppl 28.28 | wps 24843.3 | ups 6.3 | wpb 3943.1 | bsz 156.9 | num_updates 31651 | lr 0.000177749 | gnorm 0.901 | train_wall 159 | wall 5068
2024-05-31 17:38:19 | INFO | fairseq.trainer | begin training epoch 32
2024-05-31 17:38:27 | INFO | train_inner | epoch 032:     49 / 1021 loss=5.978, nll_loss=4.802, ppl=27.9, wps=24564.1, ups=6.21, wpb=3958.7, bsz=174.4, num_updates=31700, lr=0.000177611, gnorm=0.916, train_wall=16, wall=5076
2024-05-31 17:38:43 | INFO | train_inner | epoch 032:    149 / 1021 loss=5.932, nll_loss=4.745, ppl=26.82, wps=24931.4, ups=6.34, wpb=3934.9, bsz=143.7, num_updates=31800, lr=0.000177332, gnorm=0.896, train_wall=16, wall=5092
2024-05-31 17:38:58 | INFO | train_inner | epoch 032:    249 / 1021 loss=5.956, nll_loss=4.775, ppl=27.37, wps=25199.3, ups=6.34, wpb=3972.8, bsz=155.8, num_updates=31900, lr=0.000177054, gnorm=0.891, train_wall=16, wall=5108
2024-05-31 17:39:14 | INFO | train_inner | epoch 032:    349 / 1021 loss=5.975, nll_loss=4.795, ppl=27.76, wps=25138.6, ups=6.38, wpb=3938.8, bsz=164.8, num_updates=32000, lr=0.000176777, gnorm=0.962, train_wall=16, wall=5124
2024-05-31 17:39:14 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:39:15 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_32_32000.pt (epoch 32 @ 32000 updates, score None) (writing took 0.9802493136376143 seconds)
2024-05-31 17:39:30 | INFO | train_inner | epoch 032:    449 / 1021 loss=5.986, nll_loss=4.807, ppl=28, wps=25078.2, ups=6.4, wpb=3921, bsz=135.8, num_updates=32100, lr=0.000176501, gnorm=0.927, train_wall=15, wall=5139
2024-05-31 17:39:45 | INFO | train_inner | epoch 032:    549 / 1021 loss=6.003, nll_loss=4.827, ppl=28.39, wps=25181.6, ups=6.43, wpb=3916.2, bsz=145.8, num_updates=32200, lr=0.000176227, gnorm=0.935, train_wall=15, wall=5155
2024-05-31 17:40:01 | INFO | train_inner | epoch 032:    649 / 1021 loss=5.999, nll_loss=4.822, ppl=28.29, wps=25056.8, ups=6.35, wpb=3948, bsz=154.7, num_updates=32300, lr=0.000175954, gnorm=0.924, train_wall=16, wall=5170
2024-05-31 17:40:17 | INFO | train_inner | epoch 032:    749 / 1021 loss=6.001, nll_loss=4.825, ppl=28.35, wps=24839.6, ups=6.3, wpb=3945.5, bsz=144.3, num_updates=32400, lr=0.000175682, gnorm=0.888, train_wall=16, wall=5186
2024-05-31 17:40:33 | INFO | train_inner | epoch 032:    849 / 1021 loss=6.016, nll_loss=4.843, ppl=28.69, wps=25111.2, ups=6.4, wpb=3921.2, bsz=169.4, num_updates=32500, lr=0.000175412, gnorm=0.942, train_wall=15, wall=5202
2024-05-31 17:40:48 | INFO | train_inner | epoch 032:    949 / 1021 loss=5.998, nll_loss=4.825, ppl=28.34, wps=25106, ups=6.32, wpb=3969.8, bsz=186, num_updates=32600, lr=0.000175142, gnorm=0.901, train_wall=16, wall=5218
2024-05-31 17:41:00 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:41:00 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2024-05-31 17:41:00 | INFO | train | epoch 032 | loss 5.984 | nll_loss 4.806 | ppl 27.98 | wps 25044.9 | ups 6.35 | wpb 3943.1 | bsz 156.9 | num_updates 32672 | lr 0.000174949 | gnorm 0.915 | train_wall 158 | wall 5229
2024-05-31 17:41:00 | INFO | fairseq.trainer | begin training epoch 33
2024-05-31 17:41:04 | INFO | train_inner | epoch 033:     28 / 1021 loss=5.983, nll_loss=4.806, ppl=27.98, wps=24959.2, ups=6.3, wpb=3959, bsz=163.2, num_updates=32700, lr=0.000174874, gnorm=0.891, train_wall=16, wall=5234
2024-05-31 17:41:20 | INFO | train_inner | epoch 033:    128 / 1021 loss=5.948, nll_loss=4.764, ppl=27.17, wps=25177.7, ups=6.39, wpb=3940.8, bsz=155.8, num_updates=32800, lr=0.000174608, gnorm=0.916, train_wall=15, wall=5249
2024-05-31 17:41:36 | INFO | train_inner | epoch 033:    228 / 1021 loss=5.937, nll_loss=4.75, ppl=26.91, wps=25146.1, ups=6.37, wpb=3947.2, bsz=143.8, num_updates=32900, lr=0.000174342, gnorm=0.887, train_wall=16, wall=5265
2024-05-31 17:41:51 | INFO | train_inner | epoch 033:    328 / 1021 loss=5.949, nll_loss=4.765, ppl=27.2, wps=25115.5, ups=6.34, wpb=3963.6, bsz=158.2, num_updates=33000, lr=0.000174078, gnorm=0.9, train_wall=16, wall=5281
2024-05-31 17:41:51 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:41:52 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_33_33000.pt (epoch 33 @ 33000 updates, score None) (writing took 0.97649239981547 seconds)
2024-05-31 17:42:08 | INFO | train_inner | epoch 033:    428 / 1021 loss=5.97, nll_loss=4.789, ppl=27.64, wps=24047.9, ups=6.09, wpb=3951.1, bsz=143.4, num_updates=33100, lr=0.000173814, gnorm=0.897, train_wall=15, wall=5297
2024-05-31 17:42:24 | INFO | train_inner | epoch 033:    528 / 1021 loss=5.983, nll_loss=4.804, ppl=27.93, wps=25067.1, ups=6.36, wpb=3943.1, bsz=165, num_updates=33200, lr=0.000173553, gnorm=0.94, train_wall=16, wall=5313
2024-05-31 17:42:39 | INFO | train_inner | epoch 033:    628 / 1021 loss=6.004, nll_loss=4.828, ppl=28.4, wps=25140.9, ups=6.38, wpb=3941.3, bsz=153.6, num_updates=33300, lr=0.000173292, gnorm=0.912, train_wall=16, wall=5329
2024-05-31 17:42:55 | INFO | train_inner | epoch 033:    728 / 1021 loss=6.001, nll_loss=4.825, ppl=28.35, wps=25095.3, ups=6.39, wpb=3929.6, bsz=162.4, num_updates=33400, lr=0.000173032, gnorm=0.924, train_wall=16, wall=5344
2024-05-31 17:43:11 | INFO | train_inner | epoch 033:    828 / 1021 loss=5.988, nll_loss=4.81, ppl=28.04, wps=24986.9, ups=6.38, wpb=3916.5, bsz=157.4, num_updates=33500, lr=0.000172774, gnorm=0.908, train_wall=16, wall=5360
2024-05-31 17:43:26 | INFO | train_inner | epoch 033:    928 / 1021 loss=5.986, nll_loss=4.809, ppl=28.03, wps=24986.1, ups=6.34, wpb=3938.1, bsz=178.9, num_updates=33600, lr=0.000172516, gnorm=0.982, train_wall=16, wall=5376
2024-05-31 17:43:41 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:43:41 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2024-05-31 17:43:41 | INFO | train | epoch 033 | loss 5.974 | nll_loss 4.794 | ppl 27.75 | wps 24953.3 | ups 6.33 | wpb 3943.1 | bsz 156.9 | num_updates 33693 | lr 0.000172278 | gnorm 0.913 | train_wall 159 | wall 5390
2024-05-31 17:43:41 | INFO | fairseq.trainer | begin training epoch 34
2024-05-31 17:43:42 | INFO | train_inner | epoch 034:      7 / 1021 loss=5.995, nll_loss=4.819, ppl=28.23, wps=24720.5, ups=6.24, wpb=3959.3, bsz=149.4, num_updates=33700, lr=0.00017226, gnorm=0.88, train_wall=16, wall=5392
2024-05-31 17:43:58 | INFO | train_inner | epoch 034:    107 / 1021 loss=5.91, nll_loss=4.721, ppl=26.37, wps=25059.5, ups=6.31, wpb=3968.8, bsz=162.2, num_updates=33800, lr=0.000172005, gnorm=0.897, train_wall=16, wall=5408
2024-05-31 17:44:14 | INFO | train_inner | epoch 034:    207 / 1021 loss=5.927, nll_loss=4.739, ppl=26.71, wps=25085, ups=6.35, wpb=3950.5, bsz=159.8, num_updates=33900, lr=0.000171751, gnorm=0.901, train_wall=16, wall=5423
2024-05-31 17:44:30 | INFO | train_inner | epoch 034:    307 / 1021 loss=5.962, nll_loss=4.778, ppl=27.43, wps=25048.6, ups=6.36, wpb=3939.6, bsz=139, num_updates=34000, lr=0.000171499, gnorm=0.916, train_wall=16, wall=5439
2024-05-31 17:44:30 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:44:31 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_34_34000.pt (epoch 34 @ 34000 updates, score None) (writing took 0.9828832331113517 seconds)
2024-05-31 17:44:45 | INFO | train_inner | epoch 034:    407 / 1021 loss=5.973, nll_loss=4.79, ppl=27.66, wps=24978.8, ups=6.38, wpb=3914.9, bsz=148.3, num_updates=34100, lr=0.000171247, gnorm=0.971, train_wall=15, wall=5455
2024-05-31 17:45:00 | INFO | train_inner | epoch 034:    507 / 1021 loss=5.952, nll_loss=4.769, ppl=27.27, wps=26721.8, ups=6.75, wpb=3955.9, bsz=174.7, num_updates=34200, lr=0.000170996, gnorm=0.919, train_wall=15, wall=5470
2024-05-31 17:45:16 | INFO | train_inner | epoch 034:    607 / 1021 loss=5.965, nll_loss=4.783, ppl=27.54, wps=25191.2, ups=6.36, wpb=3961, bsz=166.1, num_updates=34300, lr=0.000170747, gnorm=0.912, train_wall=16, wall=5485
2024-05-31 17:45:32 | INFO | train_inner | epoch 034:    707 / 1021 loss=5.984, nll_loss=4.805, ppl=27.95, wps=25155.9, ups=6.38, wpb=3941.9, bsz=164.2, num_updates=34400, lr=0.000170499, gnorm=0.945, train_wall=16, wall=5501
2024-05-31 17:45:47 | INFO | train_inner | epoch 034:    807 / 1021 loss=5.981, nll_loss=4.801, ppl=27.87, wps=25191.7, ups=6.36, wpb=3960.6, bsz=147.5, num_updates=34500, lr=0.000170251, gnorm=0.896, train_wall=16, wall=5517
2024-05-31 17:46:03 | INFO | train_inner | epoch 034:    907 / 1021 loss=5.985, nll_loss=4.807, ppl=27.99, wps=25183.6, ups=6.4, wpb=3934.2, bsz=159.5, num_updates=34600, lr=0.000170005, gnorm=0.913, train_wall=15, wall=5532
2024-05-31 17:46:18 | INFO | train_inner | epoch 034:   1007 / 1021 loss=6.006, nll_loss=4.83, ppl=28.44, wps=25089.6, ups=6.41, wpb=3914.3, bsz=149.8, num_updates=34700, lr=0.00016976, gnorm=0.941, train_wall=15, wall=5548
2024-05-31 17:46:21 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:46:21 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2024-05-31 17:46:21 | INFO | train | epoch 034 | loss 5.964 | nll_loss 4.782 | ppl 27.52 | wps 25223.7 | ups 6.4 | wpb 3943.1 | bsz 156.9 | num_updates 34714 | lr 0.000169726 | gnorm 0.922 | train_wall 157 | wall 5550
2024-05-31 17:46:21 | INFO | fairseq.trainer | begin training epoch 35
2024-05-31 17:46:35 | INFO | train_inner | epoch 035:     86 / 1021 loss=5.912, nll_loss=4.722, ppl=26.4, wps=24196.2, ups=6.15, wpb=3937.1, bsz=150.3, num_updates=34800, lr=0.000169516, gnorm=0.913, train_wall=16, wall=5564
2024-05-31 17:46:51 | INFO | train_inner | epoch 035:    186 / 1021 loss=5.927, nll_loss=4.738, ppl=26.68, wps=24926.8, ups=6.32, wpb=3943.1, bsz=146.3, num_updates=34900, lr=0.000169273, gnorm=0.906, train_wall=16, wall=5580
2024-05-31 17:47:06 | INFO | train_inner | epoch 035:    286 / 1021 loss=5.929, nll_loss=4.742, ppl=26.76, wps=25064, ups=6.35, wpb=3950.1, bsz=162, num_updates=35000, lr=0.000169031, gnorm=0.925, train_wall=16, wall=5596
2024-05-31 17:47:06 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:47:07 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_35_35000.pt (epoch 35 @ 35000 updates, score None) (writing took 0.9738854221068323 seconds)
2024-05-31 17:47:23 | INFO | train_inner | epoch 035:    386 / 1021 loss=5.948, nll_loss=4.763, ppl=27.15, wps=23965.5, ups=6.07, wpb=3949.6, bsz=153.7, num_updates=35100, lr=0.00016879, gnorm=0.925, train_wall=15, wall=5612
2024-05-31 17:47:38 | INFO | train_inner | epoch 035:    486 / 1021 loss=5.947, nll_loss=4.761, ppl=27.11, wps=25225, ups=6.38, wpb=3951.1, bsz=144.5, num_updates=35200, lr=0.00016855, gnorm=0.91, train_wall=16, wall=5628
2024-05-31 17:47:54 | INFO | train_inner | epoch 035:    586 / 1021 loss=5.943, nll_loss=4.758, ppl=27.05, wps=25122, ups=6.4, wpb=3926.2, bsz=167.7, num_updates=35300, lr=0.000168311, gnorm=0.928, train_wall=15, wall=5643
2024-05-31 17:48:10 | INFO | train_inner | epoch 035:    686 / 1021 loss=5.975, nll_loss=4.795, ppl=27.76, wps=25180.8, ups=6.35, wpb=3966.1, bsz=176.2, num_updates=35400, lr=0.000168073, gnorm=0.952, train_wall=16, wall=5659
2024-05-31 17:48:25 | INFO | train_inner | epoch 035:    786 / 1021 loss=5.984, nll_loss=4.803, ppl=27.92, wps=25189.3, ups=6.42, wpb=3925.6, bsz=139, num_updates=35500, lr=0.000167836, gnorm=0.92, train_wall=15, wall=5675
2024-05-31 17:48:41 | INFO | train_inner | epoch 035:    886 / 1021 loss=5.968, nll_loss=4.788, ppl=27.62, wps=24934.8, ups=6.31, wpb=3949.1, bsz=178.3, num_updates=35600, lr=0.0001676, gnorm=0.988, train_wall=16, wall=5691
2024-05-31 17:48:58 | INFO | train_inner | epoch 035:    986 / 1021 loss=5.989, nll_loss=4.81, ppl=28.05, wps=22920.2, ups=5.86, wpb=3910.4, bsz=150.5, num_updates=35700, lr=0.000167365, gnorm=0.934, train_wall=17, wall=5708
2024-05-31 17:49:05 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:49:05 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2024-05-31 17:49:05 | INFO | train | epoch 035 | loss 5.953 | nll_loss 4.769 | ppl 27.26 | wps 24545.9 | ups 6.23 | wpb 3943.1 | bsz 156.9 | num_updates 35735 | lr 0.000167284 | gnorm 0.929 | train_wall 161 | wall 5714
2024-05-31 17:49:05 | INFO | fairseq.trainer | begin training epoch 36
2024-05-31 17:49:16 | INFO | train_inner | epoch 036:     65 / 1021 loss=5.918, nll_loss=4.73, ppl=26.54, wps=22914.2, ups=5.78, wpb=3962.9, bsz=171.6, num_updates=35800, lr=0.000167132, gnorm=0.927, train_wall=17, wall=5725
2024-05-31 17:49:32 | INFO | train_inner | epoch 036:    165 / 1021 loss=5.895, nll_loss=4.702, ppl=26.02, wps=23712, ups=6.04, wpb=3928.8, bsz=165.1, num_updates=35900, lr=0.000166899, gnorm=0.983, train_wall=16, wall=5742
2024-05-31 17:49:48 | INFO | train_inner | epoch 036:    265 / 1021 loss=5.93, nll_loss=4.741, ppl=26.75, wps=24563.8, ups=6.22, wpb=3946.6, bsz=159.8, num_updates=36000, lr=0.000166667, gnorm=0.93, train_wall=16, wall=5758
2024-05-31 17:49:48 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:49:49 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_36_36000.pt (epoch 36 @ 36000 updates, score None) (writing took 0.9750823806971312 seconds)
2024-05-31 17:50:04 | INFO | train_inner | epoch 036:    365 / 1021 loss=5.945, nll_loss=4.757, ppl=27.04, wps=24679.9, ups=6.26, wpb=3941.5, bsz=141, num_updates=36100, lr=0.000166436, gnorm=0.909, train_wall=15, wall=5774
2024-05-31 17:50:20 | INFO | train_inner | epoch 036:    465 / 1021 loss=5.922, nll_loss=4.733, ppl=26.59, wps=24717.6, ups=6.3, wpb=3921.2, bsz=167.3, num_updates=36200, lr=0.000166206, gnorm=0.966, train_wall=16, wall=5789
2024-05-31 17:50:36 | INFO | train_inner | epoch 036:    565 / 1021 loss=5.938, nll_loss=4.751, ppl=26.93, wps=25173.2, ups=6.34, wpb=3969.5, bsz=162.1, num_updates=36300, lr=0.000165977, gnorm=0.902, train_wall=16, wall=5805
2024-05-31 17:50:52 | INFO | train_inner | epoch 036:    665 / 1021 loss=5.945, nll_loss=4.758, ppl=27.05, wps=24947.1, ups=6.39, wpb=3904, bsz=141.5, num_updates=36400, lr=0.000165748, gnorm=0.926, train_wall=15, wall=5821
2024-05-31 17:51:07 | INFO | train_inner | epoch 036:    765 / 1021 loss=5.963, nll_loss=4.779, ppl=27.46, wps=25135.6, ups=6.32, wpb=3974.1, bsz=153.3, num_updates=36500, lr=0.000165521, gnorm=0.905, train_wall=16, wall=5837
2024-05-31 17:51:23 | INFO | train_inner | epoch 036:    865 / 1021 loss=5.988, nll_loss=4.809, ppl=28.03, wps=25151.8, ups=6.39, wpb=3939.1, bsz=155.8, num_updates=36600, lr=0.000165295, gnorm=0.948, train_wall=16, wall=5852
2024-05-31 17:51:39 | INFO | train_inner | epoch 036:    965 / 1021 loss=5.976, nll_loss=4.795, ppl=27.76, wps=25017.3, ups=6.32, wpb=3957.4, bsz=159.6, num_updates=36700, lr=0.00016507, gnorm=0.918, train_wall=16, wall=5868
2024-05-31 17:51:49 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:51:49 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2024-05-31 17:51:49 | INFO | train | epoch 036 | loss 5.943 | nll_loss 4.756 | ppl 27.03 | wps 24564.9 | ups 6.23 | wpb 3943.1 | bsz 156.9 | num_updates 36756 | lr 0.000164944 | gnorm 0.933 | train_wall 161 | wall 5878
2024-05-31 17:51:49 | INFO | fairseq.trainer | begin training epoch 37
2024-05-31 17:51:56 | INFO | train_inner | epoch 037:     44 / 1021 loss=5.94, nll_loss=4.752, ppl=26.95, wps=23288.2, ups=5.91, wpb=3941.3, bsz=145.4, num_updates=36800, lr=0.000164845, gnorm=0.92, train_wall=17, wall=5885
2024-05-31 17:52:12 | INFO | train_inner | epoch 037:    144 / 1021 loss=5.892, nll_loss=4.698, ppl=25.96, wps=24880.3, ups=6.33, wpb=3929, bsz=170.3, num_updates=36900, lr=0.000164622, gnorm=0.954, train_wall=16, wall=5901
2024-05-31 17:52:27 | INFO | train_inner | epoch 037:    244 / 1021 loss=5.895, nll_loss=4.702, ppl=26.03, wps=24979.4, ups=6.35, wpb=3933.7, bsz=170.9, num_updates=37000, lr=0.000164399, gnorm=0.943, train_wall=16, wall=5917
2024-05-31 17:52:27 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:52:28 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_37_37000.pt (epoch 37 @ 37000 updates, score None) (writing took 0.9745637131854892 seconds)
2024-05-31 17:52:43 | INFO | train_inner | epoch 037:    344 / 1021 loss=5.916, nll_loss=4.725, ppl=26.44, wps=24927.6, ups=6.35, wpb=3928, bsz=151.6, num_updates=37100, lr=0.000164177, gnorm=0.956, train_wall=15, wall=5932
2024-05-31 17:52:58 | INFO | train_inner | epoch 037:    444 / 1021 loss=5.934, nll_loss=4.745, ppl=26.82, wps=26686.9, ups=6.79, wpb=3929.4, bsz=144.8, num_updates=37200, lr=0.000163956, gnorm=0.933, train_wall=15, wall=5947
2024-05-31 17:53:13 | INFO | train_inner | epoch 037:    544 / 1021 loss=5.938, nll_loss=4.75, ppl=26.92, wps=26693.8, ups=6.75, wpb=3954.4, bsz=154.8, num_updates=37300, lr=0.000163737, gnorm=0.94, train_wall=15, wall=5962
2024-05-31 17:53:27 | INFO | train_inner | epoch 037:    644 / 1021 loss=5.954, nll_loss=4.768, ppl=27.25, wps=26703.5, ups=6.75, wpb=3954.5, bsz=158, num_updates=37400, lr=0.000163517, gnorm=0.94, train_wall=15, wall=5977
2024-05-31 17:53:43 | INFO | train_inner | epoch 037:    744 / 1021 loss=5.937, nll_loss=4.751, ppl=26.92, wps=25083.8, ups=6.34, wpb=3953.5, bsz=167.2, num_updates=37500, lr=0.000163299, gnorm=0.938, train_wall=16, wall=5993
2024-05-31 17:53:59 | INFO | train_inner | epoch 037:    844 / 1021 loss=5.964, nll_loss=4.78, ppl=27.48, wps=24999, ups=6.33, wpb=3949.8, bsz=150.4, num_updates=37600, lr=0.000163082, gnorm=0.934, train_wall=16, wall=6008
2024-05-31 17:54:15 | INFO | train_inner | epoch 037:    944 / 1021 loss=5.963, nll_loss=4.779, ppl=27.45, wps=25117.6, ups=6.34, wpb=3963.9, bsz=163.8, num_updates=37700, lr=0.000162866, gnorm=0.981, train_wall=16, wall=6024
2024-05-31 17:54:27 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:54:27 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2024-05-31 17:54:27 | INFO | train | epoch 037 | loss 5.934 | nll_loss 4.745 | ppl 26.82 | wps 25437.8 | ups 6.45 | wpb 3943.1 | bsz 156.9 | num_updates 37777 | lr 0.0001627 | gnorm 0.943 | train_wall 156 | wall 6036
2024-05-31 17:54:27 | INFO | fairseq.trainer | begin training epoch 38
2024-05-31 17:54:31 | INFO | train_inner | epoch 038:     23 / 1021 loss=5.962, nll_loss=4.778, ppl=27.44, wps=23950.3, ups=6.11, wpb=3921.9, bsz=141.6, num_updates=37800, lr=0.00016265, gnorm=0.938, train_wall=16, wall=6040
2024-05-31 17:54:47 | INFO | train_inner | epoch 038:    123 / 1021 loss=5.884, nll_loss=4.689, ppl=25.79, wps=24267.7, ups=6.16, wpb=3936.5, bsz=165.2, num_updates=37900, lr=0.000162435, gnorm=0.963, train_wall=16, wall=6057
2024-05-31 17:55:03 | INFO | train_inner | epoch 038:    223 / 1021 loss=5.886, nll_loss=4.689, ppl=25.79, wps=24910.6, ups=6.31, wpb=3949.1, bsz=142.6, num_updates=38000, lr=0.000162221, gnorm=0.92, train_wall=16, wall=6073
2024-05-31 17:55:03 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:55:04 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_38_38000.pt (epoch 38 @ 38000 updates, score None) (writing took 1.0766105679795146 seconds)
2024-05-31 17:55:19 | INFO | train_inner | epoch 038:    323 / 1021 loss=5.92, nll_loss=4.729, ppl=26.52, wps=24874.1, ups=6.33, wpb=3928.9, bsz=153.8, num_updates=38100, lr=0.000162008, gnorm=0.95, train_wall=15, wall=6088
2024-05-31 17:55:34 | INFO | train_inner | epoch 038:    423 / 1021 loss=5.91, nll_loss=4.718, ppl=26.31, wps=26897, ups=6.79, wpb=3961.9, bsz=159.6, num_updates=38200, lr=0.000161796, gnorm=0.932, train_wall=15, wall=6103
2024-05-31 17:55:48 | INFO | train_inner | epoch 038:    523 / 1021 loss=5.919, nll_loss=4.728, ppl=26.5, wps=26765, ups=6.77, wpb=3951.2, bsz=159.7, num_updates=38300, lr=0.000161585, gnorm=0.942, train_wall=15, wall=6118
2024-05-31 17:56:03 | INFO | train_inner | epoch 038:    623 / 1021 loss=5.915, nll_loss=4.724, ppl=26.43, wps=26811.6, ups=6.78, wpb=3951.7, bsz=158.9, num_updates=38400, lr=0.000161374, gnorm=0.924, train_wall=15, wall=6133
2024-05-31 17:56:18 | INFO | train_inner | epoch 038:    723 / 1021 loss=5.951, nll_loss=4.764, ppl=27.17, wps=26799.3, ups=6.8, wpb=3943.2, bsz=150.6, num_updates=38500, lr=0.000161165, gnorm=0.936, train_wall=15, wall=6147
2024-05-31 17:56:33 | INFO | train_inner | epoch 038:    823 / 1021 loss=5.93, nll_loss=4.742, ppl=26.75, wps=26772.6, ups=6.76, wpb=3961, bsz=162.6, num_updates=38600, lr=0.000160956, gnorm=0.933, train_wall=15, wall=6162
2024-05-31 17:56:48 | INFO | train_inner | epoch 038:    923 / 1021 loss=5.968, nll_loss=4.784, ppl=27.54, wps=25605.7, ups=6.55, wpb=3908.5, bsz=150.2, num_updates=38700, lr=0.000160748, gnorm=0.975, train_wall=15, wall=6177
2024-05-31 17:57:03 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:57:03 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2024-05-31 17:57:03 | INFO | train | epoch 038 | loss 5.924 | nll_loss 4.734 | ppl 26.62 | wps 25723.7 | ups 6.52 | wpb 3943.1 | bsz 156.9 | num_updates 38798 | lr 0.000160544 | gnorm 0.947 | train_wall 154 | wall 6193
2024-05-31 17:57:03 | INFO | fairseq.trainer | begin training epoch 39
2024-05-31 17:57:04 | INFO | train_inner | epoch 039:      2 / 1021 loss=5.963, nll_loss=4.779, ppl=27.46, wps=24947.2, ups=6.32, wpb=3948.2, bsz=164.6, num_updates=38800, lr=0.00016054, gnorm=0.99, train_wall=15, wall=6193
2024-05-31 17:57:20 | INFO | train_inner | epoch 039:    102 / 1021 loss=5.871, nll_loss=4.673, ppl=25.52, wps=24987.9, ups=6.36, wpb=3927.8, bsz=156.5, num_updates=38900, lr=0.000160334, gnorm=0.984, train_wall=16, wall=6209
2024-05-31 17:57:35 | INFO | train_inner | epoch 039:    202 / 1021 loss=5.872, nll_loss=4.675, ppl=25.54, wps=25175.6, ups=6.36, wpb=3961.2, bsz=167.6, num_updates=39000, lr=0.000160128, gnorm=0.952, train_wall=16, wall=6225
2024-05-31 17:57:35 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:57:36 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_39_39000.pt (epoch 39 @ 39000 updates, score None) (writing took 0.9712175037711859 seconds)
2024-05-31 17:57:51 | INFO | train_inner | epoch 039:    302 / 1021 loss=5.909, nll_loss=4.715, ppl=26.27, wps=25254.9, ups=6.39, wpb=3954.6, bsz=149.9, num_updates=39100, lr=0.000159923, gnorm=0.942, train_wall=15, wall=6240
2024-05-31 17:58:06 | INFO | train_inner | epoch 039:    402 / 1021 loss=5.888, nll_loss=4.692, ppl=25.85, wps=26914, ups=6.8, wpb=3960.8, bsz=161.6, num_updates=39200, lr=0.000159719, gnorm=0.947, train_wall=15, wall=6255
2024-05-31 17:58:20 | INFO | train_inner | epoch 039:    502 / 1021 loss=5.912, nll_loss=4.721, ppl=26.38, wps=26781.2, ups=6.78, wpb=3949.7, bsz=185.1, num_updates=39300, lr=0.000159516, gnorm=0.96, train_wall=15, wall=6270
2024-05-31 17:58:36 | INFO | train_inner | epoch 039:    602 / 1021 loss=5.927, nll_loss=4.736, ppl=26.65, wps=25919.6, ups=6.56, wpb=3948.8, bsz=146.4, num_updates=39400, lr=0.000159313, gnorm=0.945, train_wall=15, wall=6285
2024-05-31 17:58:51 | INFO | train_inner | epoch 039:    702 / 1021 loss=5.934, nll_loss=4.745, ppl=26.82, wps=25137.6, ups=6.37, wpb=3946.6, bsz=161, num_updates=39500, lr=0.000159111, gnorm=0.958, train_wall=16, wall=6301
2024-05-31 17:59:07 | INFO | train_inner | epoch 039:    802 / 1021 loss=5.954, nll_loss=4.766, ppl=27.21, wps=25063.7, ups=6.41, wpb=3908.7, bsz=144.2, num_updates=39600, lr=0.00015891, gnorm=0.965, train_wall=15, wall=6316
2024-05-31 17:59:23 | INFO | train_inner | epoch 039:    902 / 1021 loss=5.928, nll_loss=4.738, ppl=26.69, wps=25054.2, ups=6.36, wpb=3941.1, bsz=148.6, num_updates=39700, lr=0.00015871, gnorm=0.923, train_wall=16, wall=6332
2024-05-31 17:59:38 | INFO | train_inner | epoch 039:   1002 / 1021 loss=5.962, nll_loss=4.775, ppl=27.39, wps=25031.6, ups=6.38, wpb=3925.2, bsz=142.4, num_updates=39800, lr=0.000158511, gnorm=0.957, train_wall=16, wall=6348
2024-05-31 17:59:41 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 17:59:41 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2024-05-31 17:59:41 | INFO | train | epoch 039 | loss 5.916 | nll_loss 4.724 | ppl 26.43 | wps 25475.4 | ups 6.46 | wpb 3943.1 | bsz 156.9 | num_updates 39819 | lr 0.000158473 | gnorm 0.953 | train_wall 155 | wall 6351
2024-05-31 17:59:41 | INFO | fairseq.trainer | begin training epoch 40
2024-05-31 17:59:54 | INFO | train_inner | epoch 040:     81 / 1021 loss=5.873, nll_loss=4.676, ppl=25.56, wps=24780, ups=6.3, wpb=3936, bsz=161.8, num_updates=39900, lr=0.000158312, gnorm=0.993, train_wall=16, wall=6364
2024-05-31 18:00:10 | INFO | train_inner | epoch 040:    181 / 1021 loss=5.875, nll_loss=4.676, ppl=25.56, wps=24747.5, ups=6.32, wpb=3913.2, bsz=145.5, num_updates=40000, lr=0.000158114, gnorm=0.958, train_wall=16, wall=6379
2024-05-31 18:00:10 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:00:11 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_40_40000.pt (epoch 40 @ 40000 updates, score None) (writing took 0.9751576771959662 seconds)
2024-05-31 18:00:26 | INFO | train_inner | epoch 040:    281 / 1021 loss=5.907, nll_loss=4.71, ppl=26.17, wps=24809.8, ups=6.33, wpb=3917.9, bsz=122.7, num_updates=40100, lr=0.000157917, gnorm=0.956, train_wall=15, wall=6395
2024-05-31 18:00:41 | INFO | train_inner | epoch 040:    381 / 1021 loss=5.91, nll_loss=4.716, ppl=26.28, wps=26490.2, ups=6.69, wpb=3960.7, bsz=159.7, num_updates=40200, lr=0.00015772, gnorm=0.969, train_wall=15, wall=6410
2024-05-31 18:00:56 | INFO | train_inner | epoch 040:    481 / 1021 loss=5.892, nll_loss=4.697, ppl=25.93, wps=26059.1, ups=6.63, wpb=3931.7, bsz=166.6, num_updates=40300, lr=0.000157524, gnorm=0.957, train_wall=15, wall=6425
2024-05-31 18:01:12 | INFO | train_inner | epoch 040:    581 / 1021 loss=5.883, nll_loss=4.687, ppl=25.76, wps=24724.7, ups=6.23, wpb=3971.5, bsz=161.7, num_updates=40400, lr=0.000157329, gnorm=0.928, train_wall=16, wall=6441
2024-05-31 18:01:28 | INFO | train_inner | epoch 040:    681 / 1021 loss=5.936, nll_loss=4.747, ppl=26.85, wps=24742.7, ups=6.29, wpb=3934.4, bsz=153.2, num_updates=40500, lr=0.000157135, gnorm=0.993, train_wall=16, wall=6457
2024-05-31 18:01:44 | INFO | train_inner | epoch 040:    781 / 1021 loss=5.915, nll_loss=4.723, ppl=26.41, wps=24775.9, ups=6.25, wpb=3961.1, bsz=165, num_updates=40600, lr=0.000156941, gnorm=0.949, train_wall=16, wall=6473
2024-05-31 18:02:02 | INFO | train_inner | epoch 040:    881 / 1021 loss=5.933, nll_loss=4.744, ppl=26.8, wps=22403.1, ups=5.65, wpb=3964.9, bsz=166.5, num_updates=40700, lr=0.000156748, gnorm=0.972, train_wall=18, wall=6491
2024-05-31 18:02:17 | INFO | train_inner | epoch 040:    981 / 1021 loss=5.944, nll_loss=4.757, ppl=27.03, wps=24834.1, ups=6.31, wpb=3932.6, bsz=173.8, num_updates=40800, lr=0.000156556, gnorm=0.977, train_wall=16, wall=6507
2024-05-31 18:02:24 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:02:24 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2024-05-31 18:02:24 | INFO | train | epoch 040 | loss 5.907 | nll_loss 4.713 | ppl 26.23 | wps 24798.1 | ups 6.29 | wpb 3943.1 | bsz 156.9 | num_updates 40840 | lr 0.000156479 | gnorm 0.963 | train_wall 160 | wall 6513
2024-05-31 18:02:24 | INFO | fairseq.trainer | begin training epoch 41
2024-05-31 18:02:33 | INFO | train_inner | epoch 041:     60 / 1021 loss=5.877, nll_loss=4.679, ppl=25.62, wps=24763.8, ups=6.26, wpb=3956.8, bsz=153.4, num_updates=40900, lr=0.000156365, gnorm=0.933, train_wall=16, wall=6523
2024-05-31 18:02:49 | INFO | train_inner | epoch 041:    160 / 1021 loss=5.867, nll_loss=4.668, ppl=25.42, wps=25073.8, ups=6.33, wpb=3958.2, bsz=166.7, num_updates=41000, lr=0.000156174, gnorm=0.968, train_wall=16, wall=6538
2024-05-31 18:02:49 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:02:50 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_41_41000.pt (epoch 41 @ 41000 updates, score None) (writing took 1.0135423131287098 seconds)
2024-05-31 18:03:05 | INFO | train_inner | epoch 041:    260 / 1021 loss=5.865, nll_loss=4.664, ppl=25.35, wps=25138.1, ups=6.38, wpb=3939.8, bsz=148.2, num_updates=41100, lr=0.000155984, gnorm=0.966, train_wall=15, wall=6554
2024-05-31 18:03:20 | INFO | train_inner | epoch 041:    360 / 1021 loss=5.889, nll_loss=4.692, ppl=25.85, wps=25285.5, ups=6.42, wpb=3937.4, bsz=162, num_updates=41200, lr=0.000155794, gnorm=1.013, train_wall=15, wall=6570
2024-05-31 18:03:36 | INFO | train_inner | epoch 041:    460 / 1021 loss=5.897, nll_loss=4.702, ppl=26.04, wps=25590.9, ups=6.44, wpb=3976.4, bsz=175.7, num_updates=41300, lr=0.000155606, gnorm=0.963, train_wall=15, wall=6585
2024-05-31 18:03:51 | INFO | train_inner | epoch 041:    560 / 1021 loss=5.9, nll_loss=4.705, ppl=26.09, wps=25315.1, ups=6.42, wpb=3943.2, bsz=159.1, num_updates=41400, lr=0.000155417, gnorm=0.96, train_wall=15, wall=6601
2024-05-31 18:04:07 | INFO | train_inner | epoch 041:    660 / 1021 loss=5.913, nll_loss=4.72, ppl=26.36, wps=25474.1, ups=6.5, wpb=3921.9, bsz=162.9, num_updates=41500, lr=0.00015523, gnorm=0.973, train_wall=15, wall=6616
2024-05-31 18:04:23 | INFO | train_inner | epoch 041:    760 / 1021 loss=5.912, nll_loss=4.717, ppl=26.3, wps=25083.9, ups=6.37, wpb=3937.5, bsz=142.6, num_updates=41600, lr=0.000155043, gnorm=0.948, train_wall=16, wall=6632
2024-05-31 18:04:38 | INFO | train_inner | epoch 041:    860 / 1021 loss=5.942, nll_loss=4.75, ppl=26.92, wps=25038.8, ups=6.4, wpb=3915.3, bsz=125.2, num_updates=41700, lr=0.000154857, gnorm=0.968, train_wall=15, wall=6648
2024-05-31 18:04:54 | INFO | train_inner | epoch 041:    960 / 1021 loss=5.931, nll_loss=4.742, ppl=26.75, wps=25239.8, ups=6.4, wpb=3941.4, bsz=171.3, num_updates=41800, lr=0.000154672, gnorm=0.987, train_wall=15, wall=6663
2024-05-31 18:05:03 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:05:03 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2024-05-31 18:05:03 | INFO | train | epoch 041 | loss 5.9 | nll_loss 4.705 | ppl 26.08 | wps 25197.2 | ups 6.39 | wpb 3943.1 | bsz 156.9 | num_updates 41861 | lr 0.000154559 | gnorm 0.968 | train_wall 157 | wall 6673
2024-05-31 18:05:04 | INFO | fairseq.trainer | begin training epoch 42
2024-05-31 18:05:10 | INFO | train_inner | epoch 042:     39 / 1021 loss=5.904, nll_loss=4.71, ppl=26.17, wps=24865.2, ups=6.3, wpb=3944.8, bsz=145.9, num_updates=41900, lr=0.000154487, gnorm=0.947, train_wall=16, wall=6679
2024-05-31 18:05:25 | INFO | train_inner | epoch 042:    139 / 1021 loss=5.85, nll_loss=4.648, ppl=25.08, wps=25216.8, ups=6.36, wpb=3967.1, bsz=169.8, num_updates=42000, lr=0.000154303, gnorm=0.985, train_wall=16, wall=6695
2024-05-31 18:05:25 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:05:26 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_42_42000.pt (epoch 42 @ 42000 updates, score None) (writing took 0.9747077091597021 seconds)
2024-05-31 18:05:42 | INFO | train_inner | epoch 042:    239 / 1021 loss=5.868, nll_loss=4.666, ppl=25.39, wps=24249.3, ups=6.16, wpb=3935.8, bsz=144.2, num_updates=42100, lr=0.00015412, gnorm=0.957, train_wall=15, wall=6711
2024-05-31 18:05:57 | INFO | train_inner | epoch 042:    339 / 1021 loss=5.876, nll_loss=4.677, ppl=25.58, wps=25181.3, ups=6.37, wpb=3954.4, bsz=166.3, num_updates=42200, lr=0.000153937, gnorm=1.01, train_wall=16, wall=6727
2024-05-31 18:06:13 | INFO | train_inner | epoch 042:    439 / 1021 loss=5.892, nll_loss=4.693, ppl=25.87, wps=25160.9, ups=6.36, wpb=3958.5, bsz=142.2, num_updates=42300, lr=0.000153755, gnorm=0.942, train_wall=16, wall=6742
2024-05-31 18:06:29 | INFO | train_inner | epoch 042:    539 / 1021 loss=5.885, nll_loss=4.687, ppl=25.76, wps=24612.3, ups=6.26, wpb=3933.9, bsz=160.4, num_updates=42400, lr=0.000153574, gnorm=0.978, train_wall=16, wall=6758
2024-05-31 18:06:45 | INFO | train_inner | epoch 042:    639 / 1021 loss=5.893, nll_loss=4.697, ppl=25.93, wps=25078.9, ups=6.34, wpb=3958.6, bsz=165.7, num_updates=42500, lr=0.000153393, gnorm=0.966, train_wall=16, wall=6774
2024-05-31 18:07:00 | INFO | train_inner | epoch 042:    739 / 1021 loss=5.913, nll_loss=4.72, ppl=26.36, wps=25189.5, ups=6.41, wpb=3931, bsz=171.2, num_updates=42600, lr=0.000153213, gnorm=0.978, train_wall=15, wall=6790
2024-05-31 18:07:16 | INFO | train_inner | epoch 042:    839 / 1021 loss=5.907, nll_loss=4.714, ppl=26.24, wps=25219.6, ups=6.39, wpb=3944.5, bsz=162.3, num_updates=42700, lr=0.000153033, gnorm=0.96, train_wall=15, wall=6805
2024-05-31 18:07:32 | INFO | train_inner | epoch 042:    939 / 1021 loss=5.92, nll_loss=4.727, ppl=26.48, wps=25108.6, ups=6.36, wpb=3945.7, bsz=146.7, num_updates=42800, lr=0.000152854, gnorm=0.97, train_wall=16, wall=6821
2024-05-31 18:07:45 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:07:45 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2024-05-31 18:07:45 | INFO | train | epoch 042 | loss 5.891 | nll_loss 4.694 | ppl 25.89 | wps 24977.5 | ups 6.33 | wpb 3943.1 | bsz 156.9 | num_updates 42882 | lr 0.000152708 | gnorm 0.972 | train_wall 158 | wall 6834
2024-05-31 18:07:45 | INFO | fairseq.trainer | begin training epoch 43
2024-05-31 18:07:48 | INFO | train_inner | epoch 043:     18 / 1021 loss=5.911, nll_loss=4.718, ppl=26.31, wps=24724, ups=6.31, wpb=3916.4, bsz=156.4, num_updates=42900, lr=0.000152676, gnorm=0.969, train_wall=16, wall=6837
2024-05-31 18:08:03 | INFO | train_inner | epoch 043:    118 / 1021 loss=5.837, nll_loss=4.633, ppl=24.81, wps=24790.6, ups=6.32, wpb=3921.3, bsz=173.7, num_updates=43000, lr=0.000152499, gnorm=1.028, train_wall=16, wall=6853
2024-05-31 18:08:03 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:08:04 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_43_43000.pt (epoch 43 @ 43000 updates, score None) (writing took 0.9751063450239599 seconds)
2024-05-31 18:08:19 | INFO | train_inner | epoch 043:    218 / 1021 loss=5.85, nll_loss=4.646, ppl=25.03, wps=24899, ups=6.31, wpb=3944.5, bsz=151.7, num_updates=43100, lr=0.000152322, gnorm=0.959, train_wall=15, wall=6869
2024-05-31 18:08:34 | INFO | train_inner | epoch 043:    318 / 1021 loss=5.887, nll_loss=4.688, ppl=25.77, wps=26512, ups=6.72, wpb=3947.9, bsz=158.8, num_updates=43200, lr=0.000152145, gnorm=0.99, train_wall=15, wall=6884
2024-05-31 18:08:49 | INFO | train_inner | epoch 043:    418 / 1021 loss=5.867, nll_loss=4.665, ppl=25.38, wps=26362.7, ups=6.71, wpb=3928.7, bsz=151.2, num_updates=43300, lr=0.000151969, gnorm=0.969, train_wall=15, wall=6899
2024-05-31 18:09:04 | INFO | train_inner | epoch 043:    518 / 1021 loss=5.863, nll_loss=4.662, ppl=25.32, wps=26396.3, ups=6.72, wpb=3930, bsz=163.5, num_updates=43400, lr=0.000151794, gnorm=0.97, train_wall=15, wall=6913
2024-05-31 18:09:19 | INFO | train_inner | epoch 043:    618 / 1021 loss=5.893, nll_loss=4.695, ppl=25.9, wps=26444, ups=6.69, wpb=3954.8, bsz=149.7, num_updates=43500, lr=0.00015162, gnorm=0.98, train_wall=15, wall=6928
2024-05-31 18:09:34 | INFO | train_inner | epoch 043:    718 / 1021 loss=5.893, nll_loss=4.696, ppl=25.93, wps=26425.9, ups=6.68, wpb=3958.4, bsz=158.6, num_updates=43600, lr=0.000151446, gnorm=0.975, train_wall=15, wall=6943
2024-05-31 18:09:49 | INFO | train_inner | epoch 043:    818 / 1021 loss=5.913, nll_loss=4.72, ppl=26.35, wps=26507.7, ups=6.68, wpb=3966.8, bsz=157, num_updates=43700, lr=0.000151272, gnorm=0.964, train_wall=15, wall=6958
2024-05-31 18:10:06 | INFO | train_inner | epoch 043:    918 / 1021 loss=5.917, nll_loss=4.723, ppl=26.41, wps=22774.7, ups=5.8, wpb=3928.8, bsz=151.5, num_updates=43800, lr=0.000151099, gnorm=0.983, train_wall=17, wall=6976
2024-05-31 18:10:22 | INFO | train_inner | epoch 043:   1018 / 1021 loss=5.908, nll_loss=4.714, ppl=26.24, wps=25101.9, ups=6.36, wpb=3945.6, bsz=151.3, num_updates=43900, lr=0.000150927, gnorm=0.965, train_wall=16, wall=6991
2024-05-31 18:10:22 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:10:22 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2024-05-31 18:10:22 | INFO | train | epoch 043 | loss 5.882 | nll_loss 4.683 | ppl 25.69 | wps 25524 | ups 6.47 | wpb 3943.1 | bsz 156.9 | num_updates 43903 | lr 0.000150922 | gnorm 0.977 | train_wall 155 | wall 6992
2024-05-31 18:10:22 | INFO | fairseq.trainer | begin training epoch 44
2024-05-31 18:10:38 | INFO | train_inner | epoch 044:     97 / 1021 loss=5.834, nll_loss=4.628, ppl=24.73, wps=24794.8, ups=6.29, wpb=3939.6, bsz=146.9, num_updates=44000, lr=0.000150756, gnorm=1.002, train_wall=16, wall=7007
2024-05-31 18:10:38 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:10:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_44_44000.pt (epoch 44 @ 44000 updates, score None) (writing took 0.9905113303102553 seconds)
2024-05-31 18:10:54 | INFO | train_inner | epoch 044:    197 / 1021 loss=5.84, nll_loss=4.636, ppl=24.87, wps=25169.4, ups=6.35, wpb=3963.3, bsz=173, num_updates=44100, lr=0.000150585, gnorm=0.993, train_wall=15, wall=7023
2024-05-31 18:11:08 | INFO | train_inner | epoch 044:    297 / 1021 loss=5.855, nll_loss=4.652, ppl=25.15, wps=26746.8, ups=6.77, wpb=3952.3, bsz=164.2, num_updates=44200, lr=0.000150414, gnorm=0.983, train_wall=15, wall=7038
2024-05-31 18:11:23 | INFO | train_inner | epoch 044:    397 / 1021 loss=5.858, nll_loss=4.654, ppl=25.18, wps=26683.7, ups=6.76, wpb=3949.7, bsz=150.4, num_updates=44300, lr=0.000150244, gnorm=0.976, train_wall=15, wall=7052
2024-05-31 18:11:38 | INFO | train_inner | epoch 044:    497 / 1021 loss=5.87, nll_loss=4.669, ppl=25.44, wps=26656.1, ups=6.75, wpb=3951.5, bsz=164.1, num_updates=44400, lr=0.000150075, gnorm=0.976, train_wall=15, wall=7067
2024-05-31 18:11:53 | INFO | train_inner | epoch 044:    597 / 1021 loss=5.88, nll_loss=4.68, ppl=25.63, wps=26721.7, ups=6.77, wpb=3949.3, bsz=143.3, num_updates=44500, lr=0.000149906, gnorm=0.957, train_wall=15, wall=7082
2024-05-31 18:12:07 | INFO | train_inner | epoch 044:    697 / 1021 loss=5.895, nll_loss=4.698, ppl=25.95, wps=26782.8, ups=6.77, wpb=3955, bsz=146.5, num_updates=44600, lr=0.000149738, gnorm=0.962, train_wall=15, wall=7097
2024-05-31 18:12:23 | INFO | train_inner | epoch 044:    797 / 1021 loss=5.906, nll_loss=4.71, ppl=26.18, wps=24762.3, ups=6.34, wpb=3904.9, bsz=152.6, num_updates=44700, lr=0.000149571, gnorm=1.01, train_wall=16, wall=7113
2024-05-31 18:12:39 | INFO | train_inner | epoch 044:    897 / 1021 loss=5.893, nll_loss=4.695, ppl=25.91, wps=24103.7, ups=6.18, wpb=3901.6, bsz=157.4, num_updates=44800, lr=0.000149404, gnorm=1.012, train_wall=16, wall=7129
2024-05-31 18:12:56 | INFO | train_inner | epoch 044:    997 / 1021 loss=5.906, nll_loss=4.712, ppl=26.21, wps=23295.7, ups=5.87, wpb=3967.1, bsz=173.4, num_updates=44900, lr=0.000149237, gnorm=0.974, train_wall=17, wall=7146
2024-05-31 18:13:00 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:13:00 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2024-05-31 18:13:00 | INFO | train | epoch 044 | loss 5.875 | nll_loss 4.675 | ppl 25.54 | wps 25467 | ups 6.46 | wpb 3943.1 | bsz 156.9 | num_updates 44924 | lr 0.000149197 | gnorm 0.984 | train_wall 156 | wall 7150
2024-05-31 18:13:01 | INFO | fairseq.trainer | begin training epoch 45
2024-05-31 18:13:13 | INFO | train_inner | epoch 045:     76 / 1021 loss=5.839, nll_loss=4.633, ppl=24.81, wps=23230.1, ups=5.91, wpb=3933.7, bsz=150.2, num_updates=45000, lr=0.000149071, gnorm=1.025, train_wall=17, wall=7163
2024-05-31 18:13:13 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:13:14 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_45_45000.pt (epoch 45 @ 45000 updates, score None) (writing took 0.9645255720242858 seconds)
2024-05-31 18:13:31 | INFO | train_inner | epoch 045:    176 / 1021 loss=5.846, nll_loss=4.642, ppl=24.96, wps=22364.3, ups=5.68, wpb=3938, bsz=163.1, num_updates=45100, lr=0.000148906, gnorm=1.004, train_wall=17, wall=7180
2024-05-31 18:13:47 | INFO | train_inner | epoch 045:    276 / 1021 loss=5.838, nll_loss=4.632, ppl=24.8, wps=24079.6, ups=6.12, wpb=3937.6, bsz=163.8, num_updates=45200, lr=0.000148741, gnorm=0.994, train_wall=16, wall=7197
2024-05-31 18:14:03 | INFO | train_inner | epoch 045:    376 / 1021 loss=5.845, nll_loss=4.639, ppl=24.92, wps=25000.5, ups=6.35, wpb=3934.8, bsz=151.8, num_updates=45300, lr=0.000148577, gnorm=0.961, train_wall=16, wall=7212
2024-05-31 18:14:19 | INFO | train_inner | epoch 045:    476 / 1021 loss=5.87, nll_loss=4.67, ppl=25.46, wps=24901.1, ups=6.3, wpb=3953.8, bsz=174.2, num_updates=45400, lr=0.000148413, gnorm=1.007, train_wall=16, wall=7228
2024-05-31 18:14:35 | INFO | train_inner | epoch 045:    576 / 1021 loss=5.878, nll_loss=4.678, ppl=25.6, wps=25093.1, ups=6.33, wpb=3965.8, bsz=159.6, num_updates=45500, lr=0.00014825, gnorm=0.974, train_wall=16, wall=7244
2024-05-31 18:14:51 | INFO | train_inner | epoch 045:    676 / 1021 loss=5.868, nll_loss=4.666, ppl=25.39, wps=24914.3, ups=6.34, wpb=3928.3, bsz=165.6, num_updates=45600, lr=0.000148087, gnorm=1.001, train_wall=16, wall=7260
2024-05-31 18:15:06 | INFO | train_inner | epoch 045:    776 / 1021 loss=5.9, nll_loss=4.701, ppl=26.01, wps=24928.3, ups=6.36, wpb=3917, bsz=136.9, num_updates=45700, lr=0.000147925, gnorm=1.002, train_wall=16, wall=7276
2024-05-31 18:15:22 | INFO | train_inner | epoch 045:    876 / 1021 loss=5.889, nll_loss=4.691, ppl=25.82, wps=25047.5, ups=6.34, wpb=3950.7, bsz=155.4, num_updates=45800, lr=0.000147764, gnorm=0.98, train_wall=16, wall=7291
2024-05-31 18:15:38 | INFO | train_inner | epoch 045:    976 / 1021 loss=5.899, nll_loss=4.702, ppl=26.02, wps=24914.5, ups=6.27, wpb=3971.6, bsz=153.5, num_updates=45900, lr=0.000147602, gnorm=0.978, train_wall=16, wall=7307
2024-05-31 18:15:45 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:15:45 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2024-05-31 18:15:45 | INFO | train | epoch 045 | loss 5.868 | nll_loss 4.666 | ppl 25.39 | wps 24456.1 | ups 6.2 | wpb 3943.1 | bsz 156.9 | num_updates 45945 | lr 0.00014753 | gnorm 0.992 | train_wall 162 | wall 7314
2024-05-31 18:15:45 | INFO | fairseq.trainer | begin training epoch 46
2024-05-31 18:15:55 | INFO | train_inner | epoch 046:     55 / 1021 loss=5.863, nll_loss=4.661, ppl=25.3, wps=23239.4, ups=5.9, wpb=3939.2, bsz=152.4, num_updates=46000, lr=0.000147442, gnorm=0.985, train_wall=17, wall=7324
2024-05-31 18:15:55 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:15:56 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_46_46000.pt (epoch 46 @ 46000 updates, score None) (writing took 0.9866505693644285 seconds)
2024-05-31 18:16:13 | INFO | train_inner | epoch 046:    155 / 1021 loss=5.812, nll_loss=4.602, ppl=24.29, wps=22453.2, ups=5.66, wpb=3964.2, bsz=155, num_updates=46100, lr=0.000147282, gnorm=0.984, train_wall=17, wall=7342
2024-05-31 18:16:29 | INFO | train_inner | epoch 046:    255 / 1021 loss=5.85, nll_loss=4.643, ppl=24.99, wps=23740.7, ups=6.05, wpb=3922.2, bsz=145.5, num_updates=46200, lr=0.000147122, gnorm=0.998, train_wall=16, wall=7358
2024-05-31 18:16:46 | INFO | train_inner | epoch 046:    355 / 1021 loss=5.842, nll_loss=4.634, ppl=24.84, wps=23664, ups=6.02, wpb=3933.4, bsz=137.8, num_updates=46300, lr=0.000146964, gnorm=0.974, train_wall=16, wall=7375
2024-05-31 18:17:02 | INFO | train_inner | epoch 046:    455 / 1021 loss=5.845, nll_loss=4.64, ppl=24.93, wps=23738.7, ups=6, wpb=3955.8, bsz=158.6, num_updates=46400, lr=0.000146805, gnorm=0.997, train_wall=17, wall=7392
2024-05-31 18:17:19 | INFO | train_inner | epoch 046:    555 / 1021 loss=5.862, nll_loss=4.659, ppl=25.26, wps=23905.6, ups=6.05, wpb=3953.8, bsz=154, num_updates=46500, lr=0.000146647, gnorm=0.982, train_wall=16, wall=7408
2024-05-31 18:17:35 | INFO | train_inner | epoch 046:    655 / 1021 loss=5.864, nll_loss=4.66, ppl=25.29, wps=24215.2, ups=6.15, wpb=3940, bsz=151.2, num_updates=46600, lr=0.00014649, gnorm=0.995, train_wall=16, wall=7425
2024-05-31 18:17:51 | INFO | train_inner | epoch 046:    755 / 1021 loss=5.88, nll_loss=4.681, ppl=25.66, wps=25010.8, ups=6.32, wpb=3959.9, bsz=192.2, num_updates=46700, lr=0.000146333, gnorm=1.071, train_wall=16, wall=7440
2024-05-31 18:18:07 | INFO | train_inner | epoch 046:    855 / 1021 loss=5.882, nll_loss=4.682, ppl=25.67, wps=24849.9, ups=6.35, wpb=3914.4, bsz=156.2, num_updates=46800, lr=0.000146176, gnorm=0.983, train_wall=16, wall=7456
2024-05-31 18:18:22 | INFO | train_inner | epoch 046:    955 / 1021 loss=5.897, nll_loss=4.699, ppl=25.98, wps=25104.4, ups=6.36, wpb=3944.6, bsz=152.2, num_updates=46900, lr=0.00014602, gnorm=1.005, train_wall=16, wall=7472
2024-05-31 18:18:33 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:18:33 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2024-05-31 18:18:33 | INFO | train | epoch 046 | loss 5.861 | nll_loss 4.658 | ppl 25.24 | wps 23994 | ups 6.09 | wpb 3943.1 | bsz 156.9 | num_updates 46966 | lr 0.000145918 | gnorm 0.999 | train_wall 165 | wall 7482
2024-05-31 18:18:33 | INFO | fairseq.trainer | begin training epoch 47
2024-05-31 18:18:38 | INFO | train_inner | epoch 047:     34 / 1021 loss=5.878, nll_loss=4.678, ppl=25.6, wps=24819.4, ups=6.31, wpb=3932.2, bsz=155.8, num_updates=47000, lr=0.000145865, gnorm=0.993, train_wall=16, wall=7488
2024-05-31 18:18:38 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:18:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_47_47000.pt (epoch 47 @ 47000 updates, score None) (writing took 0.9791358453221619 seconds)
2024-05-31 18:18:54 | INFO | train_inner | epoch 047:    134 / 1021 loss=5.793, nll_loss=4.58, ppl=23.91, wps=24997.3, ups=6.37, wpb=3924.1, bsz=156.7, num_updates=47100, lr=0.00014571, gnorm=0.989, train_wall=15, wall=7503
2024-05-31 18:19:09 | INFO | train_inner | epoch 047:    234 / 1021 loss=5.826, nll_loss=4.618, ppl=24.55, wps=26517.4, ups=6.69, wpb=3965.3, bsz=167, num_updates=47200, lr=0.000145556, gnorm=0.999, train_wall=15, wall=7518
2024-05-31 18:19:25 | INFO | train_inner | epoch 047:    334 / 1021 loss=5.857, nll_loss=4.653, ppl=25.15, wps=25170.9, ups=6.37, wpb=3953.5, bsz=170.2, num_updates=47300, lr=0.000145402, gnorm=1.066, train_wall=16, wall=7534
2024-05-31 18:19:40 | INFO | train_inner | epoch 047:    434 / 1021 loss=5.841, nll_loss=4.634, ppl=24.83, wps=25144.1, ups=6.37, wpb=3945.8, bsz=156.4, num_updates=47400, lr=0.000145248, gnorm=0.985, train_wall=16, wall=7550
2024-05-31 18:19:56 | INFO | train_inner | epoch 047:    534 / 1021 loss=5.856, nll_loss=4.651, ppl=25.13, wps=25101.2, ups=6.38, wpb=3936.1, bsz=159.9, num_updates=47500, lr=0.000145095, gnorm=0.993, train_wall=16, wall=7565
2024-05-31 18:20:12 | INFO | train_inner | epoch 047:    634 / 1021 loss=5.873, nll_loss=4.671, ppl=25.47, wps=24984.6, ups=6.32, wpb=3954.9, bsz=156.2, num_updates=47600, lr=0.000144943, gnorm=0.988, train_wall=16, wall=7581
2024-05-31 18:20:28 | INFO | train_inner | epoch 047:    734 / 1021 loss=5.871, nll_loss=4.669, ppl=25.44, wps=25118.3, ups=6.36, wpb=3950.2, bsz=154.2, num_updates=47700, lr=0.000144791, gnorm=1.01, train_wall=16, wall=7597
2024-05-31 18:20:43 | INFO | train_inner | epoch 047:    834 / 1021 loss=5.877, nll_loss=4.676, ppl=25.56, wps=24917.8, ups=6.37, wpb=3912.4, bsz=163, num_updates=47800, lr=0.000144639, gnorm=1.023, train_wall=16, wall=7613
2024-05-31 18:20:59 | INFO | train_inner | epoch 047:    934 / 1021 loss=5.886, nll_loss=4.685, ppl=25.73, wps=24927.5, ups=6.35, wpb=3925.4, bsz=141.8, num_updates=47900, lr=0.000144488, gnorm=0.976, train_wall=16, wall=7628
2024-05-31 18:21:13 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:21:13 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2024-05-31 18:21:13 | INFO | train | epoch 047 | loss 5.854 | nll_loss 4.65 | ppl 25.1 | wps 25177.9 | ups 6.39 | wpb 3943.1 | bsz 156.9 | num_updates 47987 | lr 0.000144357 | gnorm 1.006 | train_wall 157 | wall 7642
2024-05-31 18:21:13 | INFO | fairseq.trainer | begin training epoch 48
2024-05-31 18:21:15 | INFO | train_inner | epoch 048:     13 / 1021 loss=5.878, nll_loss=4.677, ppl=25.59, wps=24922.2, ups=6.27, wpb=3973.5, bsz=159.3, num_updates=48000, lr=0.000144338, gnorm=1.054, train_wall=16, wall=7644
2024-05-31 18:21:15 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:21:16 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_48_48000.pt (epoch 48 @ 48000 updates, score None) (writing took 0.9766759052872658 seconds)
2024-05-31 18:21:32 | INFO | train_inner | epoch 048:    113 / 1021 loss=5.78, nll_loss=4.565, ppl=23.67, wps=23404.2, ups=5.94, wpb=3939.2, bsz=150.8, num_updates=48100, lr=0.000144187, gnorm=0.979, train_wall=16, wall=7661
2024-05-31 18:21:48 | INFO | train_inner | epoch 048:    213 / 1021 loss=5.819, nll_loss=4.607, ppl=24.37, wps=24743.1, ups=6.32, wpb=3917.2, bsz=145.4, num_updates=48200, lr=0.000144038, gnorm=1.024, train_wall=16, wall=7677
2024-05-31 18:22:04 | INFO | train_inner | epoch 048:    313 / 1021 loss=5.855, nll_loss=4.649, ppl=25.08, wps=24767, ups=6.28, wpb=3941.6, bsz=154.6, num_updates=48300, lr=0.000143889, gnorm=1.036, train_wall=16, wall=7693
2024-05-31 18:22:19 | INFO | train_inner | epoch 048:    413 / 1021 loss=5.834, nll_loss=4.624, ppl=24.67, wps=24752.6, ups=6.3, wpb=3928, bsz=147.7, num_updates=48400, lr=0.00014374, gnorm=0.992, train_wall=16, wall=7709
2024-05-31 18:22:35 | INFO | train_inner | epoch 048:    513 / 1021 loss=5.849, nll_loss=4.643, ppl=24.99, wps=24826.5, ups=6.24, wpb=3975.6, bsz=161.4, num_updates=48500, lr=0.000143592, gnorm=0.992, train_wall=16, wall=7725
2024-05-31 18:22:51 | INFO | train_inner | epoch 048:    613 / 1021 loss=5.851, nll_loss=4.646, ppl=25.04, wps=24807.5, ups=6.3, wpb=3939.1, bsz=151.8, num_updates=48600, lr=0.000143444, gnorm=0.994, train_wall=16, wall=7741
2024-05-31 18:23:07 | INFO | train_inner | epoch 048:    713 / 1021 loss=5.86, nll_loss=4.655, ppl=25.2, wps=25106.3, ups=6.42, wpb=3911.4, bsz=151.7, num_updates=48700, lr=0.000143296, gnorm=1.023, train_wall=15, wall=7756
2024-05-31 18:23:23 | INFO | train_inner | epoch 048:    813 / 1021 loss=5.874, nll_loss=4.673, ppl=25.52, wps=25060.5, ups=6.34, wpb=3954.1, bsz=174.9, num_updates=48800, lr=0.00014315, gnorm=1.001, train_wall=16, wall=7772
2024-05-31 18:23:38 | INFO | train_inner | epoch 048:    913 / 1021 loss=5.871, nll_loss=4.67, ppl=25.46, wps=25138.2, ups=6.34, wpb=3962.9, bsz=175.5, num_updates=48900, lr=0.000143003, gnorm=1.051, train_wall=16, wall=7788
2024-05-31 18:23:54 | INFO | train_inner | epoch 048:   1013 / 1021 loss=5.874, nll_loss=4.672, ppl=25.49, wps=25199, ups=6.36, wpb=3963.3, bsz=150.3, num_updates=49000, lr=0.000142857, gnorm=0.965, train_wall=16, wall=7804
2024-05-31 18:23:54 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:23:55 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_48_49000.pt (epoch 48 @ 49000 updates, score None) (writing took 0.9759496720507741 seconds)
2024-05-31 18:23:56 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:23:56 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2024-05-31 18:23:56 | INFO | train | epoch 048 | loss 5.847 | nll_loss 4.641 | ppl 24.95 | wps 24605.8 | ups 6.24 | wpb 3943.1 | bsz 156.9 | num_updates 49008 | lr 0.000142845 | gnorm 1.007 | train_wall 160 | wall 7806
2024-05-31 18:23:56 | INFO | fairseq.trainer | begin training epoch 49
2024-05-31 18:24:10 | INFO | train_inner | epoch 049:     92 / 1021 loss=5.816, nll_loss=4.605, ppl=24.33, wps=24884.1, ups=6.3, wpb=3947.7, bsz=150.7, num_updates=49100, lr=0.000142712, gnorm=1.019, train_wall=15, wall=7819
2024-05-31 18:24:25 | INFO | train_inner | epoch 049:    192 / 1021 loss=5.814, nll_loss=4.602, ppl=24.29, wps=26423.3, ups=6.75, wpb=3914.8, bsz=156.5, num_updates=49200, lr=0.000142566, gnorm=1.028, train_wall=15, wall=7834
2024-05-31 18:24:42 | INFO | train_inner | epoch 049:    292 / 1021 loss=5.809, nll_loss=4.597, ppl=24.2, wps=23439.9, ups=5.94, wpb=3946.3, bsz=149.2, num_updates=49300, lr=0.000142422, gnorm=0.99, train_wall=17, wall=7851
2024-05-31 18:25:00 | INFO | train_inner | epoch 049:    392 / 1021 loss=5.818, nll_loss=4.608, ppl=24.38, wps=21806.2, ups=5.55, wpb=3931.9, bsz=156.2, num_updates=49400, lr=0.000142278, gnorm=1.008, train_wall=18, wall=7869
2024-05-31 18:25:16 | INFO | train_inner | epoch 049:    492 / 1021 loss=5.843, nll_loss=4.636, ppl=24.86, wps=24287.5, ups=6.17, wpb=3938.8, bsz=161.5, num_updates=49500, lr=0.000142134, gnorm=1.045, train_wall=16, wall=7885
2024-05-31 18:25:32 | INFO | train_inner | epoch 049:    592 / 1021 loss=5.856, nll_loss=4.651, ppl=25.12, wps=25123.3, ups=6.35, wpb=3957.9, bsz=154.8, num_updates=49600, lr=0.00014199, gnorm=1.009, train_wall=16, wall=7901
2024-05-31 18:25:47 | INFO | train_inner | epoch 049:    692 / 1021 loss=5.857, nll_loss=4.651, ppl=25.13, wps=25028.1, ups=6.36, wpb=3934.5, bsz=145.5, num_updates=49700, lr=0.000141848, gnorm=0.997, train_wall=16, wall=7917
2024-05-31 18:26:03 | INFO | train_inner | epoch 049:    792 / 1021 loss=5.858, nll_loss=4.653, ppl=25.17, wps=25096, ups=6.33, wpb=3962.3, bsz=157.4, num_updates=49800, lr=0.000141705, gnorm=1.004, train_wall=16, wall=7933
2024-05-31 18:26:19 | INFO | train_inner | epoch 049:    892 / 1021 loss=5.88, nll_loss=4.678, ppl=25.6, wps=24952.8, ups=6.37, wpb=3917, bsz=151, num_updates=49900, lr=0.000141563, gnorm=1.024, train_wall=16, wall=7948
2024-05-31 18:26:35 | INFO | train_inner | epoch 049:    992 / 1021 loss=5.858, nll_loss=4.655, ppl=25.2, wps=25028.2, ups=6.31, wpb=3966.4, bsz=178.5, num_updates=50000, lr=0.000141421, gnorm=1.087, train_wall=16, wall=7964
2024-05-31 18:26:35 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:26:36 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_49_50000.pt (epoch 49 @ 50000 updates, score None) (writing took 0.9821378299966455 seconds)
2024-05-31 18:26:40 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:26:40 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2024-05-31 18:26:40 | INFO | train | epoch 049 | loss 5.842 | nll_loss 4.634 | ppl 24.84 | wps 24587.9 | ups 6.24 | wpb 3943.1 | bsz 156.9 | num_updates 50029 | lr 0.00014138 | gnorm 1.02 | train_wall 161 | wall 7969
2024-05-31 18:26:40 | INFO | fairseq.trainer | begin training epoch 50
2024-05-31 18:26:51 | INFO | train_inner | epoch 050:     71 / 1021 loss=5.806, nll_loss=4.594, ppl=24.15, wps=24165.7, ups=6.09, wpb=3966.5, bsz=150.3, num_updates=50100, lr=0.00014128, gnorm=1.244, train_wall=15, wall=7981
2024-05-31 18:27:07 | INFO | train_inner | epoch 050:    171 / 1021 loss=5.811, nll_loss=4.598, ppl=24.22, wps=25174.3, ups=6.41, wpb=3929, bsz=147.1, num_updates=50200, lr=0.000141139, gnorm=1.018, train_wall=15, wall=7996
2024-05-31 18:27:22 | INFO | train_inner | epoch 050:    271 / 1021 loss=5.819, nll_loss=4.607, ppl=24.36, wps=25153.1, ups=6.42, wpb=3919.9, bsz=152.6, num_updates=50300, lr=0.000140999, gnorm=1.033, train_wall=15, wall=8012
2024-05-31 18:27:38 | INFO | train_inner | epoch 050:    371 / 1021 loss=5.819, nll_loss=4.607, ppl=24.37, wps=25276.4, ups=6.41, wpb=3940.3, bsz=143.5, num_updates=50400, lr=0.000140859, gnorm=0.999, train_wall=15, wall=8027
2024-05-31 18:27:54 | INFO | train_inner | epoch 050:    471 / 1021 loss=5.832, nll_loss=4.623, ppl=24.65, wps=25260.7, ups=6.38, wpb=3958.8, bsz=163.4, num_updates=50500, lr=0.00014072, gnorm=1.018, train_wall=16, wall=8043
2024-05-31 18:28:09 | INFO | train_inner | epoch 050:    571 / 1021 loss=5.828, nll_loss=4.619, ppl=24.58, wps=25190.5, ups=6.38, wpb=3950, bsz=170.1, num_updates=50600, lr=0.00014058, gnorm=1.066, train_wall=16, wall=8059
2024-05-31 18:28:25 | INFO | train_inner | epoch 050:    671 / 1021 loss=5.829, nll_loss=4.621, ppl=24.61, wps=25244.5, ups=6.39, wpb=3951, bsz=170.4, num_updates=50700, lr=0.000140442, gnorm=0.997, train_wall=15, wall=8074
2024-05-31 18:28:41 | INFO | train_inner | epoch 050:    771 / 1021 loss=5.867, nll_loss=4.664, ppl=25.34, wps=25165.9, ups=6.36, wpb=3954.6, bsz=169.4, num_updates=50800, lr=0.000140303, gnorm=1.048, train_wall=16, wall=8090
2024-05-31 18:28:56 | INFO | train_inner | epoch 050:    871 / 1021 loss=5.847, nll_loss=4.641, ppl=24.95, wps=25160.8, ups=6.36, wpb=3954.8, bsz=158.6, num_updates=50900, lr=0.000140165, gnorm=0.989, train_wall=16, wall=8106
2024-05-31 18:29:12 | INFO | train_inner | epoch 050:    971 / 1021 loss=5.875, nll_loss=4.672, ppl=25.49, wps=25092.3, ups=6.38, wpb=3932.3, bsz=157.1, num_updates=51000, lr=0.000140028, gnorm=1.052, train_wall=16, wall=8121
2024-05-31 18:29:12 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:29:13 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_50_51000.pt (epoch 50 @ 51000 updates, score None) (writing took 0.9782924386672676 seconds)
2024-05-31 18:29:20 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:29:20 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2024-05-31 18:29:20 | INFO | train | epoch 050 | loss 5.835 | nll_loss 4.627 | ppl 24.71 | wps 25121.2 | ups 6.37 | wpb 3943.1 | bsz 156.9 | num_updates 51050 | lr 0.000139959 | gnorm 1.048 | train_wall 158 | wall 8130
2024-05-31 18:29:20 | INFO | fairseq.trainer | begin training epoch 51
2024-05-31 18:29:28 | INFO | train_inner | epoch 051:     50 / 1021 loss=5.85, nll_loss=4.644, ppl=25.01, wps=24890.8, ups=6.34, wpb=3924.7, bsz=154.7, num_updates=51100, lr=0.000139891, gnorm=1.039, train_wall=15, wall=8137
2024-05-31 18:29:43 | INFO | train_inner | epoch 051:    150 / 1021 loss=5.789, nll_loss=4.574, ppl=23.82, wps=26963.2, ups=6.8, wpb=3963, bsz=164.6, num_updates=51200, lr=0.000139754, gnorm=1, train_wall=15, wall=8152
2024-05-31 18:29:58 | INFO | train_inner | epoch 051:    250 / 1021 loss=5.812, nll_loss=4.599, ppl=24.23, wps=25621.2, ups=6.48, wpb=3956, bsz=152.6, num_updates=51300, lr=0.000139618, gnorm=1.016, train_wall=15, wall=8167
2024-05-31 18:30:14 | INFO | train_inner | epoch 051:    350 / 1021 loss=5.812, nll_loss=4.598, ppl=24.22, wps=25016.8, ups=6.37, wpb=3925.6, bsz=145.5, num_updates=51400, lr=0.000139482, gnorm=1.022, train_wall=16, wall=8183
2024-05-31 18:30:29 | INFO | train_inner | epoch 051:    450 / 1021 loss=5.811, nll_loss=4.599, ppl=24.24, wps=25058.6, ups=6.36, wpb=3937, bsz=164.2, num_updates=51500, lr=0.000139347, gnorm=1.03, train_wall=16, wall=8199
2024-05-31 18:30:45 | INFO | train_inner | epoch 051:    550 / 1021 loss=5.837, nll_loss=4.629, ppl=24.75, wps=24865, ups=6.29, wpb=3953.3, bsz=174.4, num_updates=51600, lr=0.000139212, gnorm=1.085, train_wall=16, wall=8215
2024-05-31 18:31:01 | INFO | train_inner | epoch 051:    650 / 1021 loss=5.836, nll_loss=4.626, ppl=24.7, wps=24777.3, ups=6.27, wpb=3952.9, bsz=151.7, num_updates=51700, lr=0.000139077, gnorm=1.002, train_wall=16, wall=8231
2024-05-31 18:31:17 | INFO | train_inner | epoch 051:    750 / 1021 loss=5.837, nll_loss=4.629, ppl=24.74, wps=24684.7, ups=6.29, wpb=3923.5, bsz=155.4, num_updates=51800, lr=0.000138943, gnorm=1.018, train_wall=16, wall=8247
2024-05-31 18:31:33 | INFO | train_inner | epoch 051:    850 / 1021 loss=5.85, nll_loss=4.642, ppl=24.98, wps=24762.6, ups=6.29, wpb=3936.9, bsz=154.3, num_updates=51900, lr=0.000138809, gnorm=1.033, train_wall=16, wall=8262
2024-05-31 18:31:49 | INFO | train_inner | epoch 051:    950 / 1021 loss=5.849, nll_loss=4.642, ppl=24.96, wps=24785.6, ups=6.28, wpb=3945.2, bsz=146.1, num_updates=52000, lr=0.000138675, gnorm=1.004, train_wall=16, wall=8278
2024-05-31 18:31:49 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:31:50 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_51_52000.pt (epoch 51 @ 52000 updates, score None) (writing took 0.975519907195121 seconds)
2024-05-31 18:32:01 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:32:01 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2024-05-31 18:32:01 | INFO | train | epoch 051 | loss 5.828 | nll_loss 4.618 | ppl 24.55 | wps 25135.2 | ups 6.37 | wpb 3943.1 | bsz 156.9 | num_updates 52071 | lr 0.00013858 | gnorm 1.025 | train_wall 158 | wall 8290
2024-05-31 18:32:01 | INFO | fairseq.trainer | begin training epoch 52
2024-05-31 18:32:05 | INFO | train_inner | epoch 052:     29 / 1021 loss=5.837, nll_loss=4.628, ppl=24.73, wps=24541.4, ups=6.23, wpb=3936.1, bsz=154, num_updates=52100, lr=0.000138542, gnorm=1.024, train_wall=15, wall=8294
2024-05-31 18:32:20 | INFO | train_inner | epoch 052:    129 / 1021 loss=5.785, nll_loss=4.568, ppl=23.72, wps=26434.3, ups=6.7, wpb=3948.1, bsz=150.2, num_updates=52200, lr=0.000138409, gnorm=1.016, train_wall=15, wall=8309
2024-05-31 18:32:35 | INFO | train_inner | epoch 052:    229 / 1021 loss=5.78, nll_loss=4.562, ppl=23.63, wps=26430.8, ups=6.68, wpb=3954.6, bsz=148.4, num_updates=52300, lr=0.000138277, gnorm=1.011, train_wall=15, wall=8324
2024-05-31 18:32:50 | INFO | train_inner | epoch 052:    329 / 1021 loss=5.806, nll_loss=4.593, ppl=24.13, wps=26310, ups=6.68, wpb=3936.7, bsz=167.4, num_updates=52400, lr=0.000138145, gnorm=1.056, train_wall=15, wall=8339
2024-05-31 18:33:05 | INFO | train_inner | epoch 052:    429 / 1021 loss=5.814, nll_loss=4.599, ppl=24.24, wps=26449.8, ups=6.73, wpb=3928, bsz=136.6, num_updates=52500, lr=0.000138013, gnorm=1.03, train_wall=15, wall=8354
2024-05-31 18:33:20 | INFO | train_inner | epoch 052:    529 / 1021 loss=5.839, nll_loss=4.629, ppl=24.75, wps=26313.1, ups=6.71, wpb=3921.2, bsz=155.9, num_updates=52600, lr=0.000137882, gnorm=1.072, train_wall=15, wall=8369
2024-05-31 18:33:37 | INFO | train_inner | epoch 052:    629 / 1021 loss=5.843, nll_loss=4.634, ppl=24.82, wps=23376.6, ups=5.9, wpb=3962.6, bsz=146, num_updates=52700, lr=0.000137751, gnorm=1.004, train_wall=17, wall=8386
2024-05-31 18:33:54 | INFO | train_inner | epoch 052:    729 / 1021 loss=5.846, nll_loss=4.639, ppl=24.92, wps=23027.1, ups=5.85, wpb=3934.5, bsz=170.2, num_updates=52800, lr=0.00013762, gnorm=1.054, train_wall=17, wall=8403
2024-05-31 18:34:10 | INFO | train_inner | epoch 052:    829 / 1021 loss=5.846, nll_loss=4.64, ppl=24.93, wps=24094, ups=6.1, wpb=3946.7, bsz=175.8, num_updates=52900, lr=0.00013749, gnorm=1.072, train_wall=16, wall=8419
2024-05-31 18:34:26 | INFO | train_inner | epoch 052:    929 / 1021 loss=5.861, nll_loss=4.655, ppl=25.2, wps=25028.8, ups=6.35, wpb=3939.4, bsz=146.4, num_updates=53000, lr=0.000137361, gnorm=1.015, train_wall=16, wall=8435
2024-05-31 18:34:26 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:34:27 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_52_53000.pt (epoch 52 @ 53000 updates, score None) (writing took 1.0223387340083718 seconds)
2024-05-31 18:34:40 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:34:40 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2024-05-31 18:34:40 | INFO | train | epoch 052 | loss 5.823 | nll_loss 4.613 | ppl 24.46 | wps 25166.5 | ups 6.38 | wpb 3943.1 | bsz 156.9 | num_updates 53092 | lr 0.000137242 | gnorm 1.033 | train_wall 157 | wall 8450
2024-05-31 18:34:41 | INFO | fairseq.trainer | begin training epoch 53
2024-05-31 18:34:42 | INFO | train_inner | epoch 053:      8 / 1021 loss=5.832, nll_loss=4.624, ppl=24.66, wps=24748.3, ups=6.26, wpb=3954.9, bsz=168.2, num_updates=53100, lr=0.000137231, gnorm=1.007, train_wall=15, wall=8451
2024-05-31 18:34:57 | INFO | train_inner | epoch 053:    108 / 1021 loss=5.783, nll_loss=4.566, ppl=23.69, wps=25845.8, ups=6.56, wpb=3940.4, bsz=160, num_updates=53200, lr=0.000137102, gnorm=1.086, train_wall=15, wall=8466
2024-05-31 18:35:13 | INFO | train_inner | epoch 053:    208 / 1021 loss=5.757, nll_loss=4.535, ppl=23.18, wps=24775.3, ups=6.33, wpb=3912.3, bsz=143.7, num_updates=53300, lr=0.000136973, gnorm=1.007, train_wall=16, wall=8482
2024-05-31 18:35:29 | INFO | train_inner | epoch 053:    308 / 1021 loss=5.808, nll_loss=4.592, ppl=24.11, wps=24703.2, ups=6.29, wpb=3930.3, bsz=131.9, num_updates=53400, lr=0.000136845, gnorm=1.03, train_wall=16, wall=8498
2024-05-31 18:35:45 | INFO | train_inner | epoch 053:    408 / 1021 loss=5.812, nll_loss=4.6, ppl=24.25, wps=24814.5, ups=6.26, wpb=3963.6, bsz=163.7, num_updates=53500, lr=0.000136717, gnorm=1.019, train_wall=16, wall=8514
2024-05-31 18:36:01 | INFO | train_inner | epoch 053:    508 / 1021 loss=5.816, nll_loss=4.603, ppl=24.3, wps=24696.8, ups=6.3, wpb=3919.1, bsz=157.4, num_updates=53600, lr=0.00013659, gnorm=1.078, train_wall=16, wall=8530
2024-05-31 18:36:16 | INFO | train_inner | epoch 053:    608 / 1021 loss=5.818, nll_loss=4.607, ppl=24.37, wps=24752.7, ups=6.28, wpb=3941.5, bsz=173.2, num_updates=53700, lr=0.000136462, gnorm=1.042, train_wall=16, wall=8546
2024-05-31 18:36:34 | INFO | train_inner | epoch 053:    708 / 1021 loss=5.852, nll_loss=4.644, ppl=25, wps=22901.5, ups=5.77, wpb=3970.6, bsz=153.6, num_updates=53800, lr=0.000136335, gnorm=1.027, train_wall=17, wall=8563
2024-05-31 18:36:50 | INFO | train_inner | epoch 053:    808 / 1021 loss=5.847, nll_loss=4.638, ppl=24.9, wps=24648.6, ups=6.25, wpb=3944, bsz=144.1, num_updates=53900, lr=0.000136209, gnorm=1.014, train_wall=16, wall=8579
2024-05-31 18:37:06 | INFO | train_inner | epoch 053:    908 / 1021 loss=5.833, nll_loss=4.625, ppl=24.67, wps=25033, ups=6.32, wpb=3961.9, bsz=167, num_updates=54000, lr=0.000136083, gnorm=1.03, train_wall=16, wall=8595
2024-05-31 18:37:06 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:37:07 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_53_54000.pt (epoch 53 @ 54000 updates, score None) (writing took 0.975827360060066 seconds)
2024-05-31 18:37:21 | INFO | train_inner | epoch 053:   1008 / 1021 loss=5.84, nll_loss=4.633, ppl=24.82, wps=25186.5, ups=6.38, wpb=3944.7, bsz=171.3, num_updates=54100, lr=0.000135957, gnorm=1.027, train_wall=15, wall=8611
2024-05-31 18:37:23 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:37:23 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2024-05-31 18:37:23 | INFO | train | epoch 053 | loss 5.817 | nll_loss 4.605 | ppl 24.34 | wps 24731.4 | ups 6.27 | wpb 3943.1 | bsz 156.9 | num_updates 54113 | lr 0.000135941 | gnorm 1.037 | train_wall 160 | wall 8613
2024-05-31 18:37:23 | INFO | fairseq.trainer | begin training epoch 54
2024-05-31 18:37:36 | INFO | train_inner | epoch 054:     87 / 1021 loss=5.769, nll_loss=4.549, ppl=23.42, wps=26517.4, ups=6.69, wpb=3964.8, bsz=150.3, num_updates=54200, lr=0.000135831, gnorm=1.015, train_wall=15, wall=8626
2024-05-31 18:37:51 | INFO | train_inner | epoch 054:    187 / 1021 loss=5.773, nll_loss=4.555, ppl=23.51, wps=26719.8, ups=6.75, wpb=3958.5, bsz=175.2, num_updates=54300, lr=0.000135706, gnorm=1.03, train_wall=15, wall=8640
2024-05-31 18:38:06 | INFO | train_inner | epoch 054:    287 / 1021 loss=5.788, nll_loss=4.571, ppl=23.77, wps=26775.4, ups=6.78, wpb=3946.7, bsz=150.6, num_updates=54400, lr=0.000135582, gnorm=1.039, train_wall=15, wall=8655
2024-05-31 18:38:21 | INFO | train_inner | epoch 054:    387 / 1021 loss=5.81, nll_loss=4.595, ppl=24.17, wps=26698.2, ups=6.79, wpb=3933.2, bsz=153, num_updates=54500, lr=0.000135457, gnorm=1.051, train_wall=15, wall=8670
2024-05-31 18:38:36 | INFO | train_inner | epoch 054:    487 / 1021 loss=5.816, nll_loss=4.603, ppl=24.29, wps=26280.9, ups=6.64, wpb=3956.8, bsz=158, num_updates=54600, lr=0.000135333, gnorm=1.079, train_wall=15, wall=8685
2024-05-31 18:38:51 | INFO | train_inner | epoch 054:    587 / 1021 loss=5.827, nll_loss=4.617, ppl=24.53, wps=25075.8, ups=6.34, wpb=3954.3, bsz=167, num_updates=54700, lr=0.000135209, gnorm=1.038, train_wall=16, wall=8701
2024-05-31 18:39:07 | INFO | train_inner | epoch 054:    687 / 1021 loss=5.844, nll_loss=4.634, ppl=24.83, wps=25007.3, ups=6.39, wpb=3913.7, bsz=150.6, num_updates=54800, lr=0.000135086, gnorm=1.055, train_wall=15, wall=8716
2024-05-31 18:39:23 | INFO | train_inner | epoch 054:    787 / 1021 loss=5.814, nll_loss=4.602, ppl=24.28, wps=25074.6, ups=6.36, wpb=3940.6, bsz=155.4, num_updates=54900, lr=0.000134963, gnorm=1.024, train_wall=16, wall=8732
2024-05-31 18:39:39 | INFO | train_inner | epoch 054:    887 / 1021 loss=5.839, nll_loss=4.631, ppl=24.78, wps=25043.4, ups=6.35, wpb=3944.3, bsz=165.7, num_updates=55000, lr=0.00013484, gnorm=1.043, train_wall=16, wall=8748
2024-05-31 18:39:39 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:39:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_54_55000.pt (epoch 54 @ 55000 updates, score None) (writing took 0.9691633302718401 seconds)
2024-05-31 18:39:54 | INFO | train_inner | epoch 054:    987 / 1021 loss=5.841, nll_loss=4.634, ppl=24.83, wps=25247.2, ups=6.38, wpb=3957.7, bsz=165.7, num_updates=55100, lr=0.000134718, gnorm=1.019, train_wall=15, wall=8764
2024-05-31 18:39:59 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:39:59 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2024-05-31 18:39:59 | INFO | train | epoch 054 | loss 5.812 | nll_loss 4.599 | ppl 24.23 | wps 25840.2 | ups 6.55 | wpb 3943.1 | bsz 156.9 | num_updates 55134 | lr 0.000134676 | gnorm 1.039 | train_wall 153 | wall 8768
2024-05-31 18:39:59 | INFO | fairseq.trainer | begin training epoch 55
2024-05-31 18:40:10 | INFO | train_inner | epoch 055:     66 / 1021 loss=5.788, nll_loss=4.57, ppl=23.75, wps=25348.2, ups=6.48, wpb=3912.5, bsz=134.6, num_updates=55200, lr=0.000134595, gnorm=1.035, train_wall=15, wall=8779
2024-05-31 18:40:25 | INFO | train_inner | epoch 055:    166 / 1021 loss=5.766, nll_loss=4.545, ppl=23.34, wps=24991.4, ups=6.37, wpb=3924.1, bsz=153.4, num_updates=55300, lr=0.000134474, gnorm=1.046, train_wall=16, wall=8795
2024-05-31 18:40:41 | INFO | train_inner | epoch 055:    266 / 1021 loss=5.783, nll_loss=4.566, ppl=23.68, wps=25054, ups=6.34, wpb=3950.9, bsz=160.9, num_updates=55400, lr=0.000134352, gnorm=1.037, train_wall=16, wall=8810
2024-05-31 18:40:57 | INFO | train_inner | epoch 055:    366 / 1021 loss=5.797, nll_loss=4.579, ppl=23.9, wps=25008.4, ups=6.35, wpb=3939.3, bsz=139.4, num_updates=55500, lr=0.000134231, gnorm=1.046, train_wall=16, wall=8826
2024-05-31 18:41:13 | INFO | train_inner | epoch 055:    466 / 1021 loss=5.805, nll_loss=4.589, ppl=24.06, wps=25050, ups=6.34, wpb=3951.8, bsz=153.8, num_updates=55600, lr=0.00013411, gnorm=1.084, train_wall=16, wall=8842
2024-05-31 18:41:28 | INFO | train_inner | epoch 055:    566 / 1021 loss=5.797, nll_loss=4.582, ppl=23.95, wps=25171.6, ups=6.33, wpb=3974.8, bsz=160, num_updates=55700, lr=0.00013399, gnorm=1.013, train_wall=16, wall=8858
2024-05-31 18:41:44 | INFO | train_inner | epoch 055:    666 / 1021 loss=5.838, nll_loss=4.628, ppl=24.73, wps=25022.8, ups=6.35, wpb=3939.4, bsz=163.8, num_updates=55800, lr=0.00013387, gnorm=1.079, train_wall=16, wall=8874
2024-05-31 18:42:00 | INFO | train_inner | epoch 055:    766 / 1021 loss=5.807, nll_loss=4.594, ppl=24.14, wps=25019.4, ups=6.4, wpb=3910.1, bsz=159.3, num_updates=55900, lr=0.00013375, gnorm=1.04, train_wall=15, wall=8889
2024-05-31 18:42:15 | INFO | train_inner | epoch 055:    866 / 1021 loss=5.836, nll_loss=4.626, ppl=24.7, wps=25010, ups=6.37, wpb=3928, bsz=168.9, num_updates=56000, lr=0.000133631, gnorm=1.091, train_wall=16, wall=8905
2024-05-31 18:42:15 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:42:16 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_55_56000.pt (epoch 55 @ 56000 updates, score None) (writing took 0.9863710920326412 seconds)
2024-05-31 18:42:31 | INFO | train_inner | epoch 055:    966 / 1021 loss=5.841, nll_loss=4.632, ppl=24.79, wps=25076.5, ups=6.33, wpb=3958.7, bsz=162.1, num_updates=56100, lr=0.000133511, gnorm=1.029, train_wall=15, wall=8921
2024-05-31 18:42:39 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:42:39 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2024-05-31 18:42:39 | INFO | train | epoch 055 | loss 5.806 | nll_loss 4.592 | ppl 24.11 | wps 25112.6 | ups 6.37 | wpb 3943.1 | bsz 156.9 | num_updates 56155 | lr 0.000133446 | gnorm 1.048 | train_wall 158 | wall 8929
2024-05-31 18:42:39 | INFO | fairseq.trainer | begin training epoch 56
2024-05-31 18:42:46 | INFO | train_inner | epoch 056:     45 / 1021 loss=5.801, nll_loss=4.587, ppl=24.03, wps=26541.6, ups=6.73, wpb=3946.4, bsz=157, num_updates=56200, lr=0.000133393, gnorm=1.033, train_wall=15, wall=8936
2024-05-31 18:43:01 | INFO | train_inner | epoch 056:    145 / 1021 loss=5.76, nll_loss=4.539, ppl=23.24, wps=26132.7, ups=6.65, wpb=3927.2, bsz=155.7, num_updates=56300, lr=0.000133274, gnorm=1.042, train_wall=15, wall=8951
2024-05-31 18:43:17 | INFO | train_inner | epoch 056:    245 / 1021 loss=5.767, nll_loss=4.547, ppl=23.38, wps=25234.6, ups=6.35, wpb=3971.1, bsz=159.7, num_updates=56400, lr=0.000133156, gnorm=1.04, train_wall=16, wall=8966
2024-05-31 18:43:33 | INFO | train_inner | epoch 056:    345 / 1021 loss=5.78, nll_loss=4.561, ppl=23.61, wps=25281.5, ups=6.39, wpb=3958.5, bsz=152.9, num_updates=56500, lr=0.000133038, gnorm=1.027, train_wall=16, wall=8982
2024-05-31 18:43:48 | INFO | train_inner | epoch 056:    445 / 1021 loss=5.799, nll_loss=4.583, ppl=23.96, wps=25163.7, ups=6.36, wpb=3956.6, bsz=167.2, num_updates=56600, lr=0.00013292, gnorm=1.079, train_wall=16, wall=8998
2024-05-31 18:44:04 | INFO | train_inner | epoch 056:    545 / 1021 loss=5.809, nll_loss=4.594, ppl=24.14, wps=25195.3, ups=6.39, wpb=3942.2, bsz=156.2, num_updates=56700, lr=0.000132803, gnorm=1.055, train_wall=15, wall=9013
2024-05-31 18:44:19 | INFO | train_inner | epoch 056:    645 / 1021 loss=5.808, nll_loss=4.592, ppl=24.12, wps=25166, ups=6.43, wpb=3911.5, bsz=138.3, num_updates=56800, lr=0.000132686, gnorm=1.03, train_wall=15, wall=9029
2024-05-31 18:44:35 | INFO | train_inner | epoch 056:    745 / 1021 loss=5.825, nll_loss=4.613, ppl=24.47, wps=25201.8, ups=6.39, wpb=3946.8, bsz=162.1, num_updates=56900, lr=0.00013257, gnorm=1.069, train_wall=16, wall=9045
2024-05-31 18:44:51 | INFO | train_inner | epoch 056:    845 / 1021 loss=5.834, nll_loss=4.623, ppl=24.63, wps=25089.9, ups=6.36, wpb=3944.8, bsz=157.9, num_updates=57000, lr=0.000132453, gnorm=1.098, train_wall=16, wall=9060
2024-05-31 18:44:51 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:44:52 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_56_57000.pt (epoch 56 @ 57000 updates, score None) (writing took 0.9726548450998962 seconds)
2024-05-31 18:45:07 | INFO | train_inner | epoch 056:    945 / 1021 loss=5.826, nll_loss=4.613, ppl=24.48, wps=24187.5, ups=6.17, wpb=3920.6, bsz=152, num_updates=57100, lr=0.000132337, gnorm=1.037, train_wall=15, wall=9076
2024-05-31 18:45:19 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:45:19 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2024-05-31 18:45:19 | INFO | train | epoch 056 | loss 5.801 | nll_loss 4.586 | ppl 24.01 | wps 25214 | ups 6.39 | wpb 3943.1 | bsz 156.9 | num_updates 57176 | lr 0.000132249 | gnorm 1.054 | train_wall 157 | wall 9088
2024-05-31 18:45:19 | INFO | fairseq.trainer | begin training epoch 57
2024-05-31 18:45:23 | INFO | train_inner | epoch 057:     24 / 1021 loss=5.822, nll_loss=4.611, ppl=24.43, wps=24897.4, ups=6.27, wpb=3970, bsz=165.4, num_updates=57200, lr=0.000132221, gnorm=1.062, train_wall=16, wall=9092
2024-05-31 18:45:39 | INFO | train_inner | epoch 057:    124 / 1021 loss=5.741, nll_loss=4.516, ppl=22.88, wps=25169.6, ups=6.39, wpb=3941.2, bsz=153.3, num_updates=57300, lr=0.000132106, gnorm=1.024, train_wall=16, wall=9108
2024-05-31 18:45:54 | INFO | train_inner | epoch 057:    224 / 1021 loss=5.77, nll_loss=4.549, ppl=23.4, wps=25102.6, ups=6.41, wpb=3915.7, bsz=153.9, num_updates=57400, lr=0.000131991, gnorm=1.073, train_wall=15, wall=9124
2024-05-31 18:46:10 | INFO | train_inner | epoch 057:    324 / 1021 loss=5.767, nll_loss=4.546, ppl=23.36, wps=25235.6, ups=6.36, wpb=3965.9, bsz=163, num_updates=57500, lr=0.000131876, gnorm=1.045, train_wall=16, wall=9139
2024-05-31 18:46:26 | INFO | train_inner | epoch 057:    424 / 1021 loss=5.776, nll_loss=4.557, ppl=23.53, wps=25016.7, ups=6.38, wpb=3921.5, bsz=171.3, num_updates=57600, lr=0.000131762, gnorm=1.128, train_wall=16, wall=9155
2024-05-31 18:46:41 | INFO | train_inner | epoch 057:    524 / 1021 loss=5.788, nll_loss=4.571, ppl=23.76, wps=25182, ups=6.38, wpb=3945, bsz=155, num_updates=57700, lr=0.000131647, gnorm=1.039, train_wall=16, wall=9171
2024-05-31 18:46:57 | INFO | train_inner | epoch 057:    624 / 1021 loss=5.812, nll_loss=4.598, ppl=24.22, wps=25149.4, ups=6.34, wpb=3968.4, bsz=162, num_updates=57800, lr=0.000131533, gnorm=1.044, train_wall=16, wall=9187
2024-05-31 18:47:13 | INFO | train_inner | epoch 057:    724 / 1021 loss=5.824, nll_loss=4.611, ppl=24.44, wps=25163.7, ups=6.36, wpb=3954.8, bsz=154.1, num_updates=57900, lr=0.00013142, gnorm=1.047, train_wall=16, wall=9202
2024-05-31 18:47:29 | INFO | train_inner | epoch 057:    824 / 1021 loss=5.829, nll_loss=4.616, ppl=24.52, wps=25095.8, ups=6.38, wpb=3933.7, bsz=150.6, num_updates=58000, lr=0.000131306, gnorm=1.058, train_wall=16, wall=9218
2024-05-31 18:47:29 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:47:29 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_57_58000.pt (epoch 57 @ 58000 updates, score None) (writing took 0.9760037614032626 seconds)
2024-05-31 18:47:44 | INFO | train_inner | epoch 057:    924 / 1021 loss=5.823, nll_loss=4.609, ppl=24.41, wps=24933.3, ups=6.32, wpb=3947.5, bsz=148.2, num_updates=58100, lr=0.000131193, gnorm=1.036, train_wall=15, wall=9234
2024-05-31 18:48:00 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:48:00 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2024-05-31 18:48:00 | INFO | train | epoch 057 | loss 5.796 | nll_loss 4.579 | ppl 23.9 | wps 25042.6 | ups 6.35 | wpb 3943.1 | bsz 156.9 | num_updates 58197 | lr 0.000131084 | gnorm 1.057 | train_wall 158 | wall 9249
2024-05-31 18:48:00 | INFO | fairseq.trainer | begin training epoch 58
2024-05-31 18:48:00 | INFO | train_inner | epoch 058:      3 / 1021 loss=5.834, nll_loss=4.623, ppl=24.65, wps=24316.9, ups=6.2, wpb=3924.4, bsz=161, num_updates=58200, lr=0.000131081, gnorm=1.085, train_wall=16, wall=9250
2024-05-31 18:48:16 | INFO | train_inner | epoch 058:    103 / 1021 loss=5.745, nll_loss=4.521, ppl=22.96, wps=24700.6, ups=6.3, wpb=3918.9, bsz=157.7, num_updates=58300, lr=0.000130968, gnorm=1.062, train_wall=16, wall=9266
2024-05-31 18:48:32 | INFO | train_inner | epoch 058:    203 / 1021 loss=5.77, nll_loss=4.549, ppl=23.4, wps=24775.1, ups=6.25, wpb=3962.5, bsz=160.3, num_updates=58400, lr=0.000130856, gnorm=1.056, train_wall=16, wall=9282
2024-05-31 18:48:50 | INFO | train_inner | epoch 058:    303 / 1021 loss=5.756, nll_loss=4.534, ppl=23.17, wps=22563.8, ups=5.73, wpb=3938.5, bsz=165, num_updates=58500, lr=0.000130744, gnorm=1.051, train_wall=17, wall=9299
2024-05-31 18:49:07 | INFO | train_inner | epoch 058:    403 / 1021 loss=5.792, nll_loss=4.574, ppl=23.82, wps=22803.3, ups=5.77, wpb=3953.8, bsz=162.8, num_updates=58600, lr=0.000130632, gnorm=1.128, train_wall=17, wall=9317
2024-05-31 18:49:23 | INFO | train_inner | epoch 058:    503 / 1021 loss=5.787, nll_loss=4.566, ppl=23.69, wps=24983.6, ups=6.35, wpb=3936.3, bsz=141.8, num_updates=58700, lr=0.000130521, gnorm=1.047, train_wall=16, wall=9332
2024-05-31 18:49:39 | INFO | train_inner | epoch 058:    603 / 1021 loss=5.797, nll_loss=4.579, ppl=23.91, wps=24948.3, ups=6.35, wpb=3926.4, bsz=148.8, num_updates=58800, lr=0.00013041, gnorm=1.053, train_wall=16, wall=9348
2024-05-31 18:49:54 | INFO | train_inner | epoch 058:    703 / 1021 loss=5.806, nll_loss=4.59, ppl=24.09, wps=24907.4, ups=6.31, wpb=3948.2, bsz=164.6, num_updates=58900, lr=0.000130299, gnorm=1.062, train_wall=16, wall=9364
2024-05-31 18:50:10 | INFO | train_inner | epoch 058:    803 / 1021 loss=5.816, nll_loss=4.601, ppl=24.27, wps=24786.8, ups=6.28, wpb=3946.7, bsz=151.9, num_updates=59000, lr=0.000130189, gnorm=1.053, train_wall=16, wall=9380
2024-05-31 18:50:10 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:50:11 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_58_59000.pt (epoch 58 @ 59000 updates, score None) (writing took 0.9724142597988248 seconds)
2024-05-31 18:50:28 | INFO | train_inner | epoch 058:    903 / 1021 loss=5.81, nll_loss=4.596, ppl=24.18, wps=22718.3, ups=5.74, wpb=3954.5, bsz=155.6, num_updates=59100, lr=0.000130079, gnorm=1.032, train_wall=16, wall=9397
2024-05-31 18:50:44 | INFO | train_inner | epoch 058:   1003 / 1021 loss=5.823, nll_loss=4.61, ppl=24.43, wps=24499.3, ups=6.21, wpb=3946, bsz=158.8, num_updates=59200, lr=0.000129969, gnorm=1.068, train_wall=16, wall=9413
2024-05-31 18:50:47 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:50:47 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2024-05-31 18:50:47 | INFO | train | epoch 058 | loss 5.79 | nll_loss 4.572 | ppl 23.79 | wps 24114 | ups 6.12 | wpb 3943.1 | bsz 156.9 | num_updates 59218 | lr 0.000129949 | gnorm 1.06 | train_wall 164 | wall 9416
2024-05-31 18:50:47 | INFO | fairseq.trainer | begin training epoch 59
2024-05-31 18:51:00 | INFO | train_inner | epoch 059:     82 / 1021 loss=5.75, nll_loss=4.527, ppl=23.05, wps=24822.8, ups=6.28, wpb=3953.6, bsz=143, num_updates=59300, lr=0.000129859, gnorm=1.029, train_wall=16, wall=9429
2024-05-31 18:51:16 | INFO | train_inner | epoch 059:    182 / 1021 loss=5.752, nll_loss=4.527, ppl=23.05, wps=25035.4, ups=6.32, wpb=3960.2, bsz=142.4, num_updates=59400, lr=0.00012975, gnorm=1.047, train_wall=16, wall=9445
2024-05-31 18:51:31 | INFO | train_inner | epoch 059:    282 / 1021 loss=5.75, nll_loss=4.525, ppl=23.02, wps=25002.4, ups=6.44, wpb=3882.7, bsz=145.7, num_updates=59500, lr=0.000129641, gnorm=1.076, train_wall=15, wall=9461
2024-05-31 18:51:47 | INFO | train_inner | epoch 059:    382 / 1021 loss=5.766, nll_loss=4.545, ppl=23.34, wps=25115.1, ups=6.38, wpb=3936.5, bsz=167, num_updates=59600, lr=0.000129532, gnorm=1.123, train_wall=16, wall=9476
2024-05-31 18:52:03 | INFO | train_inner | epoch 059:    482 / 1021 loss=5.787, nll_loss=4.567, ppl=23.7, wps=25036.7, ups=6.4, wpb=3913.5, bsz=145.4, num_updates=59700, lr=0.000129423, gnorm=1.085, train_wall=15, wall=9492
2024-05-31 18:52:18 | INFO | train_inner | epoch 059:    582 / 1021 loss=5.789, nll_loss=4.571, ppl=23.77, wps=25270.1, ups=6.34, wpb=3987.7, bsz=162.2, num_updates=59800, lr=0.000129315, gnorm=1.038, train_wall=16, wall=9508
2024-05-31 18:52:34 | INFO | train_inner | epoch 059:    682 / 1021 loss=5.811, nll_loss=4.597, ppl=24.2, wps=25149.6, ups=6.37, wpb=3945.1, bsz=166.5, num_updates=59900, lr=0.000129207, gnorm=1.062, train_wall=16, wall=9523
2024-05-31 18:52:50 | INFO | train_inner | epoch 059:    782 / 1021 loss=5.806, nll_loss=4.59, ppl=24.09, wps=25174.1, ups=6.38, wpb=3948.1, bsz=161.4, num_updates=60000, lr=0.000129099, gnorm=1.075, train_wall=16, wall=9539
2024-05-31 18:52:50 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:52:51 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_59_60000.pt (epoch 59 @ 60000 updates, score None) (writing took 1.084993330296129 seconds)
2024-05-31 18:53:05 | INFO | train_inner | epoch 059:    882 / 1021 loss=5.805, nll_loss=4.59, ppl=24.08, wps=25072, ups=6.37, wpb=3936, bsz=158.6, num_updates=60100, lr=0.000128992, gnorm=1.05, train_wall=14, wall=9555
2024-05-31 18:53:20 | INFO | train_inner | epoch 059:    982 / 1021 loss=5.83, nll_loss=4.619, ppl=24.58, wps=26955.3, ups=6.81, wpb=3957.5, bsz=171, num_updates=60200, lr=0.000128885, gnorm=1.08, train_wall=15, wall=9569
2024-05-31 18:53:26 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:53:26 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2024-05-31 18:53:26 | INFO | train | epoch 059 | loss 5.785 | nll_loss 4.566 | ppl 23.69 | wps 25314.5 | ups 6.42 | wpb 3943.1 | bsz 156.9 | num_updates 60239 | lr 0.000128843 | gnorm 1.065 | train_wall 156 | wall 9575
2024-05-31 18:53:26 | INFO | fairseq.trainer | begin training epoch 60
2024-05-31 18:53:36 | INFO | train_inner | epoch 060:     61 / 1021 loss=5.764, nll_loss=4.543, ppl=23.31, wps=25066.8, ups=6.4, wpb=3918, bsz=163, num_updates=60300, lr=0.000128778, gnorm=1.066, train_wall=15, wall=9585
2024-05-31 18:53:53 | INFO | train_inner | epoch 060:    161 / 1021 loss=5.745, nll_loss=4.521, ppl=22.95, wps=22304.5, ups=5.63, wpb=3962.6, bsz=175.2, num_updates=60400, lr=0.000128671, gnorm=1.073, train_wall=18, wall=9603
2024-05-31 18:54:09 | INFO | train_inner | epoch 060:    261 / 1021 loss=5.758, nll_loss=4.533, ppl=23.16, wps=24897.8, ups=6.33, wpb=3935.6, bsz=142, num_updates=60500, lr=0.000128565, gnorm=1.06, train_wall=16, wall=9619
2024-05-31 18:54:25 | INFO | train_inner | epoch 060:    361 / 1021 loss=5.772, nll_loss=4.55, ppl=23.43, wps=25053.6, ups=6.32, wpb=3963.4, bsz=158.3, num_updates=60600, lr=0.000128459, gnorm=1.05, train_wall=16, wall=9634
2024-05-31 18:54:41 | INFO | train_inner | epoch 060:    461 / 1021 loss=5.781, nll_loss=4.561, ppl=23.6, wps=25079.6, ups=6.32, wpb=3967.2, bsz=173.3, num_updates=60700, lr=0.000128353, gnorm=1.138, train_wall=16, wall=9650
2024-05-31 18:54:57 | INFO | train_inner | epoch 060:    561 / 1021 loss=5.783, nll_loss=4.563, ppl=23.64, wps=25123.4, ups=6.36, wpb=3952.9, bsz=149.7, num_updates=60800, lr=0.000128247, gnorm=1.039, train_wall=16, wall=9666
2024-05-31 18:55:13 | INFO | train_inner | epoch 060:    661 / 1021 loss=5.786, nll_loss=4.568, ppl=23.72, wps=23636.7, ups=6.01, wpb=3931.1, bsz=151.6, num_updates=60900, lr=0.000128142, gnorm=1.055, train_wall=16, wall=9683
2024-05-31 18:55:31 | INFO | train_inner | epoch 060:    761 / 1021 loss=5.796, nll_loss=4.578, ppl=23.89, wps=21878.7, ups=5.57, wpb=3927, bsz=152.2, num_updates=61000, lr=0.000128037, gnorm=1.065, train_wall=18, wall=9701
2024-05-31 18:55:31 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:55:32 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_60_61000.pt (epoch 60 @ 61000 updates, score None) (writing took 0.97642478113994 seconds)
2024-05-31 18:55:49 | INFO | train_inner | epoch 060:    861 / 1021 loss=5.796, nll_loss=4.578, ppl=23.89, wps=22484.5, ups=5.71, wpb=3937.1, bsz=148.6, num_updates=61100, lr=0.000127932, gnorm=1.056, train_wall=16, wall=9718
2024-05-31 18:56:05 | INFO | train_inner | epoch 060:    961 / 1021 loss=5.813, nll_loss=4.599, ppl=24.23, wps=23838.6, ups=6.03, wpb=3955.4, bsz=166.1, num_updates=61200, lr=0.000127827, gnorm=1.068, train_wall=16, wall=9735
2024-05-31 18:56:15 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:56:15 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2024-05-31 18:56:15 | INFO | train | epoch 060 | loss 5.78 | nll_loss 4.56 | ppl 23.59 | wps 23771.6 | ups 6.03 | wpb 3943.1 | bsz 156.9 | num_updates 61260 | lr 0.000127765 | gnorm 1.069 | train_wall 167 | wall 9745
2024-05-31 18:56:15 | INFO | fairseq.trainer | begin training epoch 61
2024-05-31 18:56:22 | INFO | train_inner | epoch 061:     40 / 1021 loss=5.769, nll_loss=4.548, ppl=23.4, wps=23572.7, ups=5.99, wpb=3935.8, bsz=153.4, num_updates=61300, lr=0.000127723, gnorm=1.055, train_wall=16, wall=9751
2024-05-31 18:56:38 | INFO | train_inner | epoch 061:    140 / 1021 loss=5.734, nll_loss=4.507, ppl=22.74, wps=23817.9, ups=6.09, wpb=3913.9, bsz=158.5, num_updates=61400, lr=0.000127619, gnorm=1.13, train_wall=16, wall=9768
2024-05-31 18:56:55 | INFO | train_inner | epoch 061:    240 / 1021 loss=5.729, nll_loss=4.502, ppl=22.65, wps=23750.8, ups=6.01, wpb=3951.7, bsz=151.4, num_updates=61500, lr=0.000127515, gnorm=1.048, train_wall=17, wall=9784
2024-05-31 18:57:11 | INFO | train_inner | epoch 061:    340 / 1021 loss=5.759, nll_loss=4.535, ppl=23.19, wps=24964.6, ups=6.32, wpb=3948.9, bsz=155.7, num_updates=61600, lr=0.000127412, gnorm=1.085, train_wall=16, wall=9800
2024-05-31 18:57:27 | INFO | train_inner | epoch 061:    440 / 1021 loss=5.765, nll_loss=4.543, ppl=23.31, wps=25031.3, ups=6.32, wpb=3960.8, bsz=155.9, num_updates=61700, lr=0.000127309, gnorm=1.045, train_wall=16, wall=9816
2024-05-31 18:57:42 | INFO | train_inner | epoch 061:    540 / 1021 loss=5.798, nll_loss=4.577, ppl=23.87, wps=24879.4, ups=6.39, wpb=3895.7, bsz=139, num_updates=61800, lr=0.000127205, gnorm=1.094, train_wall=16, wall=9832
2024-05-31 18:57:58 | INFO | train_inner | epoch 061:    640 / 1021 loss=5.8, nll_loss=4.583, ppl=23.97, wps=25152.1, ups=6.34, wpb=3964.5, bsz=167.9, num_updates=61900, lr=0.000127103, gnorm=1.079, train_wall=16, wall=9848
2024-05-31 18:58:14 | INFO | train_inner | epoch 061:    740 / 1021 loss=5.789, nll_loss=4.57, ppl=23.76, wps=25097.2, ups=6.37, wpb=3941.6, bsz=153.5, num_updates=62000, lr=0.000127, gnorm=1.066, train_wall=16, wall=9863
2024-05-31 18:58:14 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:58:15 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_61_62000.pt (epoch 61 @ 62000 updates, score None) (writing took 0.9939595102332532 seconds)
2024-05-31 18:58:30 | INFO | train_inner | epoch 061:    840 / 1021 loss=5.81, nll_loss=4.596, ppl=24.19, wps=25036.2, ups=6.36, wpb=3936.5, bsz=175.1, num_updates=62100, lr=0.000126898, gnorm=1.113, train_wall=15, wall=9879
2024-05-31 18:58:44 | INFO | train_inner | epoch 061:    940 / 1021 loss=5.806, nll_loss=4.588, ppl=24.05, wps=26937, ups=6.84, wpb=3939.2, bsz=138, num_updates=62200, lr=0.000126796, gnorm=1.056, train_wall=15, wall=9894
2024-05-31 18:58:57 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 18:58:57 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2024-05-31 18:58:57 | INFO | train | epoch 061 | loss 5.776 | nll_loss 4.556 | ppl 23.52 | wps 24885.7 | ups 6.31 | wpb 3943.1 | bsz 156.9 | num_updates 62281 | lr 0.000126713 | gnorm 1.077 | train_wall 159 | wall 9906
2024-05-31 18:58:57 | INFO | fairseq.trainer | begin training epoch 62
2024-05-31 18:59:00 | INFO | train_inner | epoch 062:     19 / 1021 loss=5.791, nll_loss=4.575, ppl=23.84, wps=24999.1, ups=6.29, wpb=3975.4, bsz=183.4, num_updates=62300, lr=0.000126694, gnorm=1.137, train_wall=16, wall=9909
2024-05-31 18:59:16 | INFO | train_inner | epoch 062:    119 / 1021 loss=5.726, nll_loss=4.498, ppl=22.6, wps=24845.2, ups=6.32, wpb=3931, bsz=167, num_updates=62400, lr=0.000126592, gnorm=1.073, train_wall=16, wall=9925
2024-05-31 18:59:32 | INFO | train_inner | epoch 062:    219 / 1021 loss=5.755, nll_loss=4.533, ppl=23.15, wps=24879.4, ups=6.24, wpb=3984.4, bsz=186.9, num_updates=62500, lr=0.000126491, gnorm=1.106, train_wall=16, wall=9941
2024-05-31 18:59:49 | INFO | train_inner | epoch 062:    319 / 1021 loss=5.751, nll_loss=4.524, ppl=23.01, wps=23582.9, ups=5.98, wpb=3941.4, bsz=138, num_updates=62600, lr=0.00012639, gnorm=1.055, train_wall=17, wall=9958
2024-05-31 19:00:06 | INFO | train_inner | epoch 062:    419 / 1021 loss=5.761, nll_loss=4.537, ppl=23.21, wps=22652.6, ups=5.72, wpb=3961.6, bsz=150.1, num_updates=62700, lr=0.000126289, gnorm=1.068, train_wall=17, wall=9976
2024-05-31 19:00:22 | INFO | train_inner | epoch 062:    519 / 1021 loss=5.774, nll_loss=4.552, ppl=23.46, wps=24925, ups=6.34, wpb=3933.5, bsz=150.8, num_updates=62800, lr=0.000126189, gnorm=1.068, train_wall=16, wall=9991
2024-05-31 19:00:38 | INFO | train_inner | epoch 062:    619 / 1021 loss=5.773, nll_loss=4.551, ppl=23.43, wps=25048, ups=6.39, wpb=3920.9, bsz=147.5, num_updates=62900, lr=0.000126088, gnorm=1.068, train_wall=16, wall=10007
2024-05-31 19:00:53 | INFO | train_inner | epoch 062:    719 / 1021 loss=5.8, nll_loss=4.583, ppl=23.96, wps=24935.8, ups=6.35, wpb=3924.4, bsz=156.6, num_updates=63000, lr=0.000125988, gnorm=1.088, train_wall=16, wall=10023
2024-05-31 19:00:53 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:00:54 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_62_63000.pt (epoch 62 @ 63000 updates, score None) (writing took 0.9762599100358784 seconds)
2024-05-31 19:01:09 | INFO | train_inner | epoch 062:    819 / 1021 loss=5.785, nll_loss=4.564, ppl=23.65, wps=25060.8, ups=6.4, wpb=3916.1, bsz=147.4, num_updates=63100, lr=0.000125888, gnorm=1.077, train_wall=15, wall=10038
2024-05-31 19:01:24 | INFO | train_inner | epoch 062:    919 / 1021 loss=5.795, nll_loss=4.578, ppl=23.88, wps=26777.9, ups=6.78, wpb=3951.2, bsz=168.6, num_updates=63200, lr=0.000125789, gnorm=1.079, train_wall=15, wall=10053
2024-05-31 19:01:38 | INFO | train_inner | epoch 062:   1019 / 1021 loss=5.802, nll_loss=4.584, ppl=23.99, wps=26910.8, ups=6.79, wpb=3962.5, bsz=144.1, num_updates=63300, lr=0.000125689, gnorm=1.05, train_wall=15, wall=10068
2024-05-31 19:01:39 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:01:39 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2024-05-31 19:01:39 | INFO | train | epoch 062 | loss 5.771 | nll_loss 4.55 | ppl 23.42 | wps 24880.8 | ups 6.31 | wpb 3943.1 | bsz 156.9 | num_updates 63302 | lr 0.000125687 | gnorm 1.08 | train_wall 159 | wall 10068
2024-05-31 19:01:39 | INFO | fairseq.trainer | begin training epoch 63
2024-05-31 19:01:54 | INFO | train_inner | epoch 063:     98 / 1021 loss=5.726, nll_loss=4.499, ppl=22.61, wps=24891.5, ups=6.31, wpb=3943.9, bsz=167.6, num_updates=63400, lr=0.00012559, gnorm=1.105, train_wall=16, wall=10084
2024-05-31 19:02:10 | INFO | train_inner | epoch 063:    198 / 1021 loss=5.741, nll_loss=4.513, ppl=22.84, wps=25191.6, ups=6.37, wpb=3956.3, bsz=151.5, num_updates=63500, lr=0.000125491, gnorm=1.075, train_wall=16, wall=10099
2024-05-31 19:02:26 | INFO | train_inner | epoch 063:    298 / 1021 loss=5.736, nll_loss=4.508, ppl=22.76, wps=25020.4, ups=6.4, wpb=3912.5, bsz=145.3, num_updates=63600, lr=0.000125392, gnorm=1.083, train_wall=15, wall=10115
2024-05-31 19:02:41 | INFO | train_inner | epoch 063:    398 / 1021 loss=5.76, nll_loss=4.534, ppl=23.17, wps=25224.5, ups=6.38, wpb=3954.3, bsz=142.3, num_updates=63700, lr=0.000125294, gnorm=1.058, train_wall=16, wall=10131
2024-05-31 19:02:57 | INFO | train_inner | epoch 063:    498 / 1021 loss=5.756, nll_loss=4.532, ppl=23.13, wps=25470.4, ups=6.45, wpb=3946.4, bsz=167.7, num_updates=63800, lr=0.000125196, gnorm=1.142, train_wall=15, wall=10146
2024-05-31 19:03:12 | INFO | train_inner | epoch 063:    598 / 1021 loss=5.771, nll_loss=4.549, ppl=23.4, wps=25182.9, ups=6.38, wpb=3949.1, bsz=149.8, num_updates=63900, lr=0.000125098, gnorm=1.077, train_wall=16, wall=10162
2024-05-31 19:03:28 | INFO | train_inner | epoch 063:    698 / 1021 loss=5.789, nll_loss=4.571, ppl=23.77, wps=25237.6, ups=6.38, wpb=3952.8, bsz=171.7, num_updates=64000, lr=0.000125, gnorm=1.08, train_wall=16, wall=10178
2024-05-31 19:03:28 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:03:29 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_63_64000.pt (epoch 63 @ 64000 updates, score None) (writing took 0.9881296432577074 seconds)
2024-05-31 19:03:44 | INFO | train_inner | epoch 063:    798 / 1021 loss=5.797, nll_loss=4.577, ppl=23.87, wps=25226.8, ups=6.4, wpb=3938.7, bsz=145.6, num_updates=64100, lr=0.000124902, gnorm=1.108, train_wall=15, wall=10193
2024-05-31 19:03:58 | INFO | train_inner | epoch 063:    898 / 1021 loss=5.803, nll_loss=4.587, ppl=24.04, wps=26868.2, ups=6.79, wpb=3958.1, bsz=177.6, num_updates=64200, lr=0.000124805, gnorm=1.081, train_wall=15, wall=10208
2024-05-31 19:04:14 | INFO | train_inner | epoch 063:    998 / 1021 loss=5.804, nll_loss=4.586, ppl=24.01, wps=25107.8, ups=6.39, wpb=3927.4, bsz=143, num_updates=64300, lr=0.000124708, gnorm=1.078, train_wall=15, wall=10223
2024-05-31 19:04:18 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:04:18 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2024-05-31 19:04:18 | INFO | train | epoch 063 | loss 5.769 | nll_loss 4.546 | ppl 23.36 | wps 25319.8 | ups 6.42 | wpb 3943.1 | bsz 156.9 | num_updates 64323 | lr 0.000124686 | gnorm 1.089 | train_wall 156 | wall 10227
2024-05-31 19:04:18 | INFO | fairseq.trainer | begin training epoch 64
2024-05-31 19:04:30 | INFO | train_inner | epoch 064:     77 / 1021 loss=5.73, nll_loss=4.504, ppl=22.68, wps=24800.9, ups=6.29, wpb=3945.4, bsz=169.5, num_updates=64400, lr=0.000124611, gnorm=1.078, train_wall=16, wall=10239
2024-05-31 19:04:46 | INFO | train_inner | epoch 064:    177 / 1021 loss=5.713, nll_loss=4.483, ppl=22.36, wps=24980.6, ups=6.34, wpb=3941.3, bsz=164, num_updates=64500, lr=0.000124515, gnorm=1.07, train_wall=16, wall=10255
2024-05-31 19:05:02 | INFO | train_inner | epoch 064:    277 / 1021 loss=5.725, nll_loss=4.497, ppl=22.58, wps=24894.4, ups=6.27, wpb=3971.8, bsz=173.8, num_updates=64600, lr=0.000124418, gnorm=1.082, train_wall=16, wall=10271
2024-05-31 19:05:18 | INFO | train_inner | epoch 064:    377 / 1021 loss=5.748, nll_loss=4.522, ppl=22.98, wps=24966.8, ups=6.31, wpb=3957.5, bsz=162.6, num_updates=64700, lr=0.000124322, gnorm=1.095, train_wall=16, wall=10287
2024-05-31 19:05:33 | INFO | train_inner | epoch 064:    477 / 1021 loss=5.761, nll_loss=4.535, ppl=23.19, wps=25137.6, ups=6.39, wpb=3934.5, bsz=148.3, num_updates=64800, lr=0.000124226, gnorm=1.068, train_wall=15, wall=10303
2024-05-31 19:05:49 | INFO | train_inner | epoch 064:    577 / 1021 loss=5.777, nll_loss=4.555, ppl=23.51, wps=24967.4, ups=6.37, wpb=3922.1, bsz=151.4, num_updates=64900, lr=0.00012413, gnorm=1.112, train_wall=16, wall=10318
2024-05-31 19:06:06 | INFO | train_inner | epoch 064:    677 / 1021 loss=5.787, nll_loss=4.566, ppl=23.68, wps=22923.2, ups=5.86, wpb=3911.7, bsz=151.4, num_updates=65000, lr=0.000124035, gnorm=1.15, train_wall=17, wall=10335
2024-05-31 19:06:06 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:06:07 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_64_65000.pt (epoch 64 @ 65000 updates, score None) (writing took 0.958528021350503 seconds)
2024-05-31 19:06:23 | INFO | train_inner | epoch 064:    777 / 1021 loss=5.783, nll_loss=4.562, ppl=23.62, wps=23467.4, ups=5.93, wpb=3954.8, bsz=153.2, num_updates=65100, lr=0.000123939, gnorm=1.077, train_wall=16, wall=10352
2024-05-31 19:06:39 | INFO | train_inner | epoch 064:    877 / 1021 loss=5.798, nll_loss=4.58, ppl=23.91, wps=25016.1, ups=6.34, wpb=3942.8, bsz=159.6, num_updates=65200, lr=0.000123844, gnorm=1.095, train_wall=16, wall=10368
2024-05-31 19:06:54 | INFO | train_inner | epoch 064:    977 / 1021 loss=5.794, nll_loss=4.574, ppl=23.82, wps=25196.9, ups=6.41, wpb=3929.7, bsz=145, num_updates=65300, lr=0.000123749, gnorm=1.068, train_wall=15, wall=10384
2024-05-31 19:07:01 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:07:01 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2024-05-31 19:07:01 | INFO | train | epoch 064 | loss 5.762 | nll_loss 4.538 | ppl 23.24 | wps 24631.6 | ups 6.25 | wpb 3943.1 | bsz 156.9 | num_updates 65344 | lr 0.000123708 | gnorm 1.088 | train_wall 161 | wall 10391
2024-05-31 19:07:01 | INFO | fairseq.trainer | begin training epoch 65
2024-05-31 19:07:10 | INFO | train_inner | epoch 065:     56 / 1021 loss=5.74, nll_loss=4.514, ppl=22.85, wps=24884.4, ups=6.31, wpb=3946.2, bsz=156.2, num_updates=65400, lr=0.000123655, gnorm=1.089, train_wall=16, wall=10399
2024-05-31 19:07:25 | INFO | train_inner | epoch 065:    156 / 1021 loss=5.731, nll_loss=4.5, ppl=22.63, wps=25711.9, ups=6.53, wpb=3935.2, bsz=143.4, num_updates=65500, lr=0.00012356, gnorm=1.098, train_wall=15, wall=10415
2024-05-31 19:07:42 | INFO | train_inner | epoch 065:    256 / 1021 loss=5.719, nll_loss=4.488, ppl=22.44, wps=24306, ups=6.17, wpb=3942.5, bsz=148.5, num_updates=65600, lr=0.000123466, gnorm=1.07, train_wall=16, wall=10431
2024-05-31 19:07:57 | INFO | train_inner | epoch 065:    356 / 1021 loss=5.739, nll_loss=4.511, ppl=22.8, wps=24821.1, ups=6.36, wpb=3900.9, bsz=148.6, num_updates=65700, lr=0.000123372, gnorm=1.098, train_wall=16, wall=10447
2024-05-31 19:08:13 | INFO | train_inner | epoch 065:    456 / 1021 loss=5.751, nll_loss=4.526, ppl=23.04, wps=25020.1, ups=6.32, wpb=3959.5, bsz=169.8, num_updates=65800, lr=0.000123278, gnorm=1.162, train_wall=16, wall=10463
2024-05-31 19:08:29 | INFO | train_inner | epoch 065:    556 / 1021 loss=5.769, nll_loss=4.546, ppl=23.36, wps=24879, ups=6.27, wpb=3967.2, bsz=154.8, num_updates=65900, lr=0.000123185, gnorm=1.082, train_wall=16, wall=10478
2024-05-31 19:08:45 | INFO | train_inner | epoch 065:    656 / 1021 loss=5.774, nll_loss=4.553, ppl=23.47, wps=25023.8, ups=6.33, wpb=3953, bsz=164.8, num_updates=66000, lr=0.000123091, gnorm=1.082, train_wall=16, wall=10494
2024-05-31 19:08:45 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:08:46 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_65_66000.pt (epoch 65 @ 66000 updates, score None) (writing took 1.0810717730782926 seconds)
2024-05-31 19:09:02 | INFO | train_inner | epoch 065:    756 / 1021 loss=5.771, nll_loss=4.548, ppl=23.39, wps=23683.1, ups=6.01, wpb=3937.6, bsz=159, num_updates=66100, lr=0.000122998, gnorm=1.079, train_wall=15, wall=10511
2024-05-31 19:09:17 | INFO | train_inner | epoch 065:    856 / 1021 loss=5.787, nll_loss=4.569, ppl=23.73, wps=25112.4, ups=6.33, wpb=3969.1, bsz=174.1, num_updates=66200, lr=0.000122905, gnorm=1.099, train_wall=16, wall=10527
2024-05-31 19:09:33 | INFO | train_inner | epoch 065:    956 / 1021 loss=5.802, nll_loss=4.584, ppl=23.99, wps=25148.9, ups=6.4, wpb=3928.2, bsz=153.6, num_updates=66300, lr=0.000122813, gnorm=1.099, train_wall=15, wall=10542
2024-05-31 19:09:43 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:09:43 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2024-05-31 19:09:43 | INFO | train | epoch 065 | loss 5.759 | nll_loss 4.535 | ppl 23.18 | wps 24848.8 | ups 6.3 | wpb 3943.1 | bsz 156.9 | num_updates 66365 | lr 0.000122753 | gnorm 1.096 | train_wall 159 | wall 10553
2024-05-31 19:09:43 | INFO | fairseq.trainer | begin training epoch 66
2024-05-31 19:09:49 | INFO | train_inner | epoch 066:     35 / 1021 loss=5.76, nll_loss=4.537, ppl=23.21, wps=24725.5, ups=6.26, wpb=3949.2, bsz=153.7, num_updates=66400, lr=0.00012272, gnorm=1.075, train_wall=16, wall=10558
2024-05-31 19:10:05 | INFO | train_inner | epoch 066:    135 / 1021 loss=5.709, nll_loss=4.48, ppl=22.31, wps=25087.6, ups=6.34, wpb=3956.9, bsz=188.1, num_updates=66500, lr=0.000122628, gnorm=1.155, train_wall=16, wall=10574
2024-05-31 19:10:20 | INFO | train_inner | epoch 066:    235 / 1021 loss=5.732, nll_loss=4.503, ppl=22.68, wps=25037.3, ups=6.36, wpb=3937.1, bsz=153.3, num_updates=66600, lr=0.000122536, gnorm=1.081, train_wall=16, wall=10590
2024-05-31 19:10:36 | INFO | train_inner | epoch 066:    335 / 1021 loss=5.741, nll_loss=4.511, ppl=22.8, wps=24943.6, ups=6.38, wpb=3908.9, bsz=133.8, num_updates=66700, lr=0.000122444, gnorm=1.101, train_wall=16, wall=10605
2024-05-31 19:10:52 | INFO | train_inner | epoch 066:    435 / 1021 loss=5.73, nll_loss=4.5, ppl=22.63, wps=25232.4, ups=6.4, wpb=3943.3, bsz=143.6, num_updates=66800, lr=0.000122352, gnorm=1.063, train_wall=15, wall=10621
2024-05-31 19:11:07 | INFO | train_inner | epoch 066:    535 / 1021 loss=5.749, nll_loss=4.523, ppl=22.98, wps=25247.6, ups=6.37, wpb=3965.4, bsz=151.8, num_updates=66900, lr=0.000122261, gnorm=1.066, train_wall=16, wall=10637
2024-05-31 19:11:23 | INFO | train_inner | epoch 066:    635 / 1021 loss=5.788, nll_loss=4.568, ppl=23.71, wps=24976.3, ups=6.31, wpb=3956.3, bsz=162.6, num_updates=67000, lr=0.000122169, gnorm=1.122, train_wall=16, wall=10653
2024-05-31 19:11:23 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:11:24 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_66_67000.pt (epoch 66 @ 67000 updates, score None) (writing took 0.9849045057781041 seconds)
2024-05-31 19:11:41 | INFO | train_inner | epoch 066:    735 / 1021 loss=5.782, nll_loss=4.56, ppl=23.59, wps=22525.5, ups=5.75, wpb=3920.9, bsz=154.9, num_updates=67100, lr=0.000122078, gnorm=1.121, train_wall=16, wall=10670
2024-05-31 19:11:57 | INFO | train_inner | epoch 066:    835 / 1021 loss=5.767, nll_loss=4.544, ppl=23.32, wps=24624.6, ups=6.23, wpb=3951.8, bsz=163.4, num_updates=67200, lr=0.000121988, gnorm=1.097, train_wall=16, wall=10686
2024-05-31 19:12:13 | INFO | train_inner | epoch 066:    935 / 1021 loss=5.772, nll_loss=4.55, ppl=23.43, wps=24694.7, ups=6.25, wpb=3954.2, bsz=162.7, num_updates=67300, lr=0.000121897, gnorm=1.082, train_wall=16, wall=10702
2024-05-31 19:12:26 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:12:26 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2024-05-31 19:12:26 | INFO | train | epoch 066 | loss 5.754 | nll_loss 4.528 | ppl 23.08 | wps 24661 | ups 6.25 | wpb 3943.1 | bsz 156.9 | num_updates 67386 | lr 0.000121819 | gnorm 1.097 | train_wall 160 | wall 10716
2024-05-31 19:12:26 | INFO | fairseq.trainer | begin training epoch 67
2024-05-31 19:12:29 | INFO | train_inner | epoch 067:     14 / 1021 loss=5.771, nll_loss=4.551, ppl=23.44, wps=24341.3, ups=6.18, wpb=3938.3, bsz=168.6, num_updates=67400, lr=0.000121806, gnorm=1.093, train_wall=16, wall=10718
2024-05-31 19:12:45 | INFO | train_inner | epoch 067:    114 / 1021 loss=5.693, nll_loss=4.459, ppl=21.99, wps=24725, ups=6.3, wpb=3925.3, bsz=154.3, num_updates=67500, lr=0.000121716, gnorm=1.085, train_wall=16, wall=10734
2024-05-31 19:13:01 | INFO | train_inner | epoch 067:    214 / 1021 loss=5.728, nll_loss=4.498, ppl=22.6, wps=24599.4, ups=6.24, wpb=3940.9, bsz=162.5, num_updates=67600, lr=0.000121626, gnorm=1.11, train_wall=16, wall=10750
2024-05-31 19:13:17 | INFO | train_inner | epoch 067:    314 / 1021 loss=5.728, nll_loss=4.498, ppl=22.59, wps=25151.4, ups=6.38, wpb=3944.7, bsz=144.6, num_updates=67700, lr=0.000121536, gnorm=1.071, train_wall=16, wall=10766
2024-05-31 19:13:32 | INFO | train_inner | epoch 067:    414 / 1021 loss=5.747, nll_loss=4.52, ppl=22.94, wps=25057, ups=6.33, wpb=3961, bsz=160.9, num_updates=67800, lr=0.000121447, gnorm=1.103, train_wall=16, wall=10782
2024-05-31 19:13:48 | INFO | train_inner | epoch 067:    514 / 1021 loss=5.75, nll_loss=4.522, ppl=22.97, wps=25164.9, ups=6.42, wpb=3921.8, bsz=162.6, num_updates=67900, lr=0.000121357, gnorm=1.161, train_wall=15, wall=10797
2024-05-31 19:14:04 | INFO | train_inner | epoch 067:    614 / 1021 loss=5.768, nll_loss=4.545, ppl=23.34, wps=25209.6, ups=6.36, wpb=3965.3, bsz=155.1, num_updates=68000, lr=0.000121268, gnorm=1.086, train_wall=16, wall=10813
2024-05-31 19:14:04 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:14:05 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_67_68000.pt (epoch 67 @ 68000 updates, score None) (writing took 1.091172322165221 seconds)
2024-05-31 19:14:20 | INFO | train_inner | epoch 067:    714 / 1021 loss=5.77, nll_loss=4.546, ppl=23.36, wps=23460.4, ups=5.99, wpb=3915.4, bsz=153.7, num_updates=68100, lr=0.000121179, gnorm=1.114, train_wall=15, wall=10830
2024-05-31 19:14:36 | INFO | train_inner | epoch 067:    814 / 1021 loss=5.775, nll_loss=4.552, ppl=23.46, wps=25161.3, ups=6.37, wpb=3950.2, bsz=153.9, num_updates=68200, lr=0.00012109, gnorm=1.082, train_wall=16, wall=10845
2024-05-31 19:14:52 | INFO | train_inner | epoch 067:    914 / 1021 loss=5.782, nll_loss=4.559, ppl=23.57, wps=25272.5, ups=6.4, wpb=3948.9, bsz=147, num_updates=68300, lr=0.000121001, gnorm=1.088, train_wall=15, wall=10861
2024-05-31 19:15:07 | INFO | train_inner | epoch 067:   1014 / 1021 loss=5.788, nll_loss=4.569, ppl=23.73, wps=25019.8, ups=6.33, wpb=3951.2, bsz=163.4, num_updates=68400, lr=0.000120913, gnorm=1.146, train_wall=16, wall=10877
2024-05-31 19:15:09 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:15:09 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2024-05-31 19:15:09 | INFO | train | epoch 067 | loss 5.751 | nll_loss 4.525 | ppl 23.02 | wps 24833.9 | ups 6.3 | wpb 3943.1 | bsz 156.9 | num_updates 68407 | lr 0.000120907 | gnorm 1.104 | train_wall 159 | wall 10878
2024-05-31 19:15:09 | INFO | fairseq.trainer | begin training epoch 68
2024-05-31 19:15:23 | INFO | train_inner | epoch 068:     93 / 1021 loss=5.698, nll_loss=4.464, ppl=22.07, wps=24698.3, ups=6.31, wpb=3913.4, bsz=156.5, num_updates=68500, lr=0.000120824, gnorm=1.123, train_wall=16, wall=10893
2024-05-31 19:15:39 | INFO | train_inner | epoch 068:    193 / 1021 loss=5.703, nll_loss=4.468, ppl=22.14, wps=25119.5, ups=6.41, wpb=3918.8, bsz=154.5, num_updates=68600, lr=0.000120736, gnorm=1.108, train_wall=15, wall=10908
2024-05-31 19:15:55 | INFO | train_inner | epoch 068:    293 / 1021 loss=5.732, nll_loss=4.503, ppl=22.67, wps=25059.4, ups=6.35, wpb=3944.1, bsz=156.6, num_updates=68700, lr=0.000120648, gnorm=1.091, train_wall=16, wall=10924
2024-05-31 19:16:11 | INFO | train_inner | epoch 068:    393 / 1021 loss=5.751, nll_loss=4.525, ppl=23.02, wps=24342.4, ups=6.15, wpb=3956.2, bsz=172.1, num_updates=68800, lr=0.000120561, gnorm=1.123, train_wall=16, wall=10940
2024-05-31 19:16:27 | INFO | train_inner | epoch 068:    493 / 1021 loss=5.736, nll_loss=4.506, ppl=22.73, wps=24663.5, ups=6.25, wpb=3946.9, bsz=148.6, num_updates=68900, lr=0.000120473, gnorm=1.136, train_wall=16, wall=10956
2024-05-31 19:16:43 | INFO | train_inner | epoch 068:    593 / 1021 loss=5.758, nll_loss=4.534, ppl=23.16, wps=24685.5, ups=6.23, wpb=3963.8, bsz=168.6, num_updates=69000, lr=0.000120386, gnorm=1.112, train_wall=16, wall=10972
2024-05-31 19:16:43 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:16:44 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_68_69000.pt (epoch 68 @ 69000 updates, score None) (writing took 0.9931759820319712 seconds)
2024-05-31 19:16:59 | INFO | train_inner | epoch 068:    693 / 1021 loss=5.741, nll_loss=4.515, ppl=22.87, wps=24696.7, ups=6.22, wpb=3971.3, bsz=175.8, num_updates=69100, lr=0.000120299, gnorm=1.101, train_wall=15, wall=10988
2024-05-31 19:17:14 | INFO | train_inner | epoch 068:    793 / 1021 loss=5.773, nll_loss=4.549, ppl=23.4, wps=26215.9, ups=6.65, wpb=3940.2, bsz=141, num_updates=69200, lr=0.000120212, gnorm=1.102, train_wall=15, wall=11003
2024-05-31 19:17:30 | INFO | train_inner | epoch 068:    893 / 1021 loss=5.769, nll_loss=4.545, ppl=23.35, wps=24780.3, ups=6.29, wpb=3939.2, bsz=147.7, num_updates=69300, lr=0.000120125, gnorm=1.095, train_wall=16, wall=11019
2024-05-31 19:17:46 | INFO | train_inner | epoch 068:    993 / 1021 loss=5.792, nll_loss=4.572, ppl=23.78, wps=24700.7, ups=6.3, wpb=3921.5, bsz=141.8, num_updates=69400, lr=0.000120038, gnorm=1.092, train_wall=16, wall=11035
2024-05-31 19:17:50 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:17:50 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2024-05-31 19:17:50 | INFO | train | epoch 068 | loss 5.747 | nll_loss 4.52 | ppl 22.94 | wps 24882.8 | ups 6.31 | wpb 3943.1 | bsz 156.9 | num_updates 69428 | lr 0.000120014 | gnorm 1.109 | train_wall 159 | wall 11040
2024-05-31 19:17:50 | INFO | fairseq.trainer | begin training epoch 69
2024-05-31 19:18:02 | INFO | train_inner | epoch 069:     72 / 1021 loss=5.718, nll_loss=4.489, ppl=22.45, wps=24445.6, ups=6.16, wpb=3969.9, bsz=168.5, num_updates=69500, lr=0.000119952, gnorm=1.114, train_wall=16, wall=11051
2024-05-31 19:18:18 | INFO | train_inner | epoch 069:    172 / 1021 loss=5.705, nll_loss=4.472, ppl=22.2, wps=24700.1, ups=6.24, wpb=3956.7, bsz=158, num_updates=69600, lr=0.000119866, gnorm=1.085, train_wall=16, wall=11067
2024-05-31 19:18:34 | INFO | train_inner | epoch 069:    272 / 1021 loss=5.715, nll_loss=4.481, ppl=22.33, wps=24645.1, ups=6.31, wpb=3908.7, bsz=147.3, num_updates=69700, lr=0.00011978, gnorm=1.134, train_wall=16, wall=11083
2024-05-31 19:18:50 | INFO | train_inner | epoch 069:    372 / 1021 loss=5.746, nll_loss=4.518, ppl=22.91, wps=24934.2, ups=6.32, wpb=3944.8, bsz=158.5, num_updates=69800, lr=0.000119694, gnorm=1.112, train_wall=16, wall=11099
2024-05-31 19:19:06 | INFO | train_inner | epoch 069:    472 / 1021 loss=5.735, nll_loss=4.508, ppl=22.75, wps=24820.2, ups=6.28, wpb=3955.1, bsz=169.1, num_updates=69900, lr=0.000119608, gnorm=1.112, train_wall=16, wall=11115
2024-05-31 19:19:22 | INFO | train_inner | epoch 069:    572 / 1021 loss=5.747, nll_loss=4.52, ppl=22.94, wps=24727.8, ups=6.26, wpb=3953.2, bsz=153, num_updates=70000, lr=0.000119523, gnorm=1.096, train_wall=16, wall=11131
2024-05-31 19:19:22 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:19:23 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_69_70000.pt (epoch 69 @ 70000 updates, score None) (writing took 0.9937250809744 seconds)
2024-05-31 19:19:38 | INFO | train_inner | epoch 069:    672 / 1021 loss=5.756, nll_loss=4.53, ppl=23.1, wps=23732.5, ups=6.01, wpb=3950.8, bsz=158.2, num_updates=70100, lr=0.000119438, gnorm=1.112, train_wall=16, wall=11148
2024-05-31 19:19:54 | INFO | train_inner | epoch 069:    772 / 1021 loss=5.755, nll_loss=4.528, ppl=23.07, wps=24640.1, ups=6.27, wpb=3931.4, bsz=160, num_updates=70200, lr=0.000119352, gnorm=1.162, train_wall=16, wall=11164
2024-05-31 19:20:10 | INFO | train_inner | epoch 069:    872 / 1021 loss=5.777, nll_loss=4.554, ppl=23.49, wps=24490, ups=6.22, wpb=3940, bsz=143.4, num_updates=70300, lr=0.000119268, gnorm=1.089, train_wall=16, wall=11180
2024-05-31 19:20:26 | INFO | train_inner | epoch 069:    972 / 1021 loss=5.783, nll_loss=4.562, ppl=23.62, wps=25075, ups=6.35, wpb=3949.7, bsz=167, num_updates=70400, lr=0.000119183, gnorm=1.111, train_wall=16, wall=11196
2024-05-31 19:20:34 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:20:34 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2024-05-31 19:20:34 | INFO | train | epoch 069 | loss 5.744 | nll_loss 4.516 | ppl 22.88 | wps 24606.4 | ups 6.24 | wpb 3943.1 | bsz 156.9 | num_updates 70449 | lr 0.000119141 | gnorm 1.112 | train_wall 161 | wall 11203
2024-05-31 19:20:34 | INFO | fairseq.trainer | begin training epoch 70
2024-05-31 19:20:42 | INFO | train_inner | epoch 070:     51 / 1021 loss=5.73, nll_loss=4.499, ppl=22.62, wps=24588.1, ups=6.26, wpb=3929.8, bsz=146.1, num_updates=70500, lr=0.000119098, gnorm=1.113, train_wall=16, wall=11212
2024-05-31 19:20:58 | INFO | train_inner | epoch 070:    151 / 1021 loss=5.696, nll_loss=4.461, ppl=22.02, wps=24853.2, ups=6.31, wpb=3941, bsz=145.8, num_updates=70600, lr=0.000119014, gnorm=1.106, train_wall=16, wall=11227
2024-05-31 19:21:14 | INFO | train_inner | epoch 070:    251 / 1021 loss=5.708, nll_loss=4.474, ppl=22.22, wps=25166.1, ups=6.39, wpb=3939.3, bsz=150.3, num_updates=70700, lr=0.00011893, gnorm=1.09, train_wall=16, wall=11243
2024-05-31 19:21:30 | INFO | train_inner | epoch 070:    351 / 1021 loss=5.716, nll_loss=4.484, ppl=22.37, wps=24776.8, ups=6.27, wpb=3948.9, bsz=152.5, num_updates=70800, lr=0.000118846, gnorm=1.1, train_wall=16, wall=11259
2024-05-31 19:21:46 | INFO | train_inner | epoch 070:    451 / 1021 loss=5.729, nll_loss=4.499, ppl=22.61, wps=24540.4, ups=6.19, wpb=3963.9, bsz=165.7, num_updates=70900, lr=0.000118762, gnorm=1.116, train_wall=16, wall=11275
2024-05-31 19:22:02 | INFO | train_inner | epoch 070:    551 / 1021 loss=5.755, nll_loss=4.528, ppl=23.07, wps=24168.1, ups=6.18, wpb=3913.7, bsz=155.4, num_updates=71000, lr=0.000118678, gnorm=1.137, train_wall=16, wall=11291
2024-05-31 19:22:02 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:22:03 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_70_71000.pt (epoch 70 @ 71000 updates, score None) (writing took 1.021705025807023 seconds)
2024-05-31 19:22:20 | INFO | train_inner | epoch 070:    651 / 1021 loss=5.738, nll_loss=4.51, ppl=22.78, wps=22310.8, ups=5.66, wpb=3942.6, bsz=159.3, num_updates=71100, lr=0.000118595, gnorm=1.146, train_wall=16, wall=11309
2024-05-31 19:22:36 | INFO | train_inner | epoch 070:    751 / 1021 loss=5.761, nll_loss=4.537, ppl=23.21, wps=23824.9, ups=6.01, wpb=3966.2, bsz=163.4, num_updates=71200, lr=0.000118511, gnorm=1.099, train_wall=16, wall=11326
2024-05-31 19:22:52 | INFO | train_inner | epoch 070:    851 / 1021 loss=5.757, nll_loss=4.532, ppl=23.13, wps=24843.1, ups=6.35, wpb=3910.1, bsz=165.8, num_updates=71300, lr=0.000118428, gnorm=1.144, train_wall=16, wall=11341
2024-05-31 19:23:08 | INFO | train_inner | epoch 070:    951 / 1021 loss=5.778, nll_loss=4.555, ppl=23.5, wps=25120.5, ups=6.38, wpb=3940.3, bsz=149.5, num_updates=71400, lr=0.000118345, gnorm=1.104, train_wall=16, wall=11357
2024-05-31 19:23:19 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:23:19 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2024-05-31 19:23:19 | INFO | train | epoch 070 | loss 5.738 | nll_loss 4.509 | ppl 22.77 | wps 24435.5 | ups 6.2 | wpb 3943.1 | bsz 156.9 | num_updates 71470 | lr 0.000118287 | gnorm 1.116 | train_wall 162 | wall 11368
2024-05-31 19:23:19 | INFO | fairseq.trainer | begin training epoch 71
2024-05-31 19:23:24 | INFO | train_inner | epoch 071:     30 / 1021 loss=5.745, nll_loss=4.518, ppl=22.91, wps=24798.9, ups=6.29, wpb=3939.9, bsz=157.3, num_updates=71500, lr=0.000118262, gnorm=1.115, train_wall=16, wall=11373
2024-05-31 19:23:39 | INFO | train_inner | epoch 071:    130 / 1021 loss=5.679, nll_loss=4.441, ppl=21.72, wps=25116.5, ups=6.36, wpb=3946.2, bsz=142.2, num_updates=71600, lr=0.00011818, gnorm=1.094, train_wall=16, wall=11389
2024-05-31 19:23:55 | INFO | train_inner | epoch 071:    230 / 1021 loss=5.708, nll_loss=4.474, ppl=22.22, wps=25073.5, ups=6.35, wpb=3949.7, bsz=151.3, num_updates=71700, lr=0.000118097, gnorm=1.101, train_wall=16, wall=11404
2024-05-31 19:24:11 | INFO | train_inner | epoch 071:    330 / 1021 loss=5.721, nll_loss=4.488, ppl=22.44, wps=25047.5, ups=6.34, wpb=3952.6, bsz=145.6, num_updates=71800, lr=0.000118015, gnorm=1.111, train_wall=16, wall=11420
2024-05-31 19:24:27 | INFO | train_inner | epoch 071:    430 / 1021 loss=5.737, nll_loss=4.507, ppl=22.74, wps=24970.3, ups=6.33, wpb=3943.7, bsz=156.4, num_updates=71900, lr=0.000117933, gnorm=1.182, train_wall=16, wall=11436
2024-05-31 19:24:42 | INFO | train_inner | epoch 071:    530 / 1021 loss=5.735, nll_loss=4.505, ppl=22.71, wps=24916.6, ups=6.34, wpb=3931, bsz=151, num_updates=72000, lr=0.000117851, gnorm=1.117, train_wall=16, wall=11452
2024-05-31 19:24:42 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:24:43 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_71_72000.pt (epoch 71 @ 72000 updates, score None) (writing took 0.9909315058030188 seconds)
2024-05-31 19:24:58 | INFO | train_inner | epoch 071:    630 / 1021 loss=5.739, nll_loss=4.511, ppl=22.8, wps=25105.2, ups=6.38, wpb=3937.5, bsz=159.4, num_updates=72100, lr=0.000117769, gnorm=1.113, train_wall=15, wall=11467
2024-05-31 19:25:15 | INFO | train_inner | epoch 071:    730 / 1021 loss=5.755, nll_loss=4.531, ppl=23.11, wps=23269.3, ups=5.89, wpb=3951.2, bsz=177.8, num_updates=72200, lr=0.000117688, gnorm=1.132, train_wall=17, wall=11484
2024-05-31 19:25:31 | INFO | train_inner | epoch 071:    830 / 1021 loss=5.761, nll_loss=4.535, ppl=23.19, wps=24377.4, ups=6.19, wpb=3938.8, bsz=161, num_updates=72300, lr=0.000117606, gnorm=1.123, train_wall=16, wall=11501
2024-05-31 19:25:47 | INFO | train_inner | epoch 071:    930 / 1021 loss=5.769, nll_loss=4.546, ppl=23.36, wps=25259.7, ups=6.39, wpb=3953.6, bsz=169.4, num_updates=72400, lr=0.000117525, gnorm=1.128, train_wall=15, wall=11516
2024-05-31 19:26:01 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:26:01 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2024-05-31 19:26:01 | INFO | train | epoch 071 | loss 5.735 | nll_loss 4.506 | ppl 22.72 | wps 24788.2 | ups 6.29 | wpb 3943.1 | bsz 156.9 | num_updates 72491 | lr 0.000117451 | gnorm 1.122 | train_wall 160 | wall 11530
2024-05-31 19:26:01 | INFO | fairseq.trainer | begin training epoch 72
2024-05-31 19:26:03 | INFO | train_inner | epoch 072:      9 / 1021 loss=5.763, nll_loss=4.539, ppl=23.25, wps=24905.6, ups=6.32, wpb=3941.9, bsz=161.8, num_updates=72500, lr=0.000117444, gnorm=1.124, train_wall=15, wall=11532
2024-05-31 19:26:18 | INFO | train_inner | epoch 072:    109 / 1021 loss=5.695, nll_loss=4.46, ppl=22.01, wps=25179.1, ups=6.37, wpb=3951.8, bsz=152.2, num_updates=72600, lr=0.000117363, gnorm=1.091, train_wall=16, wall=11548
2024-05-31 19:26:34 | INFO | train_inner | epoch 072:    209 / 1021 loss=5.708, nll_loss=4.474, ppl=22.22, wps=25338, ups=6.42, wpb=3946.1, bsz=156.2, num_updates=72700, lr=0.000117282, gnorm=1.14, train_wall=15, wall=11563
2024-05-31 19:26:50 | INFO | train_inner | epoch 072:    309 / 1021 loss=5.716, nll_loss=4.484, ppl=22.38, wps=24650.9, ups=6.23, wpb=3957.3, bsz=155, num_updates=72800, lr=0.000117202, gnorm=1.129, train_wall=16, wall=11579
2024-05-31 19:27:06 | INFO | train_inner | epoch 072:    409 / 1021 loss=5.727, nll_loss=4.498, ppl=22.59, wps=24592.2, ups=6.21, wpb=3960.1, bsz=182.1, num_updates=72900, lr=0.000117121, gnorm=1.167, train_wall=16, wall=11595
2024-05-31 19:27:22 | INFO | train_inner | epoch 072:    509 / 1021 loss=5.737, nll_loss=4.506, ppl=22.72, wps=25154, ups=6.39, wpb=3938.6, bsz=144.5, num_updates=73000, lr=0.000117041, gnorm=1.105, train_wall=16, wall=11611
2024-05-31 19:27:22 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:27:23 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_72_73000.pt (epoch 72 @ 73000 updates, score None) (writing took 1.0096619920805097 seconds)
2024-05-31 19:27:38 | INFO | train_inner | epoch 072:    609 / 1021 loss=5.732, nll_loss=4.501, ppl=22.65, wps=24444.3, ups=6.21, wpb=3936.2, bsz=155.9, num_updates=73100, lr=0.000116961, gnorm=1.118, train_wall=15, wall=11627
2024-05-31 19:27:53 | INFO | train_inner | epoch 072:    709 / 1021 loss=5.736, nll_loss=4.508, ppl=22.76, wps=25243.2, ups=6.42, wpb=3931.2, bsz=166.6, num_updates=73200, lr=0.000116881, gnorm=1.12, train_wall=15, wall=11643
2024-05-31 19:28:09 | INFO | train_inner | epoch 072:    809 / 1021 loss=5.739, nll_loss=4.51, ppl=22.79, wps=25256.8, ups=6.43, wpb=3930.8, bsz=161.8, num_updates=73300, lr=0.000116801, gnorm=1.17, train_wall=15, wall=11658
2024-05-31 19:28:24 | INFO | train_inner | epoch 072:    909 / 1021 loss=5.761, nll_loss=4.534, ppl=23.17, wps=25832, ups=6.57, wpb=3932.9, bsz=146.5, num_updates=73400, lr=0.000116722, gnorm=1.111, train_wall=15, wall=11674
2024-05-31 19:28:39 | INFO | train_inner | epoch 072:   1009 / 1021 loss=5.77, nll_loss=4.545, ppl=23.35, wps=26929.8, ups=6.81, wpb=3952.6, bsz=153.8, num_updates=73500, lr=0.000116642, gnorm=1.113, train_wall=15, wall=11688
2024-05-31 19:28:41 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:28:41 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2024-05-31 19:28:41 | INFO | train | epoch 072 | loss 5.732 | nll_loss 4.502 | ppl 22.65 | wps 25234.6 | ups 6.4 | wpb 3943.1 | bsz 156.9 | num_updates 73512 | lr 0.000116633 | gnorm 1.125 | train_wall 157 | wall 11690
2024-05-31 19:28:41 | INFO | fairseq.trainer | begin training epoch 73
2024-05-31 19:28:54 | INFO | train_inner | epoch 073:     88 / 1021 loss=5.693, nll_loss=4.457, ppl=21.97, wps=26273.3, ups=6.66, wpb=3946.6, bsz=152.1, num_updates=73600, lr=0.000116563, gnorm=1.213, train_wall=15, wall=11703
2024-05-31 19:29:10 | INFO | train_inner | epoch 073:    188 / 1021 loss=5.689, nll_loss=4.453, ppl=21.9, wps=24258.2, ups=6.12, wpb=3962.8, bsz=166.8, num_updates=73700, lr=0.000116484, gnorm=1.112, train_wall=16, wall=11720
2024-05-31 19:29:26 | INFO | train_inner | epoch 073:    288 / 1021 loss=5.689, nll_loss=4.451, ppl=21.88, wps=25257.2, ups=6.44, wpb=3919, bsz=143.7, num_updates=73800, lr=0.000116405, gnorm=1.114, train_wall=15, wall=11735
2024-05-31 19:29:42 | INFO | train_inner | epoch 073:    388 / 1021 loss=5.733, nll_loss=4.501, ppl=22.64, wps=24969.9, ups=6.34, wpb=3938.1, bsz=151.4, num_updates=73900, lr=0.000116326, gnorm=1.127, train_wall=16, wall=11751
2024-05-31 19:29:57 | INFO | train_inner | epoch 073:    488 / 1021 loss=5.716, nll_loss=4.483, ppl=22.36, wps=25372.8, ups=6.42, wpb=3950.3, bsz=151, num_updates=74000, lr=0.000116248, gnorm=1.095, train_wall=15, wall=11767
2024-05-31 19:29:57 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:29:58 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_73_74000.pt (epoch 73 @ 74000 updates, score None) (writing took 0.9921876550652087 seconds)
2024-05-31 19:30:14 | INFO | train_inner | epoch 073:    588 / 1021 loss=5.747, nll_loss=4.519, ppl=22.92, wps=23831.6, ups=6.03, wpb=3955.3, bsz=158.6, num_updates=74100, lr=0.000116169, gnorm=1.138, train_wall=15, wall=11783
2024-05-31 19:30:29 | INFO | train_inner | epoch 073:    688 / 1021 loss=5.759, nll_loss=4.533, ppl=23.15, wps=25135.5, ups=6.39, wpb=3934.7, bsz=164.3, num_updates=74200, lr=0.000116091, gnorm=1.141, train_wall=15, wall=11799
2024-05-31 19:30:45 | INFO | train_inner | epoch 073:    788 / 1021 loss=5.744, nll_loss=4.515, ppl=22.87, wps=24975.3, ups=6.3, wpb=3965.3, bsz=157.4, num_updates=74300, lr=0.000116013, gnorm=1.117, train_wall=16, wall=11815
2024-05-31 19:31:01 | INFO | train_inner | epoch 073:    888 / 1021 loss=5.751, nll_loss=4.524, ppl=23.01, wps=25228.7, ups=6.38, wpb=3952.1, bsz=165.4, num_updates=74400, lr=0.000115935, gnorm=1.153, train_wall=16, wall=11830
2024-05-31 19:31:17 | INFO | train_inner | epoch 073:    988 / 1021 loss=5.762, nll_loss=4.536, ppl=23.2, wps=25005.8, ups=6.41, wpb=3901.5, bsz=156.1, num_updates=74500, lr=0.000115857, gnorm=1.22, train_wall=15, wall=11846
2024-05-31 19:31:22 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:31:22 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2024-05-31 19:31:22 | INFO | train | epoch 073 | loss 5.729 | nll_loss 4.498 | ppl 22.6 | wps 25010.9 | ups 6.34 | wpb 3943.1 | bsz 156.9 | num_updates 74533 | lr 0.000115831 | gnorm 1.142 | train_wall 158 | wall 11851
2024-05-31 19:31:22 | INFO | fairseq.trainer | begin training epoch 74
2024-05-31 19:31:32 | INFO | train_inner | epoch 074:     67 / 1021 loss=5.702, nll_loss=4.468, ppl=22.13, wps=25251.6, ups=6.39, wpb=3952.6, bsz=156, num_updates=74600, lr=0.000115779, gnorm=1.1, train_wall=15, wall=11862
2024-05-31 19:31:48 | INFO | train_inner | epoch 074:    167 / 1021 loss=5.682, nll_loss=4.445, ppl=21.77, wps=25274.8, ups=6.37, wpb=3968.8, bsz=165.9, num_updates=74700, lr=0.000115702, gnorm=1.105, train_wall=16, wall=11877
2024-05-31 19:32:04 | INFO | train_inner | epoch 074:    267 / 1021 loss=5.702, nll_loss=4.467, ppl=22.11, wps=25036.5, ups=6.33, wpb=3953.4, bsz=153.5, num_updates=74800, lr=0.000115624, gnorm=1.123, train_wall=16, wall=11893
2024-05-31 19:32:19 | INFO | train_inner | epoch 074:    367 / 1021 loss=5.72, nll_loss=4.487, ppl=22.43, wps=25250.9, ups=6.4, wpb=3947.8, bsz=170.9, num_updates=74900, lr=0.000115547, gnorm=1.199, train_wall=15, wall=11909
2024-05-31 19:32:35 | INFO | train_inner | epoch 074:    467 / 1021 loss=5.724, nll_loss=4.491, ppl=22.49, wps=25246.3, ups=6.41, wpb=3938.5, bsz=147.3, num_updates=75000, lr=0.00011547, gnorm=1.125, train_wall=15, wall=11924
2024-05-31 19:32:35 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:32:36 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_74_75000.pt (epoch 74 @ 75000 updates, score None) (writing took 0.9805045109242201 seconds)
2024-05-31 19:32:51 | INFO | train_inner | epoch 074:    567 / 1021 loss=5.727, nll_loss=4.496, ppl=22.56, wps=24757.8, ups=6.34, wpb=3906.8, bsz=162.2, num_updates=75100, lr=0.000115393, gnorm=1.152, train_wall=15, wall=11940
2024-05-31 19:33:08 | INFO | train_inner | epoch 074:    667 / 1021 loss=5.729, nll_loss=4.497, ppl=22.59, wps=22313.6, ups=5.74, wpb=3888.5, bsz=148.1, num_updates=75200, lr=0.000115316, gnorm=1.855, train_wall=17, wall=11957
2024-05-31 19:33:26 | INFO | train_inner | epoch 074:    767 / 1021 loss=5.738, nll_loss=4.508, ppl=22.75, wps=21907.3, ups=5.55, wpb=3948.4, bsz=156.1, num_updates=75300, lr=0.00011524, gnorm=1.121, train_wall=18, wall=11976
2024-05-31 19:33:44 | INFO | train_inner | epoch 074:    867 / 1021 loss=5.745, nll_loss=4.517, ppl=22.89, wps=22044, ups=5.55, wpb=3970.4, bsz=151, num_updates=75400, lr=0.000115163, gnorm=1.097, train_wall=18, wall=11994
2024-05-31 19:34:02 | INFO | train_inner | epoch 074:    967 / 1021 loss=5.764, nll_loss=4.539, ppl=23.24, wps=22028.4, ups=5.57, wpb=3954.2, bsz=155.7, num_updates=75500, lr=0.000115087, gnorm=1.14, train_wall=18, wall=12011
2024-05-31 19:34:12 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:34:12 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2024-05-31 19:34:12 | INFO | train | epoch 074 | loss 5.724 | nll_loss 4.493 | ppl 22.51 | wps 23653.8 | ups 6 | wpb 3943.1 | bsz 156.9 | num_updates 75554 | lr 0.000115046 | gnorm 1.201 | train_wall 167 | wall 12021
2024-05-31 19:34:12 | INFO | fairseq.trainer | begin training epoch 75
2024-05-31 19:34:20 | INFO | train_inner | epoch 075:     46 / 1021 loss=5.727, nll_loss=4.497, ppl=22.59, wps=22510.9, ups=5.67, wpb=3972.1, bsz=167, num_updates=75600, lr=0.000115011, gnorm=1.118, train_wall=17, wall=12029
2024-05-31 19:34:36 | INFO | train_inner | epoch 075:    146 / 1021 loss=5.69, nll_loss=4.453, ppl=21.9, wps=23874.5, ups=6.05, wpb=3945.9, bsz=157.1, num_updates=75700, lr=0.000114935, gnorm=1.191, train_wall=16, wall=12046
2024-05-31 19:34:53 | INFO | train_inner | epoch 075:    246 / 1021 loss=5.696, nll_loss=4.46, ppl=22.01, wps=23768.3, ups=6.07, wpb=3918.2, bsz=159.6, num_updates=75800, lr=0.000114859, gnorm=1.14, train_wall=16, wall=12062
2024-05-31 19:35:09 | INFO | train_inner | epoch 075:    346 / 1021 loss=5.712, nll_loss=4.477, ppl=22.27, wps=24370, ups=6.19, wpb=3939.6, bsz=143, num_updates=75900, lr=0.000114783, gnorm=1.126, train_wall=16, wall=12078
2024-05-31 19:35:25 | INFO | train_inner | epoch 075:    446 / 1021 loss=5.706, nll_loss=4.473, ppl=22.21, wps=25109.8, ups=6.36, wpb=3950.9, bsz=174.9, num_updates=76000, lr=0.000114708, gnorm=1.124, train_wall=16, wall=12094
2024-05-31 19:35:25 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:35:26 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_75_76000.pt (epoch 75 @ 76000 updates, score None) (writing took 0.9736764966510236 seconds)
2024-05-31 19:35:40 | INFO | train_inner | epoch 075:    546 / 1021 loss=5.717, nll_loss=4.484, ppl=22.38, wps=25259.1, ups=6.36, wpb=3968.7, bsz=153.9, num_updates=76100, lr=0.000114632, gnorm=1.098, train_wall=15, wall=12110
2024-05-31 19:35:55 | INFO | train_inner | epoch 075:    646 / 1021 loss=5.73, nll_loss=4.5, ppl=22.63, wps=26000.2, ups=6.62, wpb=3925.3, bsz=169.2, num_updates=76200, lr=0.000114557, gnorm=1.166, train_wall=15, wall=12125
2024-05-31 19:36:11 | INFO | train_inner | epoch 075:    746 / 1021 loss=5.745, nll_loss=4.516, ppl=22.88, wps=25071.2, ups=6.36, wpb=3942.8, bsz=153.8, num_updates=76300, lr=0.000114482, gnorm=1.148, train_wall=16, wall=12141
2024-05-31 19:36:27 | INFO | train_inner | epoch 075:    846 / 1021 loss=5.74, nll_loss=4.51, ppl=22.78, wps=25047.5, ups=6.38, wpb=3925.7, bsz=143.7, num_updates=76400, lr=0.000114407, gnorm=1.114, train_wall=16, wall=12156
2024-05-31 19:36:43 | INFO | train_inner | epoch 075:    946 / 1021 loss=5.756, nll_loss=4.528, ppl=23.07, wps=25044, ups=6.36, wpb=3939.8, bsz=151.1, num_updates=76500, lr=0.000114332, gnorm=1.143, train_wall=16, wall=12172
2024-05-31 19:36:54 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:36:54 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2024-05-31 19:36:54 | INFO | train | epoch 075 | loss 5.722 | nll_loss 4.489 | ppl 22.46 | wps 24753.4 | ups 6.28 | wpb 3943.1 | bsz 156.9 | num_updates 76575 | lr 0.000114276 | gnorm 1.137 | train_wall 160 | wall 12184
2024-05-31 19:36:55 | INFO | fairseq.trainer | begin training epoch 76
2024-05-31 19:36:59 | INFO | train_inner | epoch 076:     25 / 1021 loss=5.726, nll_loss=4.495, ppl=22.55, wps=24788.4, ups=6.27, wpb=3955.1, bsz=159, num_updates=76600, lr=0.000114258, gnorm=1.119, train_wall=16, wall=12188
2024-05-31 19:37:14 | INFO | train_inner | epoch 076:    125 / 1021 loss=5.679, nll_loss=4.44, ppl=21.71, wps=25112.1, ups=6.36, wpb=3950.4, bsz=155.4, num_updates=76700, lr=0.000114183, gnorm=1.121, train_wall=16, wall=12204
2024-05-31 19:37:30 | INFO | train_inner | epoch 076:    225 / 1021 loss=5.693, nll_loss=4.456, ppl=21.95, wps=25111.9, ups=6.36, wpb=3948.4, bsz=158.6, num_updates=76800, lr=0.000114109, gnorm=1.13, train_wall=16, wall=12219
2024-05-31 19:37:46 | INFO | train_inner | epoch 076:    325 / 1021 loss=5.664, nll_loss=4.423, ppl=21.46, wps=25054.9, ups=6.36, wpb=3941.1, bsz=153.9, num_updates=76900, lr=0.000114035, gnorm=1.099, train_wall=16, wall=12235
2024-05-31 19:38:02 | INFO | train_inner | epoch 076:    425 / 1021 loss=5.732, nll_loss=4.5, ppl=22.63, wps=24707.4, ups=6.27, wpb=3938.2, bsz=168.5, num_updates=77000, lr=0.000113961, gnorm=1.247, train_wall=16, wall=12251
2024-05-31 19:38:02 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:38:03 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_76_77000.pt (epoch 76 @ 77000 updates, score None) (writing took 1.0496141463518143 seconds)
2024-05-31 19:38:19 | INFO | train_inner | epoch 076:    525 / 1021 loss=5.729, nll_loss=4.497, ppl=22.58, wps=23180.9, ups=5.91, wpb=3921.2, bsz=158.5, num_updates=77100, lr=0.000113887, gnorm=1.154, train_wall=16, wall=12268
2024-05-31 19:38:34 | INFO | train_inner | epoch 076:    625 / 1021 loss=5.721, nll_loss=4.487, ppl=22.43, wps=25261.6, ups=6.42, wpb=3934.3, bsz=139, num_updates=77200, lr=0.000113813, gnorm=1.121, train_wall=15, wall=12284
2024-05-31 19:38:50 | INFO | train_inner | epoch 076:    725 / 1021 loss=5.729, nll_loss=4.498, ppl=22.59, wps=24891.1, ups=6.31, wpb=3946.4, bsz=154.6, num_updates=77300, lr=0.000113739, gnorm=1.131, train_wall=16, wall=12299
2024-05-31 19:39:06 | INFO | train_inner | epoch 076:    825 / 1021 loss=5.733, nll_loss=4.502, ppl=22.66, wps=25080, ups=6.38, wpb=3931, bsz=155.9, num_updates=77400, lr=0.000113666, gnorm=1.161, train_wall=16, wall=12315
2024-05-31 19:39:22 | INFO | train_inner | epoch 076:    925 / 1021 loss=5.743, nll_loss=4.516, ppl=22.88, wps=24881.2, ups=6.27, wpb=3968.2, bsz=171.4, num_updates=77500, lr=0.000113592, gnorm=1.115, train_wall=16, wall=12331
2024-05-31 19:39:37 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:39:37 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2024-05-31 19:39:37 | INFO | train | epoch 076 | loss 5.716 | nll_loss 4.483 | ppl 22.36 | wps 24789.8 | ups 6.29 | wpb 3943.1 | bsz 156.9 | num_updates 77596 | lr 0.000113522 | gnorm 1.14 | train_wall 160 | wall 12346
2024-05-31 19:39:37 | INFO | fairseq.trainer | begin training epoch 77
2024-05-31 19:39:38 | INFO | train_inner | epoch 077:      4 / 1021 loss=5.753, nll_loss=4.525, ppl=23.02, wps=24662.1, ups=6.24, wpb=3949.6, bsz=149.4, num_updates=77600, lr=0.000113519, gnorm=1.128, train_wall=16, wall=12347
2024-05-31 19:39:53 | INFO | train_inner | epoch 077:    104 / 1021 loss=5.668, nll_loss=4.428, ppl=21.52, wps=25156.4, ups=6.36, wpb=3954.2, bsz=158.3, num_updates=77700, lr=0.000113446, gnorm=1.117, train_wall=16, wall=12363
2024-05-31 19:40:09 | INFO | train_inner | epoch 077:    204 / 1021 loss=5.681, nll_loss=4.445, ppl=21.78, wps=24917.1, ups=6.3, wpb=3953, bsz=178, num_updates=77800, lr=0.000113373, gnorm=1.15, train_wall=16, wall=12379
2024-05-31 19:40:25 | INFO | train_inner | epoch 077:    304 / 1021 loss=5.678, nll_loss=4.437, ppl=21.67, wps=25015.2, ups=6.37, wpb=3926.8, bsz=144.8, num_updates=77900, lr=0.0001133, gnorm=1.114, train_wall=16, wall=12394
2024-05-31 19:40:41 | INFO | train_inner | epoch 077:    404 / 1021 loss=5.725, nll_loss=4.492, ppl=22.5, wps=24923.3, ups=6.35, wpb=3921.9, bsz=147.3, num_updates=78000, lr=0.000113228, gnorm=1.169, train_wall=16, wall=12410
2024-05-31 19:40:41 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:40:42 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_77_78000.pt (epoch 77 @ 78000 updates, score None) (writing took 1.0467462288215756 seconds)
2024-05-31 19:40:57 | INFO | train_inner | epoch 077:    504 / 1021 loss=5.713, nll_loss=4.478, ppl=22.28, wps=23791.6, ups=6.07, wpb=3921.7, bsz=149.8, num_updates=78100, lr=0.000113155, gnorm=1.145, train_wall=15, wall=12427
2024-05-31 19:41:13 | INFO | train_inner | epoch 077:    604 / 1021 loss=5.716, nll_loss=4.481, ppl=22.34, wps=25159.3, ups=6.38, wpb=3942.5, bsz=157.1, num_updates=78200, lr=0.000113083, gnorm=1.159, train_wall=16, wall=12442
2024-05-31 19:41:29 | INFO | train_inner | epoch 077:    704 / 1021 loss=5.734, nll_loss=4.503, ppl=22.67, wps=25101.7, ups=6.37, wpb=3939.1, bsz=161.4, num_updates=78300, lr=0.000113011, gnorm=1.196, train_wall=16, wall=12458
2024-05-31 19:41:44 | INFO | train_inner | epoch 077:    804 / 1021 loss=5.741, nll_loss=4.511, ppl=22.81, wps=25244.3, ups=6.36, wpb=3968.8, bsz=163.3, num_updates=78400, lr=0.000112938, gnorm=1.137, train_wall=16, wall=12474
2024-05-31 19:42:00 | INFO | train_inner | epoch 077:    904 / 1021 loss=5.739, nll_loss=4.507, ppl=22.74, wps=25209.6, ups=6.41, wpb=3933.4, bsz=135.3, num_updates=78500, lr=0.000112867, gnorm=1.123, train_wall=15, wall=12489
2024-05-31 19:42:16 | INFO | train_inner | epoch 077:   1004 / 1021 loss=5.735, nll_loss=4.506, ppl=22.72, wps=25189.8, ups=6.36, wpb=3963.4, bsz=168.9, num_updates=78600, lr=0.000112795, gnorm=1.141, train_wall=16, wall=12505
2024-05-31 19:42:18 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:42:18 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2024-05-31 19:42:18 | INFO | train | epoch 077 | loss 5.713 | nll_loss 4.479 | ppl 22.3 | wps 24943.6 | ups 6.33 | wpb 3943.1 | bsz 156.9 | num_updates 78617 | lr 0.000112783 | gnorm 1.144 | train_wall 159 | wall 12508
2024-05-31 19:42:18 | INFO | fairseq.trainer | begin training epoch 78
2024-05-31 19:42:32 | INFO | train_inner | epoch 078:     83 / 1021 loss=5.674, nll_loss=4.434, ppl=21.62, wps=24789.8, ups=6.28, wpb=3947.6, bsz=152.4, num_updates=78700, lr=0.000112723, gnorm=1.108, train_wall=16, wall=12521
2024-05-31 19:42:47 | INFO | train_inner | epoch 078:    183 / 1021 loss=5.662, nll_loss=4.421, ppl=21.42, wps=24922, ups=6.35, wpb=3923.4, bsz=165.1, num_updates=78800, lr=0.000112651, gnorm=1.208, train_wall=16, wall=12537
2024-05-31 19:43:03 | INFO | train_inner | epoch 078:    283 / 1021 loss=5.692, nll_loss=4.453, ppl=21.91, wps=24922.2, ups=6.31, wpb=3950.1, bsz=141, num_updates=78900, lr=0.00011258, gnorm=1.131, train_wall=16, wall=12553
2024-05-31 19:43:19 | INFO | train_inner | epoch 078:    383 / 1021 loss=5.7, nll_loss=4.463, ppl=22.05, wps=25003.7, ups=6.35, wpb=3934.6, bsz=155.5, num_updates=79000, lr=0.000112509, gnorm=1.156, train_wall=16, wall=12568
2024-05-31 19:43:19 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:43:20 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_78_79000.pt (epoch 78 @ 79000 updates, score None) (writing took 1.0200726999901235 seconds)
2024-05-31 19:43:35 | INFO | train_inner | epoch 078:    483 / 1021 loss=5.719, nll_loss=4.484, ppl=22.37, wps=23883.2, ups=6.07, wpb=3935.6, bsz=147.5, num_updates=79100, lr=0.000112438, gnorm=1.152, train_wall=15, wall=12585
2024-05-31 19:43:51 | INFO | train_inner | epoch 078:    583 / 1021 loss=5.71, nll_loss=4.478, ppl=22.28, wps=25142.8, ups=6.35, wpb=3960.6, bsz=181.5, num_updates=79200, lr=0.000112367, gnorm=1.144, train_wall=16, wall=12600
2024-05-31 19:44:07 | INFO | train_inner | epoch 078:    683 / 1021 loss=5.733, nll_loss=4.501, ppl=22.64, wps=25179.5, ups=6.39, wpb=3942.1, bsz=148.6, num_updates=79300, lr=0.000112296, gnorm=1.129, train_wall=16, wall=12616
2024-05-31 19:44:22 | INFO | train_inner | epoch 078:    783 / 1021 loss=5.732, nll_loss=4.501, ppl=22.64, wps=25264.2, ups=6.41, wpb=3941.5, bsz=157, num_updates=79400, lr=0.000112225, gnorm=1.148, train_wall=15, wall=12632
2024-05-31 19:44:38 | INFO | train_inner | epoch 078:    883 / 1021 loss=5.733, nll_loss=4.502, ppl=22.66, wps=25238.3, ups=6.36, wpb=3965.3, bsz=155.4, num_updates=79500, lr=0.000112154, gnorm=1.122, train_wall=16, wall=12647
2024-05-31 19:44:54 | INFO | train_inner | epoch 078:    983 / 1021 loss=5.735, nll_loss=4.505, ppl=22.7, wps=25181, ups=6.41, wpb=3930.9, bsz=162.5, num_updates=79600, lr=0.000112084, gnorm=1.162, train_wall=15, wall=12663
2024-05-31 19:45:00 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:45:00 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2024-05-31 19:45:00 | INFO | train | epoch 078 | loss 5.71 | nll_loss 4.475 | ppl 22.24 | wps 24930.8 | ups 6.32 | wpb 3943.1 | bsz 156.9 | num_updates 79638 | lr 0.000112057 | gnorm 1.148 | train_wall 159 | wall 12669
2024-05-31 19:45:00 | INFO | fairseq.trainer | begin training epoch 79
2024-05-31 19:45:10 | INFO | train_inner | epoch 079:     62 / 1021 loss=5.683, nll_loss=4.446, ppl=21.8, wps=24628.1, ups=6.21, wpb=3968.6, bsz=164.6, num_updates=79700, lr=0.000112014, gnorm=1.138, train_wall=16, wall=12679
2024-05-31 19:45:25 | INFO | train_inner | epoch 079:    162 / 1021 loss=5.678, nll_loss=4.437, ppl=21.67, wps=25037.4, ups=6.41, wpb=3907.4, bsz=144.5, num_updates=79800, lr=0.000111943, gnorm=1.159, train_wall=15, wall=12695
2024-05-31 19:45:41 | INFO | train_inner | epoch 079:    262 / 1021 loss=5.686, nll_loss=4.448, ppl=21.83, wps=25235.5, ups=6.38, wpb=3952.8, bsz=161.8, num_updates=79900, lr=0.000111873, gnorm=1.146, train_wall=16, wall=12710
2024-05-31 19:45:57 | INFO | train_inner | epoch 079:    362 / 1021 loss=5.688, nll_loss=4.45, ppl=21.85, wps=25192.4, ups=6.39, wpb=3940.1, bsz=164.3, num_updates=80000, lr=0.000111803, gnorm=1.161, train_wall=15, wall=12726
2024-05-31 19:45:57 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:45:58 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_79_80000.pt (epoch 79 @ 80000 updates, score None) (writing took 1.0376366400159895 seconds)
2024-05-31 19:46:13 | INFO | train_inner | epoch 079:    462 / 1021 loss=5.708, nll_loss=4.472, ppl=22.2, wps=23987.6, ups=6.08, wpb=3943.9, bsz=151.8, num_updates=80100, lr=0.000111734, gnorm=1.134, train_wall=15, wall=12743
2024-05-31 19:46:29 | INFO | train_inner | epoch 079:    562 / 1021 loss=5.709, nll_loss=4.474, ppl=22.22, wps=24895.4, ups=6.27, wpb=3969.2, bsz=163.5, num_updates=80200, lr=0.000111664, gnorm=1.128, train_wall=16, wall=12758
2024-05-31 19:46:45 | INFO | train_inner | epoch 079:    662 / 1021 loss=5.711, nll_loss=4.474, ppl=22.23, wps=25176.9, ups=6.43, wpb=3916.2, bsz=142.3, num_updates=80300, lr=0.000111594, gnorm=1.156, train_wall=15, wall=12774
2024-05-31 19:47:00 | INFO | train_inner | epoch 079:    762 / 1021 loss=5.729, nll_loss=4.497, ppl=22.58, wps=25112, ups=6.38, wpb=3934.8, bsz=149.6, num_updates=80400, lr=0.000111525, gnorm=1.144, train_wall=16, wall=12790
2024-05-31 19:47:16 | INFO | train_inner | epoch 079:    862 / 1021 loss=5.723, nll_loss=4.493, ppl=22.51, wps=25161.5, ups=6.39, wpb=3939, bsz=181.5, num_updates=80500, lr=0.000111456, gnorm=1.212, train_wall=15, wall=12805
2024-05-31 19:47:32 | INFO | train_inner | epoch 079:    962 / 1021 loss=5.743, nll_loss=4.513, ppl=22.83, wps=25248.2, ups=6.39, wpb=3953.6, bsz=144.1, num_updates=80600, lr=0.000111386, gnorm=1.129, train_wall=16, wall=12821
2024-05-31 19:47:41 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:47:41 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2024-05-31 19:47:41 | INFO | train | epoch 079 | loss 5.707 | nll_loss 4.471 | ppl 22.18 | wps 24973.2 | ups 6.33 | wpb 3943.1 | bsz 156.9 | num_updates 80659 | lr 0.000111346 | gnorm 1.152 | train_wall 158 | wall 12830
2024-05-31 19:47:41 | INFO | fairseq.trainer | begin training epoch 80
2024-05-31 19:47:48 | INFO | train_inner | epoch 080:     41 / 1021 loss=5.712, nll_loss=4.48, ppl=22.31, wps=24813.4, ups=6.25, wpb=3970.4, bsz=175.2, num_updates=80700, lr=0.000111317, gnorm=1.16, train_wall=16, wall=12837
2024-05-31 19:48:03 | INFO | train_inner | epoch 080:    141 / 1021 loss=5.641, nll_loss=4.395, ppl=21.04, wps=25023.8, ups=6.39, wpb=3915.9, bsz=141, num_updates=80800, lr=0.000111249, gnorm=1.137, train_wall=15, wall=12853
2024-05-31 19:48:19 | INFO | train_inner | epoch 080:    241 / 1021 loss=5.694, nll_loss=4.459, ppl=22, wps=25119.4, ups=6.32, wpb=3976.2, bsz=202, num_updates=80900, lr=0.00011118, gnorm=1.228, train_wall=16, wall=12868
2024-05-31 19:48:35 | INFO | train_inner | epoch 080:    341 / 1021 loss=5.688, nll_loss=4.451, ppl=21.87, wps=25218.7, ups=6.36, wpb=3966.7, bsz=167.3, num_updates=81000, lr=0.000111111, gnorm=1.136, train_wall=16, wall=12884
2024-05-31 19:48:35 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:48:36 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_80_81000.pt (epoch 80 @ 81000 updates, score None) (writing took 1.0214554178528488 seconds)
2024-05-31 19:48:51 | INFO | train_inner | epoch 080:    441 / 1021 loss=5.707, nll_loss=4.47, ppl=22.16, wps=24084.5, ups=6.11, wpb=3940.2, bsz=154.6, num_updates=81100, lr=0.000111043, gnorm=1.168, train_wall=15, wall=12901
2024-05-31 19:49:07 | INFO | train_inner | epoch 080:    541 / 1021 loss=5.702, nll_loss=4.464, ppl=22.07, wps=25244.5, ups=6.43, wpb=3926.6, bsz=137.5, num_updates=81200, lr=0.000110974, gnorm=1.136, train_wall=15, wall=12916
2024-05-31 19:49:22 | INFO | train_inner | epoch 080:    641 / 1021 loss=5.705, nll_loss=4.468, ppl=22.13, wps=25174, ups=6.4, wpb=3933.5, bsz=138.6, num_updates=81300, lr=0.000110906, gnorm=1.128, train_wall=15, wall=12932
2024-05-31 19:49:38 | INFO | train_inner | epoch 080:    741 / 1021 loss=5.734, nll_loss=4.501, ppl=22.65, wps=25067.7, ups=6.34, wpb=3956.6, bsz=149.9, num_updates=81400, lr=0.000110838, gnorm=1.161, train_wall=16, wall=12948
2024-05-31 19:49:54 | INFO | train_inner | epoch 080:    841 / 1021 loss=5.714, nll_loss=4.481, ppl=22.33, wps=24843.1, ups=6.28, wpb=3953.9, bsz=170, num_updates=81500, lr=0.00011077, gnorm=1.137, train_wall=16, wall=12963
2024-05-31 19:50:10 | INFO | train_inner | epoch 080:    941 / 1021 loss=5.732, nll_loss=4.5, ppl=22.63, wps=24917.5, ups=6.38, wpb=3905.7, bsz=156.6, num_updates=81600, lr=0.000110702, gnorm=1.187, train_wall=16, wall=12979
2024-05-31 19:50:22 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:50:22 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2024-05-31 19:50:22 | INFO | train | epoch 080 | loss 5.704 | nll_loss 4.468 | ppl 22.12 | wps 24951.5 | ups 6.33 | wpb 3943.1 | bsz 156.9 | num_updates 81680 | lr 0.000110648 | gnorm 1.155 | train_wall 159 | wall 12992
2024-05-31 19:50:22 | INFO | fairseq.trainer | begin training epoch 81
2024-05-31 19:50:26 | INFO | train_inner | epoch 081:     20 / 1021 loss=5.725, nll_loss=4.492, ppl=22.5, wps=24781.1, ups=6.28, wpb=3945.7, bsz=144.7, num_updates=81700, lr=0.000110634, gnorm=1.139, train_wall=16, wall=12995
2024-05-31 19:50:41 | INFO | train_inner | epoch 081:    120 / 1021 loss=5.657, nll_loss=4.414, ppl=21.31, wps=25108.9, ups=6.35, wpb=3955.8, bsz=155.1, num_updates=81800, lr=0.000110566, gnorm=1.157, train_wall=16, wall=13011
2024-05-31 19:50:57 | INFO | train_inner | epoch 081:    220 / 1021 loss=5.663, nll_loss=4.42, ppl=21.41, wps=25045, ups=6.39, wpb=3920.1, bsz=147.9, num_updates=81900, lr=0.000110499, gnorm=1.153, train_wall=15, wall=13026
2024-05-31 19:51:13 | INFO | train_inner | epoch 081:    320 / 1021 loss=5.678, nll_loss=4.436, ppl=21.65, wps=25233.9, ups=6.41, wpb=3936.5, bsz=146.7, num_updates=82000, lr=0.000110432, gnorm=1.152, train_wall=15, wall=13042
2024-05-31 19:51:13 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:51:14 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_81_82000.pt (epoch 81 @ 82000 updates, score None) (writing took 1.041258028242737 seconds)
2024-05-31 19:51:29 | INFO | train_inner | epoch 081:    420 / 1021 loss=5.692, nll_loss=4.454, ppl=21.92, wps=23950, ups=6.08, wpb=3941.8, bsz=147.4, num_updates=82100, lr=0.000110364, gnorm=1.135, train_wall=15, wall=13059
2024-05-31 19:51:45 | INFO | train_inner | epoch 081:    520 / 1021 loss=5.705, nll_loss=4.47, ppl=22.16, wps=25173.4, ups=6.37, wpb=3953.5, bsz=178.3, num_updates=82200, lr=0.000110297, gnorm=1.238, train_wall=16, wall=13074
2024-05-31 19:52:01 | INFO | train_inner | epoch 081:    620 / 1021 loss=5.698, nll_loss=4.462, ppl=22.04, wps=24913.2, ups=6.29, wpb=3959.9, bsz=171.4, num_updates=82300, lr=0.00011023, gnorm=1.138, train_wall=16, wall=13090
2024-05-31 19:52:16 | INFO | train_inner | epoch 081:    720 / 1021 loss=5.722, nll_loss=4.488, ppl=22.44, wps=25095.5, ups=6.35, wpb=3949.5, bsz=153, num_updates=82400, lr=0.000110163, gnorm=1.152, train_wall=16, wall=13106
2024-05-31 19:52:32 | INFO | train_inner | epoch 081:    820 / 1021 loss=5.734, nll_loss=4.502, ppl=22.66, wps=25171.6, ups=6.41, wpb=3928.2, bsz=153.4, num_updates=82500, lr=0.000110096, gnorm=1.181, train_wall=15, wall=13121
2024-05-31 19:52:48 | INFO | train_inner | epoch 081:    920 / 1021 loss=5.731, nll_loss=4.499, ppl=22.6, wps=25125.1, ups=6.39, wpb=3931.7, bsz=145.6, num_updates=82600, lr=0.00011003, gnorm=1.15, train_wall=15, wall=13137
2024-05-31 19:53:03 | INFO | train_inner | epoch 081:   1020 / 1021 loss=5.742, nll_loss=4.513, ppl=22.83, wps=25126.3, ups=6.37, wpb=3941.4, bsz=167.6, num_updates=82700, lr=0.000109963, gnorm=1.18, train_wall=16, wall=13153
2024-05-31 19:53:04 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:53:04 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2024-05-31 19:53:04 | INFO | train | epoch 081 | loss 5.701 | nll_loss 4.465 | ppl 22.08 | wps 24958.1 | ups 6.33 | wpb 3943.1 | bsz 156.9 | num_updates 82701 | lr 0.000109963 | gnorm 1.162 | train_wall 158 | wall 13153
2024-05-31 19:53:04 | INFO | fairseq.trainer | begin training epoch 82
2024-05-31 19:53:20 | INFO | train_inner | epoch 082:     99 / 1021 loss=5.637, nll_loss=4.391, ppl=20.99, wps=24250.1, ups=6.14, wpb=3949.1, bsz=148, num_updates=82800, lr=0.000109897, gnorm=1.128, train_wall=16, wall=13169
2024-05-31 19:53:36 | INFO | train_inner | epoch 082:    199 / 1021 loss=5.645, nll_loss=4.399, ppl=21.09, wps=24768.1, ups=6.32, wpb=3918, bsz=154.2, num_updates=82900, lr=0.00010983, gnorm=1.157, train_wall=16, wall=13185
2024-05-31 19:53:51 | INFO | train_inner | epoch 082:    299 / 1021 loss=5.687, nll_loss=4.446, ppl=21.79, wps=25127.9, ups=6.39, wpb=3929.9, bsz=138.1, num_updates=83000, lr=0.000109764, gnorm=1.178, train_wall=15, wall=13201
2024-05-31 19:53:51 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:53:52 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_82_83000.pt (epoch 82 @ 83000 updates, score None) (writing took 1.0304799331352115 seconds)
2024-05-31 19:54:08 | INFO | train_inner | epoch 082:    399 / 1021 loss=5.694, nll_loss=4.456, ppl=21.95, wps=23999.9, ups=6.09, wpb=3941, bsz=164.2, num_updates=83100, lr=0.000109698, gnorm=1.229, train_wall=15, wall=13217
2024-05-31 19:54:23 | INFO | train_inner | epoch 082:    499 / 1021 loss=5.679, nll_loss=4.441, ppl=21.72, wps=25127.5, ups=6.35, wpb=3956.3, bsz=179, num_updates=83200, lr=0.000109632, gnorm=1.165, train_wall=16, wall=13233
2024-05-31 19:54:39 | INFO | train_inner | epoch 082:    599 / 1021 loss=5.71, nll_loss=4.474, ppl=22.23, wps=25130.2, ups=6.4, wpb=3926.4, bsz=157.4, num_updates=83300, lr=0.000109566, gnorm=1.186, train_wall=15, wall=13248
2024-05-31 19:54:55 | INFO | train_inner | epoch 082:    699 / 1021 loss=5.707, nll_loss=4.473, ppl=22.2, wps=25105, ups=6.33, wpb=3965.1, bsz=164.4, num_updates=83400, lr=0.000109501, gnorm=1.133, train_wall=16, wall=13264
2024-05-31 19:55:11 | INFO | train_inner | epoch 082:    799 / 1021 loss=5.731, nll_loss=4.5, ppl=22.62, wps=24914.6, ups=6.31, wpb=3950.7, bsz=163.9, num_updates=83500, lr=0.000109435, gnorm=1.164, train_wall=16, wall=13280
2024-05-31 19:55:26 | INFO | train_inner | epoch 082:    899 / 1021 loss=5.731, nll_loss=4.498, ppl=22.6, wps=25146.7, ups=6.36, wpb=3955.3, bsz=154.5, num_updates=83600, lr=0.00010937, gnorm=1.17, train_wall=16, wall=13296
2024-05-31 19:55:42 | INFO | train_inner | epoch 082:    999 / 1021 loss=5.743, nll_loss=4.512, ppl=22.82, wps=25236.3, ups=6.4, wpb=3941.9, bsz=147.5, num_updates=83700, lr=0.000109304, gnorm=1.147, train_wall=15, wall=13311
2024-05-31 19:55:45 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:55:45 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2024-05-31 19:55:45 | INFO | train | epoch 082 | loss 5.698 | nll_loss 4.461 | ppl 22.02 | wps 24880.5 | ups 6.31 | wpb 3943.1 | bsz 156.9 | num_updates 83722 | lr 0.00010929 | gnorm 1.166 | train_wall 159 | wall 13315
2024-05-31 19:55:45 | INFO | fairseq.trainer | begin training epoch 83
2024-05-31 19:55:58 | INFO | train_inner | epoch 083:     78 / 1021 loss=5.662, nll_loss=4.421, ppl=21.42, wps=24743.5, ups=6.29, wpb=3933.6, bsz=168.2, num_updates=83800, lr=0.000109239, gnorm=1.176, train_wall=16, wall=13327
2024-05-31 19:56:13 | INFO | train_inner | epoch 083:    178 / 1021 loss=5.671, nll_loss=4.428, ppl=21.53, wps=25309.5, ups=6.41, wpb=3950.3, bsz=143.8, num_updates=83900, lr=0.000109174, gnorm=1.141, train_wall=15, wall=13343
2024-05-31 19:56:29 | INFO | train_inner | epoch 083:    278 / 1021 loss=5.672, nll_loss=4.429, ppl=21.55, wps=25203.5, ups=6.42, wpb=3926.7, bsz=144.8, num_updates=84000, lr=0.000109109, gnorm=1.178, train_wall=15, wall=13358
2024-05-31 19:56:29 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:56:30 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_83_84000.pt (epoch 83 @ 84000 updates, score None) (writing took 1.0012932172976434 seconds)
2024-05-31 19:56:46 | INFO | train_inner | epoch 083:    378 / 1021 loss=5.681, nll_loss=4.441, ppl=21.73, wps=23547.1, ups=5.96, wpb=3952.8, bsz=161.7, num_updates=84100, lr=0.000109044, gnorm=1.161, train_wall=16, wall=13375
2024-05-31 19:57:02 | INFO | train_inner | epoch 083:    478 / 1021 loss=5.695, nll_loss=4.456, ppl=21.95, wps=25117, ups=6.38, wpb=3937.3, bsz=145.9, num_updates=84200, lr=0.000108979, gnorm=1.14, train_wall=16, wall=13391
2024-05-31 19:57:17 | INFO | train_inner | epoch 083:    578 / 1021 loss=5.692, nll_loss=4.456, ppl=21.95, wps=25001.2, ups=6.3, wpb=3968.4, bsz=179.4, num_updates=84300, lr=0.000108915, gnorm=1.154, train_wall=16, wall=13407
2024-05-31 19:57:33 | INFO | train_inner | epoch 083:    678 / 1021 loss=5.707, nll_loss=4.471, ppl=22.18, wps=25067.2, ups=6.36, wpb=3941.2, bsz=164, num_updates=84400, lr=0.00010885, gnorm=1.224, train_wall=16, wall=13422
2024-05-31 19:57:49 | INFO | train_inner | epoch 083:    778 / 1021 loss=5.737, nll_loss=4.506, ppl=22.72, wps=25089.4, ups=6.36, wpb=3944.8, bsz=160.2, num_updates=84500, lr=0.000108786, gnorm=1.198, train_wall=16, wall=13438
2024-05-31 19:58:04 | INFO | train_inner | epoch 083:    878 / 1021 loss=5.711, nll_loss=4.476, ppl=22.25, wps=25227, ups=6.4, wpb=3942.7, bsz=153.1, num_updates=84600, lr=0.000108721, gnorm=1.145, train_wall=15, wall=13454
2024-05-31 19:58:20 | INFO | train_inner | epoch 083:    978 / 1021 loss=5.725, nll_loss=4.492, ppl=22.5, wps=25093, ups=6.38, wpb=3930.4, bsz=151.1, num_updates=84700, lr=0.000108657, gnorm=1.18, train_wall=16, wall=13470
2024-05-31 19:58:27 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:58:27 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2024-05-31 19:58:27 | INFO | train | epoch 083 | loss 5.695 | nll_loss 4.458 | ppl 21.97 | wps 24928 | ups 6.32 | wpb 3943.1 | bsz 156.9 | num_updates 84743 | lr 0.00010863 | gnorm 1.168 | train_wall 159 | wall 13476
2024-05-31 19:58:27 | INFO | fairseq.trainer | begin training epoch 84
2024-05-31 19:58:36 | INFO | train_inner | epoch 084:     57 / 1021 loss=5.661, nll_loss=4.42, ppl=21.41, wps=24762, ups=6.32, wpb=3915.4, bsz=162.7, num_updates=84800, lr=0.000108593, gnorm=1.162, train_wall=15, wall=13485
2024-05-31 19:58:52 | INFO | train_inner | epoch 084:    157 / 1021 loss=5.653, nll_loss=4.409, ppl=21.24, wps=25014.3, ups=6.36, wpb=3930.6, bsz=150.7, num_updates=84900, lr=0.000108529, gnorm=1.155, train_wall=16, wall=13501
2024-05-31 19:59:07 | INFO | train_inner | epoch 084:    257 / 1021 loss=5.677, nll_loss=4.435, ppl=21.64, wps=25060.5, ups=6.37, wpb=3931.3, bsz=152.3, num_updates=85000, lr=0.000108465, gnorm=1.189, train_wall=16, wall=13517
2024-05-31 19:59:07 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 19:59:08 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_84_85000.pt (epoch 84 @ 85000 updates, score None) (writing took 1.008220613002777 seconds)
2024-05-31 19:59:24 | INFO | train_inner | epoch 084:    357 / 1021 loss=5.686, nll_loss=4.446, ppl=21.8, wps=24145.9, ups=6.11, wpb=3955.1, bsz=162.5, num_updates=85100, lr=0.000108401, gnorm=1.251, train_wall=15, wall=13533
2024-05-31 19:59:39 | INFO | train_inner | epoch 084:    457 / 1021 loss=5.699, nll_loss=4.462, ppl=22.04, wps=25187.9, ups=6.37, wpb=3956.1, bsz=161.8, num_updates=85200, lr=0.000108338, gnorm=1.156, train_wall=16, wall=13549
2024-05-31 19:59:55 | INFO | train_inner | epoch 084:    557 / 1021 loss=5.707, nll_loss=4.471, ppl=22.18, wps=25136.7, ups=6.36, wpb=3952.4, bsz=159.8, num_updates=85300, lr=0.000108274, gnorm=1.15, train_wall=16, wall=13565
2024-05-31 20:00:11 | INFO | train_inner | epoch 084:    657 / 1021 loss=5.69, nll_loss=4.451, ppl=21.86, wps=24970.5, ups=6.4, wpb=3904, bsz=144.6, num_updates=85400, lr=0.000108211, gnorm=1.165, train_wall=15, wall=13580
2024-05-31 20:00:27 | INFO | train_inner | epoch 084:    757 / 1021 loss=5.688, nll_loss=4.449, ppl=21.84, wps=24533.3, ups=6.2, wpb=3956.6, bsz=154.3, num_updates=85500, lr=0.000108148, gnorm=1.135, train_wall=16, wall=13596
2024-05-31 20:00:43 | INFO | train_inner | epoch 084:    857 / 1021 loss=5.734, nll_loss=4.501, ppl=22.64, wps=25257.4, ups=6.38, wpb=3956.8, bsz=155.2, num_updates=85600, lr=0.000108084, gnorm=1.198, train_wall=16, wall=13612
2024-05-31 20:00:58 | INFO | train_inner | epoch 084:    957 / 1021 loss=5.72, nll_loss=4.488, ppl=22.44, wps=25185.3, ups=6.34, wpb=3971.1, bsz=167.2, num_updates=85700, lr=0.000108021, gnorm=1.149, train_wall=16, wall=13628
2024-05-31 20:01:08 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:01:08 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2024-05-31 20:01:08 | INFO | train | epoch 084 | loss 5.692 | nll_loss 4.453 | ppl 21.91 | wps 24942.1 | ups 6.33 | wpb 3943.1 | bsz 156.9 | num_updates 85764 | lr 0.000107981 | gnorm 1.171 | train_wall 159 | wall 13638
2024-05-31 20:01:08 | INFO | fairseq.trainer | begin training epoch 85
2024-05-31 20:01:14 | INFO | train_inner | epoch 085:     36 / 1021 loss=5.68, nll_loss=4.44, ppl=21.71, wps=24908.8, ups=6.32, wpb=3943.9, bsz=150.8, num_updates=85800, lr=0.000107958, gnorm=1.146, train_wall=16, wall=13644
2024-05-31 20:01:30 | INFO | train_inner | epoch 085:    136 / 1021 loss=5.633, nll_loss=4.386, ppl=20.9, wps=24989.4, ups=6.36, wpb=3929, bsz=151.3, num_updates=85900, lr=0.000107896, gnorm=1.162, train_wall=16, wall=13659
2024-05-31 20:01:46 | INFO | train_inner | epoch 085:    236 / 1021 loss=5.655, nll_loss=4.412, ppl=21.29, wps=25266.4, ups=6.41, wpb=3942.4, bsz=160.3, num_updates=86000, lr=0.000107833, gnorm=1.167, train_wall=15, wall=13675
2024-05-31 20:01:46 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:01:46 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_85_86000.pt (epoch 85 @ 86000 updates, score None) (writing took 0.986960323061794 seconds)
2024-05-31 20:02:02 | INFO | train_inner | epoch 085:    336 / 1021 loss=5.674, nll_loss=4.433, ppl=21.6, wps=23741.2, ups=5.99, wpb=3961.9, bsz=167.4, num_updates=86100, lr=0.00010777, gnorm=1.169, train_wall=16, wall=13692
2024-05-31 20:02:18 | INFO | train_inner | epoch 085:    436 / 1021 loss=5.686, nll_loss=4.448, ppl=21.82, wps=25309.3, ups=6.4, wpb=3957.2, bsz=173.4, num_updates=86200, lr=0.000107708, gnorm=1.202, train_wall=15, wall=13707
2024-05-31 20:02:33 | INFO | train_inner | epoch 085:    536 / 1021 loss=5.696, nll_loss=4.457, ppl=21.97, wps=25148.7, ups=6.39, wpb=3935.6, bsz=148.8, num_updates=86300, lr=0.000107645, gnorm=1.156, train_wall=15, wall=13723
2024-05-31 20:02:49 | INFO | train_inner | epoch 085:    636 / 1021 loss=5.704, nll_loss=4.466, ppl=22.11, wps=25256.4, ups=6.38, wpb=3960.7, bsz=160.3, num_updates=86400, lr=0.000107583, gnorm=1.184, train_wall=16, wall=13739
2024-05-31 20:03:05 | INFO | train_inner | epoch 085:    736 / 1021 loss=5.711, nll_loss=4.473, ppl=22.2, wps=24934, ups=6.37, wpb=3914.3, bsz=134.2, num_updates=86500, lr=0.000107521, gnorm=1.157, train_wall=16, wall=13754
2024-05-31 20:03:21 | INFO | train_inner | epoch 085:    836 / 1021 loss=5.712, nll_loss=4.476, ppl=22.26, wps=25128.2, ups=6.38, wpb=3936.7, bsz=147.8, num_updates=86600, lr=0.000107459, gnorm=1.168, train_wall=16, wall=13770
2024-05-31 20:03:36 | INFO | train_inner | epoch 085:    936 / 1021 loss=5.727, nll_loss=4.496, ppl=22.57, wps=24933.3, ups=6.28, wpb=3970.7, bsz=181.9, num_updates=86700, lr=0.000107397, gnorm=1.247, train_wall=16, wall=13786
2024-05-31 20:03:50 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:03:50 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2024-05-31 20:03:50 | INFO | train | epoch 085 | loss 5.689 | nll_loss 4.45 | ppl 21.85 | wps 24909.3 | ups 6.32 | wpb 3943.1 | bsz 156.9 | num_updates 86785 | lr 0.000107344 | gnorm 1.176 | train_wall 159 | wall 13799
2024-05-31 20:03:50 | INFO | fairseq.trainer | begin training epoch 86
2024-05-31 20:03:52 | INFO | train_inner | epoch 086:     15 / 1021 loss=5.698, nll_loss=4.46, ppl=22.01, wps=24537.3, ups=6.25, wpb=3926.4, bsz=141.4, num_updates=86800, lr=0.000107335, gnorm=1.146, train_wall=16, wall=13802
2024-05-31 20:04:07 | INFO | train_inner | epoch 086:    115 / 1021 loss=5.662, nll_loss=4.42, ppl=21.4, wps=26204.5, ups=6.67, wpb=3928.6, bsz=167.1, num_updates=86900, lr=0.000107273, gnorm=1.185, train_wall=15, wall=13817
2024-05-31 20:04:23 | INFO | train_inner | epoch 086:    215 / 1021 loss=5.657, nll_loss=4.414, ppl=21.31, wps=25267.1, ups=6.37, wpb=3965.1, bsz=161, num_updates=87000, lr=0.000107211, gnorm=1.162, train_wall=16, wall=13833
2024-05-31 20:04:23 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:04:24 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_86_87000.pt (epoch 86 @ 87000 updates, score None) (writing took 0.997753247153014 seconds)
2024-05-31 20:04:39 | INFO | train_inner | epoch 086:    315 / 1021 loss=5.672, nll_loss=4.429, ppl=21.54, wps=24208.4, ups=6.19, wpb=3912.4, bsz=150.1, num_updates=87100, lr=0.00010715, gnorm=1.212, train_wall=15, wall=13849
2024-05-31 20:04:55 | INFO | train_inner | epoch 086:    415 / 1021 loss=5.667, nll_loss=4.423, ppl=21.46, wps=25059.5, ups=6.32, wpb=3962.7, bsz=149.1, num_updates=87200, lr=0.000107088, gnorm=1.127, train_wall=16, wall=13865
2024-05-31 20:05:11 | INFO | train_inner | epoch 086:    515 / 1021 loss=5.678, nll_loss=4.438, ppl=21.67, wps=24930.6, ups=6.32, wpb=3943.6, bsz=168.4, num_updates=87300, lr=0.000107027, gnorm=1.173, train_wall=16, wall=13880
2024-05-31 20:05:26 | INFO | train_inner | epoch 086:    615 / 1021 loss=5.683, nll_loss=4.441, ppl=21.73, wps=25692.4, ups=6.57, wpb=3911.9, bsz=136.3, num_updates=87400, lr=0.000106966, gnorm=1.167, train_wall=15, wall=13896
2024-05-31 20:05:42 | INFO | train_inner | epoch 086:    715 / 1021 loss=5.709, nll_loss=4.475, ppl=22.23, wps=25133.3, ups=6.39, wpb=3935.9, bsz=171.8, num_updates=87500, lr=0.000106904, gnorm=1.203, train_wall=16, wall=13911
2024-05-31 20:05:58 | INFO | train_inner | epoch 086:    815 / 1021 loss=5.706, nll_loss=4.467, ppl=22.12, wps=24834.3, ups=6.31, wpb=3936.9, bsz=132.6, num_updates=87600, lr=0.000106843, gnorm=1.147, train_wall=16, wall=13927
2024-05-31 20:06:14 | INFO | train_inner | epoch 086:    915 / 1021 loss=5.714, nll_loss=4.482, ppl=22.34, wps=24973.8, ups=6.28, wpb=3976.4, bsz=189.2, num_updates=87700, lr=0.000106783, gnorm=1.249, train_wall=16, wall=13943
2024-05-31 20:06:29 | INFO | train_inner | epoch 086:   1015 / 1021 loss=5.726, nll_loss=4.492, ppl=22.5, wps=24985.7, ups=6.33, wpb=3948.2, bsz=142.4, num_updates=87800, lr=0.000106722, gnorm=1.163, train_wall=16, wall=13959
2024-05-31 20:06:30 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:06:30 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2024-05-31 20:06:30 | INFO | train | epoch 086 | loss 5.686 | nll_loss 4.447 | ppl 21.81 | wps 25092 | ups 6.36 | wpb 3943.1 | bsz 156.9 | num_updates 87806 | lr 0.000106718 | gnorm 1.178 | train_wall 158 | wall 13960
2024-05-31 20:06:30 | INFO | fairseq.trainer | begin training epoch 87
2024-05-31 20:06:45 | INFO | train_inner | epoch 087:     94 / 1021 loss=5.643, nll_loss=4.398, ppl=21.08, wps=24652.4, ups=6.26, wpb=3935.1, bsz=160.6, num_updates=87900, lr=0.000106661, gnorm=1.196, train_wall=16, wall=13975
2024-05-31 20:07:01 | INFO | train_inner | epoch 087:    194 / 1021 loss=5.647, nll_loss=4.403, ppl=21.15, wps=25031.7, ups=6.33, wpb=3956.8, bsz=163, num_updates=88000, lr=0.0001066, gnorm=1.156, train_wall=16, wall=13991
2024-05-31 20:07:01 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:07:02 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_87_88000.pt (epoch 87 @ 88000 updates, score None) (writing took 1.0199004239402711 seconds)
2024-05-31 20:07:18 | INFO | train_inner | epoch 087:    294 / 1021 loss=5.661, nll_loss=4.416, ppl=21.35, wps=23569.4, ups=5.98, wpb=3940.1, bsz=140, num_updates=88100, lr=0.00010654, gnorm=1.17, train_wall=16, wall=14007
2024-05-31 20:07:34 | INFO | train_inner | epoch 087:    394 / 1021 loss=5.671, nll_loss=4.426, ppl=21.5, wps=24614.7, ups=6.3, wpb=3906.2, bsz=130.5, num_updates=88200, lr=0.000106479, gnorm=1.175, train_wall=16, wall=14023
2024-05-31 20:07:50 | INFO | train_inner | epoch 087:    494 / 1021 loss=5.679, nll_loss=4.438, ppl=21.67, wps=24830.4, ups=6.31, wpb=3936.4, bsz=154.6, num_updates=88300, lr=0.000106419, gnorm=1.18, train_wall=16, wall=14039
2024-05-31 20:08:05 | INFO | train_inner | epoch 087:    594 / 1021 loss=5.68, nll_loss=4.441, ppl=21.72, wps=24995.7, ups=6.32, wpb=3954, bsz=168.1, num_updates=88400, lr=0.000106359, gnorm=1.161, train_wall=16, wall=14055
2024-05-31 20:08:21 | INFO | train_inner | epoch 087:    694 / 1021 loss=5.699, nll_loss=4.461, ppl=22.03, wps=24931.6, ups=6.31, wpb=3953.9, bsz=154.6, num_updates=88500, lr=0.000106299, gnorm=1.188, train_wall=16, wall=14071
2024-05-31 20:08:37 | INFO | train_inner | epoch 087:    794 / 1021 loss=5.724, nll_loss=4.491, ppl=22.49, wps=25064.4, ups=6.34, wpb=3951, bsz=174.9, num_updates=88600, lr=0.000106239, gnorm=1.193, train_wall=16, wall=14086
2024-05-31 20:08:53 | INFO | train_inner | epoch 087:    894 / 1021 loss=5.698, nll_loss=4.461, ppl=22.03, wps=25018.9, ups=6.34, wpb=3947.3, bsz=169.8, num_updates=88700, lr=0.000106179, gnorm=1.16, train_wall=16, wall=14102
2024-05-31 20:09:09 | INFO | train_inner | epoch 087:    994 / 1021 loss=5.712, nll_loss=4.476, ppl=22.25, wps=25169.5, ups=6.37, wpb=3953.3, bsz=155.4, num_updates=88800, lr=0.000106119, gnorm=1.248, train_wall=16, wall=14118
2024-05-31 20:09:13 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:09:13 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2024-05-31 20:09:13 | INFO | train | epoch 087 | loss 5.683 | nll_loss 4.443 | ppl 21.74 | wps 24787 | ups 6.29 | wpb 3943.1 | bsz 156.9 | num_updates 88827 | lr 0.000106103 | gnorm 1.183 | train_wall 160 | wall 14122
2024-05-31 20:09:13 | INFO | fairseq.trainer | begin training epoch 88
2024-05-31 20:09:25 | INFO | train_inner | epoch 088:     73 / 1021 loss=5.666, nll_loss=4.425, ppl=21.49, wps=24815.8, ups=6.25, wpb=3969.6, bsz=172.3, num_updates=88900, lr=0.000106059, gnorm=1.192, train_wall=16, wall=14134
2024-05-31 20:09:40 | INFO | train_inner | epoch 088:    173 / 1021 loss=5.65, nll_loss=4.403, ppl=21.16, wps=25077.8, ups=6.38, wpb=3932.6, bsz=155, num_updates=89000, lr=0.000106, gnorm=1.186, train_wall=16, wall=14150
2024-05-31 20:09:40 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:09:41 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_88_89000.pt (epoch 88 @ 89000 updates, score None) (writing took 1.0110090970993042 seconds)
2024-05-31 20:09:57 | INFO | train_inner | epoch 088:    273 / 1021 loss=5.645, nll_loss=4.398, ppl=21.08, wps=23577.8, ups=6.02, wpb=3918, bsz=144.6, num_updates=89100, lr=0.00010594, gnorm=1.189, train_wall=15, wall=14166
2024-05-31 20:10:13 | INFO | train_inner | epoch 088:    373 / 1021 loss=5.672, nll_loss=4.431, ppl=21.57, wps=25073.7, ups=6.33, wpb=3963.5, bsz=180.2, num_updates=89200, lr=0.000105881, gnorm=1.254, train_wall=16, wall=14182
2024-05-31 20:10:28 | INFO | train_inner | epoch 088:    473 / 1021 loss=5.667, nll_loss=4.423, ppl=21.45, wps=25127.3, ups=6.37, wpb=3942.5, bsz=144.4, num_updates=89300, lr=0.000105822, gnorm=1.158, train_wall=16, wall=14198
2024-05-31 20:10:44 | INFO | train_inner | epoch 088:    573 / 1021 loss=5.689, nll_loss=4.451, ppl=21.87, wps=25108.5, ups=6.37, wpb=3942.2, bsz=165.2, num_updates=89400, lr=0.000105762, gnorm=1.177, train_wall=16, wall=14213
2024-05-31 20:11:00 | INFO | train_inner | epoch 088:    673 / 1021 loss=5.704, nll_loss=4.467, ppl=22.11, wps=24769, ups=6.28, wpb=3943, bsz=155.1, num_updates=89500, lr=0.000105703, gnorm=1.2, train_wall=16, wall=14229
2024-05-31 20:11:15 | INFO | train_inner | epoch 088:    773 / 1021 loss=5.697, nll_loss=4.459, ppl=22, wps=26420, ups=6.69, wpb=3947.5, bsz=156.2, num_updates=89600, lr=0.000105644, gnorm=1.15, train_wall=15, wall=14244
2024-05-31 20:11:30 | INFO | train_inner | epoch 088:    873 / 1021 loss=5.717, nll_loss=4.48, ppl=22.31, wps=25750.1, ups=6.6, wpb=3899.6, bsz=138.7, num_updates=89700, lr=0.000105585, gnorm=1.185, train_wall=15, wall=14259
2024-05-31 20:11:46 | INFO | train_inner | epoch 088:    973 / 1021 loss=5.715, nll_loss=4.481, ppl=22.33, wps=25182.3, ups=6.35, wpb=3966.5, bsz=162, num_updates=89800, lr=0.000105527, gnorm=1.153, train_wall=16, wall=14275
2024-05-31 20:11:53 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:11:53 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2024-05-31 20:11:53 | INFO | train | epoch 088 | loss 5.681 | nll_loss 4.44 | ppl 21.71 | wps 25072 | ups 6.36 | wpb 3943.1 | bsz 156.9 | num_updates 89848 | lr 0.000105498 | gnorm 1.182 | train_wall 158 | wall 14283
2024-05-31 20:11:53 | INFO | fairseq.trainer | begin training epoch 89
2024-05-31 20:12:02 | INFO | train_inner | epoch 089:     52 / 1021 loss=5.664, nll_loss=4.421, ppl=21.42, wps=24796.6, ups=6.3, wpb=3938.4, bsz=149.1, num_updates=89900, lr=0.000105468, gnorm=1.212, train_wall=16, wall=14291
2024-05-31 20:12:18 | INFO | train_inner | epoch 089:    152 / 1021 loss=5.639, nll_loss=4.392, ppl=21, wps=24982, ups=6.3, wpb=3968.1, bsz=161, num_updates=90000, lr=0.000105409, gnorm=1.216, train_wall=16, wall=14307
2024-05-31 20:12:18 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:12:19 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_89_90000.pt (epoch 89 @ 90000 updates, score None) (writing took 0.9866088749840856 seconds)
2024-05-31 20:12:34 | INFO | train_inner | epoch 089:    252 / 1021 loss=5.66, nll_loss=4.416, ppl=21.35, wps=23912, ups=6.08, wpb=3935.6, bsz=164.6, num_updates=90100, lr=0.000105351, gnorm=1.198, train_wall=15, wall=14323
2024-05-31 20:12:50 | INFO | train_inner | epoch 089:    352 / 1021 loss=5.653, nll_loss=4.406, ppl=21.2, wps=25031, ups=6.39, wpb=3919.5, bsz=143, num_updates=90200, lr=0.000105292, gnorm=1.162, train_wall=16, wall=14339
2024-05-31 20:13:05 | INFO | train_inner | epoch 089:    452 / 1021 loss=5.684, nll_loss=4.443, ppl=21.75, wps=25397.9, ups=6.43, wpb=3951.1, bsz=152.8, num_updates=90300, lr=0.000105234, gnorm=1.202, train_wall=15, wall=14355
2024-05-31 20:13:21 | INFO | train_inner | epoch 089:    552 / 1021 loss=5.666, nll_loss=4.424, ppl=21.47, wps=25274.4, ups=6.37, wpb=3968, bsz=169.1, num_updates=90400, lr=0.000105176, gnorm=1.168, train_wall=16, wall=14370
2024-05-31 20:13:37 | INFO | train_inner | epoch 089:    652 / 1021 loss=5.696, nll_loss=4.458, ppl=21.97, wps=24907.8, ups=6.37, wpb=3910.6, bsz=161.5, num_updates=90500, lr=0.000105118, gnorm=1.227, train_wall=16, wall=14386
2024-05-31 20:13:52 | INFO | train_inner | epoch 089:    752 / 1021 loss=5.692, nll_loss=4.453, ppl=21.89, wps=25121.9, ups=6.37, wpb=3946.3, bsz=146.9, num_updates=90600, lr=0.00010506, gnorm=1.17, train_wall=16, wall=14402
2024-05-31 20:14:08 | INFO | train_inner | epoch 089:    852 / 1021 loss=5.701, nll_loss=4.464, ppl=22.06, wps=25001.9, ups=6.35, wpb=3940.3, bsz=154.2, num_updates=90700, lr=0.000105002, gnorm=1.186, train_wall=16, wall=14417
2024-05-31 20:14:24 | INFO | train_inner | epoch 089:    952 / 1021 loss=5.699, nll_loss=4.462, ppl=22.04, wps=25079.4, ups=6.35, wpb=3948.4, bsz=162.6, num_updates=90800, lr=0.000104944, gnorm=1.184, train_wall=16, wall=14433
2024-05-31 20:14:35 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:14:35 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2024-05-31 20:14:35 | INFO | train | epoch 089 | loss 5.679 | nll_loss 4.438 | ppl 21.67 | wps 24934.6 | ups 6.32 | wpb 3943.1 | bsz 156.9 | num_updates 90869 | lr 0.000104904 | gnorm 1.196 | train_wall 159 | wall 14444
2024-05-31 20:14:35 | INFO | fairseq.trainer | begin training epoch 90
2024-05-31 20:14:40 | INFO | train_inner | epoch 090:     31 / 1021 loss=5.702, nll_loss=4.465, ppl=22.08, wps=24608.9, ups=6.22, wpb=3959.3, bsz=165, num_updates=90900, lr=0.000104886, gnorm=1.2, train_wall=16, wall=14449
2024-05-31 20:14:56 | INFO | train_inner | epoch 090:    131 / 1021 loss=5.616, nll_loss=4.366, ppl=20.62, wps=25043.1, ups=6.4, wpb=3910.7, bsz=150.1, num_updates=91000, lr=0.000104828, gnorm=1.174, train_wall=15, wall=14465
2024-05-31 20:14:56 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:14:57 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_90_91000.pt (epoch 90 @ 91000 updates, score None) (writing took 0.9953294168226421 seconds)
2024-05-31 20:15:12 | INFO | train_inner | epoch 090:    231 / 1021 loss=5.654, nll_loss=4.407, ppl=21.22, wps=23730.9, ups=5.99, wpb=3963.7, bsz=149.6, num_updates=91100, lr=0.000104771, gnorm=1.171, train_wall=16, wall=14482
2024-05-31 20:15:28 | INFO | train_inner | epoch 090:    331 / 1021 loss=5.666, nll_loss=4.423, ppl=21.45, wps=25102.3, ups=6.38, wpb=3935.4, bsz=164.4, num_updates=91200, lr=0.000104713, gnorm=1.218, train_wall=16, wall=14497
2024-05-31 20:15:44 | INFO | train_inner | epoch 090:    431 / 1021 loss=5.682, nll_loss=4.441, ppl=21.72, wps=25194.7, ups=6.4, wpb=3939.7, bsz=161.4, num_updates=91300, lr=0.000104656, gnorm=1.208, train_wall=15, wall=14513
2024-05-31 20:15:59 | INFO | train_inner | epoch 090:    531 / 1021 loss=5.677, nll_loss=4.436, ppl=21.64, wps=25136.3, ups=6.38, wpb=3940.2, bsz=154.9, num_updates=91400, lr=0.000104599, gnorm=1.175, train_wall=16, wall=14529
2024-05-31 20:16:15 | INFO | train_inner | epoch 090:    631 / 1021 loss=5.676, nll_loss=4.435, ppl=21.63, wps=24947.1, ups=6.31, wpb=3955.8, bsz=161.8, num_updates=91500, lr=0.000104542, gnorm=1.173, train_wall=16, wall=14544
2024-05-31 20:16:31 | INFO | train_inner | epoch 090:    731 / 1021 loss=5.684, nll_loss=4.443, ppl=21.75, wps=24892.7, ups=6.32, wpb=3940.5, bsz=145, num_updates=91600, lr=0.000104485, gnorm=1.173, train_wall=16, wall=14560
2024-05-31 20:16:47 | INFO | train_inner | epoch 090:    831 / 1021 loss=5.683, nll_loss=4.441, ppl=21.72, wps=25054.9, ups=6.4, wpb=3914.9, bsz=139.8, num_updates=91700, lr=0.000104428, gnorm=1.172, train_wall=15, wall=14576
2024-05-31 20:17:02 | INFO | train_inner | epoch 090:    931 / 1021 loss=5.705, nll_loss=4.47, ppl=22.16, wps=25094.5, ups=6.32, wpb=3971.8, bsz=175.6, num_updates=91800, lr=0.000104371, gnorm=1.228, train_wall=16, wall=14592
2024-05-31 20:17:17 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:17:17 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2024-05-31 20:17:17 | INFO | train | epoch 090 | loss 5.674 | nll_loss 4.432 | ppl 21.59 | wps 24872.6 | ups 6.31 | wpb 3943.1 | bsz 156.9 | num_updates 91890 | lr 0.00010432 | gnorm 1.193 | train_wall 159 | wall 14606
2024-05-31 20:17:17 | INFO | fairseq.trainer | begin training epoch 91
2024-05-31 20:17:18 | INFO | train_inner | epoch 091:     10 / 1021 loss=5.704, nll_loss=4.467, ppl=22.11, wps=24654.4, ups=6.25, wpb=3941.8, bsz=160.6, num_updates=91900, lr=0.000104314, gnorm=1.244, train_wall=16, wall=14608
2024-05-31 20:17:34 | INFO | train_inner | epoch 091:    110 / 1021 loss=5.62, nll_loss=4.371, ppl=20.69, wps=25168.1, ups=6.35, wpb=3963.6, bsz=164.9, num_updates=92000, lr=0.000104257, gnorm=1.155, train_wall=16, wall=14624
2024-05-31 20:17:34 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:17:35 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_91_92000.pt (epoch 91 @ 92000 updates, score None) (writing took 0.9820950520224869 seconds)
2024-05-31 20:17:51 | INFO | train_inner | epoch 091:    210 / 1021 loss=5.649, nll_loss=4.404, ppl=21.17, wps=23675.3, ups=5.98, wpb=3960.6, bsz=163, num_updates=92100, lr=0.000104201, gnorm=1.186, train_wall=16, wall=14640
2024-05-31 20:18:07 | INFO | train_inner | epoch 091:    310 / 1021 loss=5.643, nll_loss=4.397, ppl=21.07, wps=24843.5, ups=6.31, wpb=3938.3, bsz=165.9, num_updates=92200, lr=0.000104144, gnorm=1.199, train_wall=16, wall=14656
2024-05-31 20:18:22 | INFO | train_inner | epoch 091:    410 / 1021 loss=5.673, nll_loss=4.43, ppl=21.56, wps=25038.7, ups=6.34, wpb=3950.4, bsz=159.3, num_updates=92300, lr=0.000104088, gnorm=1.208, train_wall=16, wall=14672
2024-05-31 20:18:38 | INFO | train_inner | epoch 091:    510 / 1021 loss=5.677, nll_loss=4.436, ppl=21.65, wps=25166.2, ups=6.35, wpb=3963.8, bsz=165, num_updates=92400, lr=0.000104031, gnorm=1.169, train_wall=16, wall=14688
2024-05-31 20:18:54 | INFO | train_inner | epoch 091:    610 / 1021 loss=5.684, nll_loss=4.442, ppl=21.73, wps=25059.5, ups=6.41, wpb=3907.1, bsz=144.2, num_updates=92500, lr=0.000103975, gnorm=1.269, train_wall=15, wall=14703
2024-05-31 20:19:10 | INFO | train_inner | epoch 091:    710 / 1021 loss=5.681, nll_loss=4.439, ppl=21.7, wps=25099.4, ups=6.35, wpb=3955.7, bsz=150.5, num_updates=92600, lr=0.000103919, gnorm=1.191, train_wall=16, wall=14719
2024-05-31 20:19:25 | INFO | train_inner | epoch 091:    810 / 1021 loss=5.696, nll_loss=4.455, ppl=21.93, wps=25146.6, ups=6.43, wpb=3909.2, bsz=145.3, num_updates=92700, lr=0.000103863, gnorm=1.202, train_wall=15, wall=14735
2024-05-31 20:19:41 | INFO | train_inner | epoch 091:    910 / 1021 loss=5.709, nll_loss=4.472, ppl=22.19, wps=24955.3, ups=6.33, wpb=3941.9, bsz=162.9, num_updates=92800, lr=0.000103807, gnorm=1.204, train_wall=16, wall=14750
2024-05-31 20:19:57 | INFO | train_inner | epoch 091:   1010 / 1021 loss=5.702, nll_loss=4.464, ppl=22.08, wps=25101, ups=6.34, wpb=3959.8, bsz=146.1, num_updates=92900, lr=0.000103751, gnorm=1.161, train_wall=16, wall=14766
2024-05-31 20:19:58 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:19:58 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2024-05-31 20:19:58 | INFO | train | epoch 091 | loss 5.673 | nll_loss 4.431 | ppl 21.57 | wps 24892.4 | ups 6.31 | wpb 3943.1 | bsz 156.9 | num_updates 92911 | lr 0.000103745 | gnorm 1.196 | train_wall 159 | wall 14768
2024-05-31 20:19:58 | INFO | fairseq.trainer | begin training epoch 92
2024-05-31 20:20:13 | INFO | train_inner | epoch 092:     89 / 1021 loss=5.64, nll_loss=4.394, ppl=21.02, wps=24664.5, ups=6.25, wpb=3945.2, bsz=159.3, num_updates=93000, lr=0.000103695, gnorm=1.219, train_wall=16, wall=14782
2024-05-31 20:20:13 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:20:14 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_92_93000.pt (epoch 92 @ 93000 updates, score None) (writing took 1.0009434036910534 seconds)
2024-05-31 20:20:29 | INFO | train_inner | epoch 092:    189 / 1021 loss=5.639, nll_loss=4.392, ppl=20.99, wps=23581.2, ups=5.99, wpb=3936.5, bsz=156, num_updates=93100, lr=0.000103639, gnorm=1.183, train_wall=16, wall=14799
2024-05-31 20:20:45 | INFO | train_inner | epoch 092:    289 / 1021 loss=5.652, nll_loss=4.405, ppl=21.18, wps=25033.1, ups=6.39, wpb=3919, bsz=147.7, num_updates=93200, lr=0.000103584, gnorm=1.213, train_wall=16, wall=14814
2024-05-31 20:21:01 | INFO | train_inner | epoch 092:    389 / 1021 loss=5.657, nll_loss=4.412, ppl=21.28, wps=25242.3, ups=6.38, wpb=3957.7, bsz=153.5, num_updates=93300, lr=0.000103528, gnorm=1.167, train_wall=16, wall=14830
2024-05-31 20:21:16 | INFO | train_inner | epoch 092:    489 / 1021 loss=5.657, nll_loss=4.414, ppl=21.32, wps=25215.1, ups=6.38, wpb=3949.6, bsz=174.2, num_updates=93400, lr=0.000103473, gnorm=1.212, train_wall=16, wall=14846
2024-05-31 20:21:32 | INFO | train_inner | epoch 092:    589 / 1021 loss=5.676, nll_loss=4.433, ppl=21.6, wps=24898.1, ups=6.34, wpb=3929.6, bsz=143.3, num_updates=93500, lr=0.000103418, gnorm=1.192, train_wall=16, wall=14862
2024-05-31 20:21:48 | INFO | train_inner | epoch 092:    689 / 1021 loss=5.692, nll_loss=4.453, ppl=21.9, wps=24980.7, ups=6.31, wpb=3959.7, bsz=161.1, num_updates=93600, lr=0.000103362, gnorm=1.2, train_wall=16, wall=14877
2024-05-31 20:22:04 | INFO | train_inner | epoch 092:    789 / 1021 loss=5.671, nll_loss=4.428, ppl=21.52, wps=25145.5, ups=6.4, wpb=3927.2, bsz=156.3, num_updates=93700, lr=0.000103307, gnorm=1.203, train_wall=15, wall=14893
2024-05-31 20:22:19 | INFO | train_inner | epoch 092:    889 / 1021 loss=5.703, nll_loss=4.466, ppl=22.1, wps=25197.2, ups=6.4, wpb=3940, bsz=165.4, num_updates=93800, lr=0.000103252, gnorm=1.266, train_wall=15, wall=14909
2024-05-31 20:22:35 | INFO | train_inner | epoch 092:    989 / 1021 loss=5.707, nll_loss=4.47, ppl=22.17, wps=25137.1, ups=6.36, wpb=3950.1, bsz=159.5, num_updates=93900, lr=0.000103197, gnorm=1.185, train_wall=16, wall=14924
2024-05-31 20:22:40 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:22:40 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2024-05-31 20:22:40 | INFO | train | epoch 092 | loss 5.67 | nll_loss 4.427 | ppl 21.51 | wps 24904.1 | ups 6.32 | wpb 3943.1 | bsz 156.9 | num_updates 93932 | lr 0.000103179 | gnorm 1.2 | train_wall 159 | wall 14929
2024-05-31 20:22:40 | INFO | fairseq.trainer | begin training epoch 93
2024-05-31 20:22:51 | INFO | train_inner | epoch 093:     68 / 1021 loss=5.644, nll_loss=4.397, ppl=21.07, wps=24835.8, ups=6.3, wpb=3943.7, bsz=148, num_updates=94000, lr=0.000103142, gnorm=1.179, train_wall=16, wall=14940
2024-05-31 20:22:51 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:22:52 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_93_94000.pt (epoch 93 @ 94000 updates, score None) (writing took 0.9871540740132332 seconds)
2024-05-31 20:23:07 | INFO | train_inner | epoch 093:    168 / 1021 loss=5.638, nll_loss=4.39, ppl=20.96, wps=24039.7, ups=6.11, wpb=3934.8, bsz=157.1, num_updates=94100, lr=0.000103087, gnorm=1.211, train_wall=15, wall=14957
2024-05-31 20:23:23 | INFO | train_inner | epoch 093:    268 / 1021 loss=5.649, nll_loss=4.403, ppl=21.16, wps=25116.9, ups=6.37, wpb=3944.7, bsz=168.2, num_updates=94200, lr=0.000103033, gnorm=1.2, train_wall=16, wall=14972
2024-05-31 20:23:39 | INFO | train_inner | epoch 093:    368 / 1021 loss=5.633, nll_loss=4.385, ppl=20.89, wps=25126.6, ups=6.38, wpb=3940.1, bsz=160.3, num_updates=94300, lr=0.000102978, gnorm=1.172, train_wall=16, wall=14988
2024-05-31 20:23:54 | INFO | train_inner | epoch 093:    468 / 1021 loss=5.678, nll_loss=4.435, ppl=21.63, wps=25043.4, ups=6.39, wpb=3917, bsz=152.6, num_updates=94400, lr=0.000102923, gnorm=1.252, train_wall=15, wall=15004
2024-05-31 20:24:10 | INFO | train_inner | epoch 093:    568 / 1021 loss=5.652, nll_loss=4.406, ppl=21.2, wps=24966.4, ups=6.31, wpb=3954.1, bsz=151.4, num_updates=94500, lr=0.000102869, gnorm=1.154, train_wall=16, wall=15020
2024-05-31 20:24:26 | INFO | train_inner | epoch 093:    668 / 1021 loss=5.676, nll_loss=4.434, ppl=21.62, wps=25202.1, ups=6.38, wpb=3951.7, bsz=160.6, num_updates=94600, lr=0.000102815, gnorm=1.205, train_wall=16, wall=15035
2024-05-31 20:24:42 | INFO | train_inner | epoch 093:    768 / 1021 loss=5.703, nll_loss=4.465, ppl=22.09, wps=25195, ups=6.36, wpb=3959.5, bsz=165, num_updates=94700, lr=0.00010276, gnorm=1.269, train_wall=16, wall=15051
2024-05-31 20:24:57 | INFO | train_inner | epoch 093:    868 / 1021 loss=5.708, nll_loss=4.47, ppl=22.17, wps=25337.2, ups=6.38, wpb=3970.8, bsz=160.4, num_updates=94800, lr=0.000102706, gnorm=1.208, train_wall=16, wall=15067
2024-05-31 20:25:13 | INFO | train_inner | epoch 093:    968 / 1021 loss=5.696, nll_loss=4.457, ppl=21.97, wps=24843.3, ups=6.32, wpb=3929.2, bsz=148.5, num_updates=94900, lr=0.000102652, gnorm=1.214, train_wall=16, wall=15082
2024-05-31 20:25:21 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:25:21 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2024-05-31 20:25:21 | INFO | train | epoch 093 | loss 5.668 | nll_loss 4.424 | ppl 21.47 | wps 24967.7 | ups 6.33 | wpb 3943.1 | bsz 156.9 | num_updates 94953 | lr 0.000102623 | gnorm 1.206 | train_wall 158 | wall 15091
2024-05-31 20:25:21 | INFO | fairseq.trainer | begin training epoch 94
2024-05-31 20:25:29 | INFO | train_inner | epoch 094:     47 / 1021 loss=5.661, nll_loss=4.416, ppl=21.35, wps=24874, ups=6.35, wpb=3917.5, bsz=137.1, num_updates=95000, lr=0.000102598, gnorm=1.183, train_wall=15, wall=15098
2024-05-31 20:25:29 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:25:30 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_94_95000.pt (epoch 94 @ 95000 updates, score None) (writing took 0.9857124872505665 seconds)
2024-05-31 20:25:45 | INFO | train_inner | epoch 094:    147 / 1021 loss=5.621, nll_loss=4.371, ppl=20.69, wps=23960.4, ups=6.1, wpb=3927.9, bsz=155, num_updates=95100, lr=0.000102544, gnorm=1.227, train_wall=15, wall=15115
2024-05-31 20:26:01 | INFO | train_inner | epoch 094:    247 / 1021 loss=5.616, nll_loss=4.363, ppl=20.58, wps=24958.7, ups=6.41, wpb=3896.1, bsz=144.7, num_updates=95200, lr=0.00010249, gnorm=1.254, train_wall=15, wall=15130
2024-05-31 20:26:17 | INFO | train_inner | epoch 094:    347 / 1021 loss=5.653, nll_loss=4.408, ppl=21.22, wps=25119.4, ups=6.33, wpb=3966.1, bsz=160, num_updates=95300, lr=0.000102436, gnorm=1.202, train_wall=16, wall=15146
2024-05-31 20:26:32 | INFO | train_inner | epoch 094:    447 / 1021 loss=5.666, nll_loss=4.422, ppl=21.43, wps=25050.5, ups=6.34, wpb=3948.8, bsz=155, num_updates=95400, lr=0.000102383, gnorm=1.199, train_wall=16, wall=15162
2024-05-31 20:26:48 | INFO | train_inner | epoch 094:    547 / 1021 loss=5.671, nll_loss=4.429, ppl=21.53, wps=24899.8, ups=6.29, wpb=3958, bsz=165.1, num_updates=95500, lr=0.000102329, gnorm=1.204, train_wall=16, wall=15178
2024-05-31 20:27:04 | INFO | train_inner | epoch 094:    647 / 1021 loss=5.677, nll_loss=4.436, ppl=21.64, wps=25159.6, ups=6.33, wpb=3973.7, bsz=161, num_updates=95600, lr=0.000102275, gnorm=1.188, train_wall=16, wall=15193
2024-05-31 20:27:20 | INFO | train_inner | epoch 094:    747 / 1021 loss=5.673, nll_loss=4.432, ppl=21.58, wps=25106.4, ups=6.38, wpb=3937.2, bsz=166.6, num_updates=95700, lr=0.000102222, gnorm=1.197, train_wall=16, wall=15209
2024-05-31 20:27:35 | INFO | train_inner | epoch 094:    847 / 1021 loss=5.668, nll_loss=4.427, ppl=21.51, wps=25133.4, ups=6.36, wpb=3954.4, bsz=169.3, num_updates=95800, lr=0.000102169, gnorm=1.181, train_wall=16, wall=15225
2024-05-31 20:27:51 | INFO | train_inner | epoch 094:    947 / 1021 loss=5.716, nll_loss=4.479, ppl=22.3, wps=25150.9, ups=6.37, wpb=3950.2, bsz=151.7, num_updates=95900, lr=0.000102115, gnorm=1.219, train_wall=16, wall=15241
2024-05-31 20:28:03 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:28:03 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2024-05-31 20:28:03 | INFO | train | epoch 094 | loss 5.664 | nll_loss 4.42 | ppl 21.41 | wps 24944 | ups 6.33 | wpb 3943.1 | bsz 156.9 | num_updates 95974 | lr 0.000102076 | gnorm 1.21 | train_wall 159 | wall 15252
2024-05-31 20:28:03 | INFO | fairseq.trainer | begin training epoch 95
2024-05-31 20:28:07 | INFO | train_inner | epoch 095:     26 / 1021 loss=5.684, nll_loss=4.443, ppl=21.75, wps=24877, ups=6.33, wpb=3931.7, bsz=148.4, num_updates=96000, lr=0.000102062, gnorm=1.227, train_wall=15, wall=15256
2024-05-31 20:28:07 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:28:08 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_95_96000.pt (epoch 95 @ 96000 updates, score None) (writing took 0.9928142740391195 seconds)
2024-05-31 20:28:23 | INFO | train_inner | epoch 095:    126 / 1021 loss=5.641, nll_loss=4.394, ppl=21.02, wps=24137.8, ups=6.13, wpb=3940.2, bsz=165.8, num_updates=96100, lr=0.000102009, gnorm=1.276, train_wall=15, wall=15273
2024-05-31 20:28:39 | INFO | train_inner | epoch 095:    226 / 1021 loss=5.644, nll_loss=4.397, ppl=21.06, wps=24947.6, ups=6.31, wpb=3952.6, bsz=167.9, num_updates=96200, lr=0.000101956, gnorm=1.207, train_wall=16, wall=15288
2024-05-31 20:28:55 | INFO | train_inner | epoch 095:    326 / 1021 loss=5.639, nll_loss=4.39, ppl=20.97, wps=25037.8, ups=6.36, wpb=3936.2, bsz=153.1, num_updates=96300, lr=0.000101903, gnorm=1.211, train_wall=16, wall=15304
2024-05-31 20:29:10 | INFO | train_inner | epoch 095:    426 / 1021 loss=5.659, nll_loss=4.414, ppl=21.32, wps=25637.6, ups=6.52, wpb=3929.4, bsz=162.1, num_updates=96400, lr=0.00010185, gnorm=1.225, train_wall=15, wall=15320
2024-05-31 20:29:26 | INFO | train_inner | epoch 095:    526 / 1021 loss=5.651, nll_loss=4.405, ppl=21.18, wps=25384.1, ups=6.43, wpb=3948.7, bsz=154.1, num_updates=96500, lr=0.000101797, gnorm=1.19, train_wall=15, wall=15335
2024-05-31 20:29:41 | INFO | train_inner | epoch 095:    626 / 1021 loss=5.668, nll_loss=4.424, ppl=21.46, wps=25104.7, ups=6.37, wpb=3938.8, bsz=150.6, num_updates=96600, lr=0.000101745, gnorm=1.178, train_wall=16, wall=15351
2024-05-31 20:29:57 | INFO | train_inner | epoch 095:    726 / 1021 loss=5.677, nll_loss=4.433, ppl=21.6, wps=25156.4, ups=6.39, wpb=3939.1, bsz=135.3, num_updates=96700, lr=0.000101692, gnorm=1.206, train_wall=16, wall=15366
2024-05-31 20:30:13 | INFO | train_inner | epoch 095:    826 / 1021 loss=5.696, nll_loss=4.457, ppl=21.96, wps=25085.5, ups=6.32, wpb=3967.2, bsz=158.6, num_updates=96800, lr=0.000101639, gnorm=1.191, train_wall=16, wall=15382
2024-05-31 20:30:29 | INFO | train_inner | epoch 095:    926 / 1021 loss=5.688, nll_loss=4.449, ppl=21.84, wps=25081.5, ups=6.38, wpb=3928.4, bsz=176.2, num_updates=96900, lr=0.000101587, gnorm=1.242, train_wall=16, wall=15398
2024-05-31 20:30:43 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:30:43 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2024-05-31 20:30:43 | INFO | train | epoch 095 | loss 5.663 | nll_loss 4.419 | ppl 21.39 | wps 25046.1 | ups 6.35 | wpb 3943.1 | bsz 156.9 | num_updates 96995 | lr 0.000101537 | gnorm 1.21 | train_wall 158 | wall 15413
2024-05-31 20:30:44 | INFO | fairseq.trainer | begin training epoch 96
2024-05-31 20:30:44 | INFO | train_inner | epoch 096:      5 / 1021 loss=5.683, nll_loss=4.443, ppl=21.75, wps=24883.7, ups=6.29, wpb=3957.2, bsz=154.1, num_updates=97000, lr=0.000101535, gnorm=1.183, train_wall=16, wall=15414
2024-05-31 20:30:44 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:30:45 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_96_97000.pt (epoch 96 @ 97000 updates, score None) (writing took 0.9940060130320489 seconds)
2024-05-31 20:31:00 | INFO | train_inner | epoch 096:    105 / 1021 loss=5.628, nll_loss=4.379, ppl=20.81, wps=24892.3, ups=6.28, wpb=3964.6, bsz=170.7, num_updates=97100, lr=0.000101482, gnorm=1.209, train_wall=15, wall=15430
2024-05-31 20:31:16 | INFO | train_inner | epoch 096:    205 / 1021 loss=5.636, nll_loss=4.391, ppl=20.97, wps=25227.6, ups=6.34, wpb=3977.7, bsz=180.2, num_updates=97200, lr=0.00010143, gnorm=1.217, train_wall=16, wall=15446
2024-05-31 20:31:32 | INFO | train_inner | epoch 096:    305 / 1021 loss=5.655, nll_loss=4.408, ppl=21.23, wps=25099.7, ups=6.41, wpb=3918.3, bsz=164.4, num_updates=97300, lr=0.000101378, gnorm=1.249, train_wall=15, wall=15461
2024-05-31 20:31:47 | INFO | train_inner | epoch 096:    405 / 1021 loss=5.633, nll_loss=4.384, ppl=20.88, wps=25016.3, ups=6.41, wpb=3904.6, bsz=149.8, num_updates=97400, lr=0.000101326, gnorm=1.258, train_wall=15, wall=15477
2024-05-31 20:32:03 | INFO | train_inner | epoch 096:    505 / 1021 loss=5.646, nll_loss=4.399, ppl=21.1, wps=25163.2, ups=6.37, wpb=3951.9, bsz=161.1, num_updates=97500, lr=0.000101274, gnorm=1.186, train_wall=16, wall=15492
2024-05-31 20:32:19 | INFO | train_inner | epoch 096:    605 / 1021 loss=5.675, nll_loss=4.431, ppl=21.57, wps=24865.6, ups=6.33, wpb=3927.3, bsz=145.5, num_updates=97600, lr=0.000101222, gnorm=1.222, train_wall=16, wall=15508
2024-05-31 20:32:35 | INFO | train_inner | epoch 096:    705 / 1021 loss=5.664, nll_loss=4.42, ppl=21.4, wps=24779.2, ups=6.26, wpb=3955.6, bsz=155.4, num_updates=97700, lr=0.00010117, gnorm=1.165, train_wall=16, wall=15524
2024-05-31 20:32:51 | INFO | train_inner | epoch 096:    805 / 1021 loss=5.666, nll_loss=4.421, ppl=21.42, wps=24768.1, ups=6.26, wpb=3956.1, bsz=137.9, num_updates=97800, lr=0.000101118, gnorm=1.181, train_wall=16, wall=15540
2024-05-31 20:33:07 | INFO | train_inner | epoch 096:    905 / 1021 loss=5.678, nll_loss=4.435, ppl=21.63, wps=24732, ups=6.3, wpb=3927.8, bsz=144.8, num_updates=97900, lr=0.000101067, gnorm=1.21, train_wall=16, wall=15556
2024-05-31 20:33:23 | INFO | train_inner | epoch 096:   1005 / 1021 loss=5.708, nll_loss=4.471, ppl=22.17, wps=24866.8, ups=6.31, wpb=3941.3, bsz=157.5, num_updates=98000, lr=0.000101015, gnorm=1.228, train_wall=16, wall=15572
2024-05-31 20:33:23 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:33:24 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_96_98000.pt (epoch 96 @ 98000 updates, score None) (writing took 0.9828018620610237 seconds)
2024-05-31 20:33:26 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:33:26 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2024-05-31 20:33:26 | INFO | train | epoch 096 | loss 5.659 | nll_loss 4.415 | ppl 21.33 | wps 24782.9 | ups 6.29 | wpb 3943.1 | bsz 156.9 | num_updates 98016 | lr 0.000101007 | gnorm 1.212 | train_wall 159 | wall 15575
2024-05-31 20:33:26 | INFO | fairseq.trainer | begin training epoch 97
2024-05-31 20:33:39 | INFO | train_inner | epoch 097:     84 / 1021 loss=5.626, nll_loss=4.378, ppl=20.79, wps=24608.6, ups=6.21, wpb=3961.7, bsz=174.3, num_updates=98100, lr=0.000100964, gnorm=1.224, train_wall=15, wall=15588
2024-05-31 20:33:53 | INFO | train_inner | epoch 097:    184 / 1021 loss=5.628, nll_loss=4.378, ppl=20.79, wps=26569.4, ups=6.76, wpb=3929, bsz=144.3, num_updates=98200, lr=0.000100912, gnorm=1.203, train_wall=15, wall=15603
2024-05-31 20:34:08 | INFO | train_inner | epoch 097:    284 / 1021 loss=5.617, nll_loss=4.367, ppl=20.63, wps=26511.5, ups=6.69, wpb=3964.3, bsz=168.5, num_updates=98300, lr=0.000100861, gnorm=1.176, train_wall=15, wall=15618
2024-05-31 20:34:23 | INFO | train_inner | epoch 097:    384 / 1021 loss=5.637, nll_loss=4.388, ppl=20.94, wps=26512.7, ups=6.72, wpb=3947.3, bsz=155.3, num_updates=98400, lr=0.00010081, gnorm=1.216, train_wall=15, wall=15633
2024-05-31 20:34:38 | INFO | train_inner | epoch 097:    484 / 1021 loss=5.667, nll_loss=4.423, ppl=21.45, wps=26594.7, ups=6.71, wpb=3964.9, bsz=148.3, num_updates=98500, lr=0.000100759, gnorm=1.204, train_wall=15, wall=15648
2024-05-31 20:34:53 | INFO | train_inner | epoch 097:    584 / 1021 loss=5.662, nll_loss=4.417, ppl=21.36, wps=26065.3, ups=6.63, wpb=3933.3, bsz=147.9, num_updates=98600, lr=0.000100707, gnorm=1.201, train_wall=15, wall=15663
2024-05-31 20:35:09 | INFO | train_inner | epoch 097:    684 / 1021 loss=5.68, nll_loss=4.437, ppl=21.66, wps=24677, ups=6.26, wpb=3941.2, bsz=152.3, num_updates=98700, lr=0.000100656, gnorm=1.233, train_wall=16, wall=15679
2024-05-31 20:35:25 | INFO | train_inner | epoch 097:    784 / 1021 loss=5.674, nll_loss=4.432, ppl=21.59, wps=24688, ups=6.27, wpb=3936.7, bsz=161.8, num_updates=98800, lr=0.000100605, gnorm=1.213, train_wall=16, wall=15695
2024-05-31 20:35:41 | INFO | train_inner | epoch 097:    884 / 1021 loss=5.685, nll_loss=4.445, ppl=21.78, wps=24617, ups=6.26, wpb=3934.5, bsz=171.8, num_updates=98900, lr=0.000100555, gnorm=1.302, train_wall=16, wall=15711
2024-05-31 20:35:57 | INFO | train_inner | epoch 097:    984 / 1021 loss=5.69, nll_loss=4.45, ppl=21.85, wps=24633.3, ups=6.26, wpb=3932.5, bsz=147.1, num_updates=99000, lr=0.000100504, gnorm=1.2, train_wall=16, wall=15727
2024-05-31 20:35:57 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:35:58 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_97_99000.pt (epoch 97 @ 99000 updates, score None) (writing took 0.9972297833301127 seconds)
2024-05-31 20:36:04 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:36:04 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2024-05-31 20:36:04 | INFO | train | epoch 097 | loss 5.658 | nll_loss 4.413 | ppl 21.3 | wps 25479.5 | ups 6.46 | wpb 3943.1 | bsz 156.9 | num_updates 99037 | lr 0.000100485 | gnorm 1.22 | train_wall 155 | wall 15733
2024-05-31 20:36:04 | INFO | fairseq.trainer | begin training epoch 98
2024-05-31 20:36:14 | INFO | train_inner | epoch 098:     63 / 1021 loss=5.648, nll_loss=4.402, ppl=21.14, wps=23075.3, ups=5.88, wpb=3925.1, bsz=154.6, num_updates=99100, lr=0.000100453, gnorm=1.23, train_wall=16, wall=15744
2024-05-31 20:36:30 | INFO | train_inner | epoch 098:    163 / 1021 loss=5.613, nll_loss=4.361, ppl=20.55, wps=24816.3, ups=6.27, wpb=3959, bsz=153.4, num_updates=99200, lr=0.000100402, gnorm=1.193, train_wall=16, wall=15759
2024-05-31 20:36:46 | INFO | train_inner | epoch 098:    263 / 1021 loss=5.64, nll_loss=4.392, ppl=20.99, wps=24738.1, ups=6.3, wpb=3925.3, bsz=164.8, num_updates=99300, lr=0.000100352, gnorm=1.225, train_wall=16, wall=15775
2024-05-31 20:37:02 | INFO | train_inner | epoch 098:    363 / 1021 loss=5.651, nll_loss=4.402, ppl=21.14, wps=24735.1, ups=6.29, wpb=3935.4, bsz=139.1, num_updates=99400, lr=0.000100301, gnorm=1.232, train_wall=16, wall=15791
2024-05-31 20:37:18 | INFO | train_inner | epoch 098:    463 / 1021 loss=5.642, nll_loss=4.394, ppl=21.03, wps=24747.2, ups=6.28, wpb=3943, bsz=154.9, num_updates=99500, lr=0.000100251, gnorm=1.185, train_wall=16, wall=15807
2024-05-31 20:37:34 | INFO | train_inner | epoch 098:    563 / 1021 loss=5.661, nll_loss=4.417, ppl=21.37, wps=24845.5, ups=6.32, wpb=3934.3, bsz=168.3, num_updates=99600, lr=0.000100201, gnorm=1.261, train_wall=16, wall=15823
2024-05-31 20:37:50 | INFO | train_inner | epoch 098:    663 / 1021 loss=5.663, nll_loss=4.417, ppl=21.36, wps=24895.3, ups=6.3, wpb=3952.3, bsz=141.2, num_updates=99700, lr=0.00010015, gnorm=1.22, train_wall=16, wall=15839
2024-05-31 20:38:05 | INFO | train_inner | epoch 098:    763 / 1021 loss=5.679, nll_loss=4.436, ppl=21.65, wps=24777, ups=6.29, wpb=3936.6, bsz=155.7, num_updates=99800, lr=0.0001001, gnorm=1.223, train_wall=16, wall=15855
2024-05-31 20:38:21 | INFO | train_inner | epoch 098:    863 / 1021 loss=5.675, nll_loss=4.433, ppl=21.61, wps=24855.9, ups=6.27, wpb=3962.7, bsz=169, num_updates=99900, lr=0.00010005, gnorm=1.22, train_wall=16, wall=15871
2024-05-31 20:38:37 | INFO | train_inner | epoch 098:    963 / 1021 loss=5.677, nll_loss=4.436, ppl=21.65, wps=24754.5, ups=6.27, wpb=3948.5, bsz=165.7, num_updates=100000, lr=0.0001, gnorm=1.267, train_wall=16, wall=15887
2024-05-31 20:38:37 | INFO | fairseq_cli.train | begin save checkpoint
2024-05-31 20:38:38 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_98_100000.pt (epoch 98 @ 100000 updates, score None) (writing took 0.9901966629549861 seconds)
2024-05-31 20:38:38 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2024-05-31 20:38:38 | INFO | train | epoch 098 | loss 5.653 | nll_loss 4.407 | ppl 21.21 | wps 24594.8 | ups 6.24 | wpb 3943.5 | bsz 156.8 | num_updates 100000 | lr 0.0001 | gnorm 1.224 | train_wall 152 | wall 15888
2024-05-31 20:38:38 | INFO | fairseq_cli.train | done training in 15887.5 seconds
